<examlList>
    <exam>
        <question>To run an application, a DevOps Engineer launches an Amazon EC2 instances with public IP addresses ina public subnet. A user data script obtains the application artifacts and installs them on the instances uponlaunch. A change to the security classification of the application now requires the instances to run with noaccess to the Internet. While the instances launch successfully and show as healthy, the application doesnot seem to be installed.Which of the following should successfully install the application while complying with the new rule?</question>
        <choices>
            <choice>A. Launch the instances in a public subnet with Elastic IP addresses attached. Once the application isinstalled and running, run a script to disassociate the Elastic IP addresses afterwards.</choice>
            <choice>B. Set up a NAT gateway. Deploy the EC2 instances to a private subnet. Update the private subnet's routetable to use the NAT gateway as the default route.</choice>
            <choice>C. Publish the application artifacts to an Amazon S3 bucket and create a VPC endpoint for S3. Assign anIAM instance profile to the EC2 instances so they can read the application artifacts from the S3 bucket.</choice>
            <choice>D. Create a security group for the application instances and whitelist only outbound traffic to the artifactrepository. Remove the security group rule once the install is complete.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>An IT department manages a portfolio with Windows and Linux (Amazon and Red Hat Enterprise Linux)servers both on-premises and on AWS. An audit reveals that there is no process for updating OS and coreapplication patches, and that the servers have inconsistent patch levels.Which of the following provides the MOST reliable and consistent mechanism for updating and maintainingall servers at the recent OS and core application patch levels?</question>
        <choices>
            <choice>A. Install AWS Systems Manager agent on all on-premises and AWS servers. Create Systems ManagerResource Groups. Use Systems Manager Patch Manager with a preconfigured patch baseline to runscheduled patch updates during maintenance windows.</choice>
            <choice>B. Install the AWS OpsWorks agent on all on-premises and AWS servers. Create an OpsWorks stack withseparate layers for each operating system, and get a recipe from the Chef supermarket to run the patchcommands for each layer during maintenance windows.</choice>
            <choice>C. Use a shell script to install the latest OS patches on the Linux servers using yum and schedule it to runautomatically using cron. Use Windows Update to automatically patch Windows servers.</choice>
            <choice>D. Use AWS Systems Manager Parameter Store to securely store credentials for each Linux and Windowsserver. Create Systems Manager Resource Groups. Use the Systems Manager Run Command toremotely deploy patch updates using the credentials in Systems Manager Parameter Store</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A company is setting up a centralized logging solution on AWS and has several requirements. Thecompany wants its Amazon CloudWatch Logs and VPC Flow logs to come from different sub accounts andto be delivered to a single auditing account. However, the number of sub accounts keeps changing. Thecompany also needs to index the logs in the auditing account to gather actionable insight.How should a DevOps Engineer implement the solution to meet all of the company’s requirements?</question>
        <choices>
            <choice>A. Use AWS Lambda to write logs to Amazon ES in the auditing account. Create an Amazon CloudWatchsubscription filter and use Amazon Kinesis Data Streams in the sub accounts to stream the logs to theLambda function deployed in the auditing account.</choice>
            <choice>B. Use Amazon Kinesis Streams to write logs to Amazon ES in the auditing account. Create a CloudWatchsubscription filter and use Kinesis Data Streams in the sub accounts to stream the logs to the Kinesisstream in the auditing account.</choice>
            <choice>C. Use Amazon Kinesis Firehose with Kinesis Data Streams to write logs to Amazon ES in the auditingaccount. Create a CloudWatch subscription filter and stream logs from sub accounts to the Kinesisstream in the auditing account.</choice>
            <choice>D. Use AWS Lambda to write logs to Amazon ES in the auditing account. Create a CloudWatchsubscription filter and use Lambda in the sub accounts to stream the logs to the Lambda functiondeployed in the auditing account.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A company wants to use a grid system for a proprietary enterprise in-memory data store on top of AWS.This system can run in multiple server nodes in any Linux-based distribution. The system must be able toreconfigure the entire cluster every time a node is added or removed. When adding or removing nodes, an /etc./cluster/nodes.config file must be updated, listing the IP addresses of the current node members of thatclusterThe company wants to automate the task of adding new nodes to a cluster.What can a DevOps Engineer do to meet these requirements?</question>
        <choices>
            <choice>A. Use AWS OpsWorks Stacks to layer the server nodes of that cluster. Create a Chef recipe thatpopulates the content of the /etc/cluster/nodes.config file and restarts the service by using the currentmembers of the layer. Assign that recipe to the Configure lifecycle event.</choice>
            <choice>B. Put the file nodes.config in version control. Create an AWS CodeDeploy deployment configuration anddeployment group based on an Amazon EC2 tag value for the cluster nodes. When adding a new nodeto the cluster, update the file with all tagged instances, and make a commit in version control. Deploythe new file and restart the services. ( cluster/nodes 이게 없음 다른 보기엔 있고 )</choice>
            <choice>C. Create an Amazon S3 bucket and upload a version of the etc/cluster/nodes.config file. Create a crontabscript that will poll for that S3 file and download it frequently. Use a process manager, such as Monit orsystemd, to restart the cluster services when it detects that the new file was modified. When adding anode to the cluster, edit the file's most recent members. Upload the new file to the S3 bucket.</choice>
            <choice>D. Create a user data script that lists all members of the current security group of the cluster and automatically updates the /etc/cluster/nodes.config file whenever a new instance is added to the cluster</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A company has established tagging and configuration standards for its infrastructure resources running onAWS. A DevOps Engineer is developing a design that will provide a near-real-time dashboard of thecompliance posture with the ability to highlight violations.Which approach meets the stated requirements?</question>
        <choices>
            <choice>A. Define the resource configurations in AWS Service Catalog, and monitor the AWS Service Catalogcompliance and violations in Amazon CloudWatch. Then, set up and share a live CloudWatchdashboard. Set up Amazon SNS notifications for violations and corrections.</choice>
            <choice>B. Use AWS Config to record configuration changes and output the data to an Amazon S3 bucket. Createan Amazon QuickSight analysis of the dataset, and use the information on dashboards and mobiledevices.</choice>
            <choice>C. Create a resource group that displays resources with the specified tags and those without tags. Use theAWS Management Console to view compliant and non-compliant resources.</choice>
            <choice>D. Define the compliance and tagging requirements in Amazon inspector. Output the results to AmazonCloudWatch Logs. Build a metric filter to isolate the monitored elements of interest and present the datain a CloudWatch dashboard.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A production account has a requirement that any Amazon EC2 instance that has been logged into manuallymust be terminated within 24 hours. All applications in the production account are using Auto Scalinggroups with Amazon CloudWatch Logs agent configured.How can this process be automated?</question>
        <choices>
            <choice>A. Create a CloudWatch Logs subscription to an AWS Step Functions application. Configure the functionto add a tag to the EC2 instance that produced the login event and mark the instance to bedecommissioned. Then create a CloudWatch Events rule to trigger a second AWS Lambda functiononce a day that will terminate all instances with this tag.</choice>
            <choice>B. Create a CloudWatch alarm that will trigger on the login event. Send the notification to an Amazon SNStopic that the Operations team is subscribed to, and have them terminate the EC2 instance within 24hours.</choice>
            <choice>C. Create a CloudWatch alarm that will trigger on the login event. Configure the alarm to send to anAmazon SQS queue. Use a group of worker instances to process messages from the queue, whichthen schedules the Amazon CloudWatch Events rule to trigger.</choice>
            <choice>D. Create a CloudWatch Logs subscription in an AWS Lambda function. Configure the function to add atag to the EC2 instance that produced the login event and mark the instance to be decommissioned.Create a CloudWatch Events rule to trigger a daily Lambda function that terminates all instances withthis tag.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer is implementing a mechanism for canary testing an application on AWS. Theapplication was recently modified and went through security, unit, and functional testing. The applicationneeds to be deployed on an AutoScaling group and must use a Classic Load Balancer.Which design meets the requirement for canary testing?</question>
        <choices>
            <choice>A. Create a different Classic Load Balancer and Auto Scaling group for blue/green environments. UseAmazon Route 53 and create weighted A records on Classic Load Balancer.</choice>
            <choice>B. Create a single Classic Load Balancer and an Auto Scaling group for blue/green environments. UseAmazon Route 53 and create A records for Classic Load Balancer IPs. Adjust traffic using A records.</choice>
            <choice>C. Create a single Classic Load Balancer and an Auto Scaling group for blue/green environments. Createan Amazon CloudFront distribution with the Classic Load Balancer as the origin. Adjust traffic usingCloudFront.</choice>
            <choice>D. Create a different Classic Load Balancer and Auto Scaling group for blue/green environments. Createan Amazon API Gateway with a separate stage for the Classic Load Balancer. Adjust traffic by givingweights to this stage.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>An online retail company based in the United States plans to expand its operations to Europe and Asia inthe next six months. Its product currently runs on Amazon EC2 instances behind an Application LoadBalancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. Alldata is stored in an Amazon Aurora database instance.When the product is deployed in multiple regions, the company wants a single product catalog across allregions, but for compliance purposes, its customer information and purchases must be kept in each region.How should the company meet these requirements with the LEAST amount of application changes?</question>
        <choices>
            <choice>A. Use Amazon Redshift for the product catalog and Amazon DynamoDB tables for the customerinformation and purchases.</choice>
            <choice>B. Use Amazon DynamoDB global tables for the product catalog and regional tables for the customerinformation and purchases</choice>
            <choice>C. Use Aurora with read replicas for the product catalog and additional local Aurora instances in eachregion for the customer information and purchases.</choice>
            <choice>D. Use Aurora for the product catalog and Amazon DynamoDB global tables for the customer informationand purchases.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A company has several AWS accounts. The accounts are shared and used across multiple teams globally,primarily for Amazon EC2 instances. Each EC2 instance has tags for team, environment, and cost center toensure accurate cost allocations.How should a DevOps Engineer help the teams audit their costs and automate infrastructure costoptimization across multiple shared environments and accounts?</question>
        <choices>
            <choice>A. Set up a scheduled script on the EC2 instances to report utilization and store the instances in anAmazon DynamoDB table. Create a dashboard in Amazon QuickSight with DynamoDB as the sourcedata to find underutilized instances. Set up triggers from Amazon QuickSight in AWS Lambda to reduceunderutilized instances.</choice>
            <choice>B. Create a separate Amazon CloudWatch dashboard for EC2 instance tags based on cost center,environment, and team, and publish the instance tags out using unique links for each team. For eachteam, set up a CloudWatch Events rule with the CloudWatch dashboard as the source, and set up atrigger to initiate an AWS Lambda function to reduce underutilized instances.</choice>
            <choice>C. Create an Amazon CloudWatch Events rule with AWS Trusted Advisor as the source for low utilizationEC2 instances. Trigger an AWS Lambda function that filters out reported data based on tags for eachteam, environment, and cost center, and store the Lambda function in Amazon S3. Set up a secondtrigger to initiate a Lambda function to reduce underutilized instances.</choice>
            <choice>D. Use AWS Systems Manager to track instance utilization and report underutilized instances to AmazonCloudWatch. Filter data in CloudWatch based on tags for team, environment, and cost center. Set uptriggers from CloudWatch into AWS Lambda to reduce underutilized instances</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A company has a hybrid architecture solution in which some legacy systems remain on-premises, while aspecific cluster of servers is moved to AWS. The company cannot reconfigure the legacy systems, so thecluster nodes must have a fixed hostname and local IP address for each server that is part of the cluster.The DevOps Engineer must automate the configuration for a six-node cluster with high availability acrossthree Availability Zones (AZs), placing two elastic network interfaces in a specific subnet for each AZ. Eachnode's hostname and local IP address should remain the same between reboots or instance failures.Which solution involves the LEAST amount of effort to automate this task?</question>
        <choices>
            <choice>A. Create an AWS Elastic Beanstalk application and a specific environment for each server of the cluster.For each environment, give the hostname, elastic network interface, and AZ as input parameters. Usethe local health agent to name the instance and attach a specific elastic network interface based on thecurrent environment.</choice>
            <choice>B. Create a reusable AWS CloudFormation template to manage an Amazon EC2 Auto Scaling group witha minimum size of 1 and a maximum size of 1. Give the hostname, elastic network interface, and AZ asstack parameters. Use those parameters to set up an EC2 instance with EC2 Auto Scaling and a userdata script to attach to the specific elastic network interface. Use CloudFormation nested stacks to nestthe template six times for a total of six nodes needed for the cluster, and deploy using the mastertemplate.</choice>
            <choice>C. Create an Amazon DynamoDB table with the list of hostnames subnets, and elastic network interfacesto be used. Create a single AWS CloudFormation template to manage an Auto Scaling group with aminimum size of 6 and a maximum size of 6. Create a programmatic solution that is installed in eachinstance that will lock/release the assignment of each hostname and local IP address, depending on thesubnet in which a new instance will be launched.</choice>
            <choice>D. Create a reusable AWS CLI script to launch each instance individually, which will name the instance,place it in a specific AZ, and attach a specific elastic network interface. Monitor the instances, and in theevent of failure, replace the missing instance manually by running the script again.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>An education company has a Docker-based application running on multiple Amazon EC2 instances in anAmazon ECS cluster. When deploying a new version of the application, the Developer, pushes a newimage to a private Docker container registry, and then stops and starts all tasks to ensure that they all havethe latest version of the application. The Developer discovers that the new tasks are occasionally runningwith an old image.How can this issue be prevented?  </question>
        <choices>
            <choice>A.</choice>
            <choice>B.</choice>
            <choice>C.</choice>
            <choice>D. After pushing the new image, restart ECS Agent, and then start the tasks.Use “latest” for the Docker image tag in the task definition.Update the digest on the task definition when pushing the new image.Use Amazon ECR for a Docker container registry  </choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A financial institution provides security-hardened AMIs of Red Hat Enterprise Linux 7.4 and WindowsServer 2016 for its application teams to use in deployments. A DevOps Engineer needs to implement anautomated daily check of each AMI to monitor for the latest CVE.How should the Engineer implement these checks using Amazon Inspector?</question>
        <choices>
            <choice>A. Install the Amazon Inspector agent in each AMI. Configure AWS Step Functions to launch an AmazonEC2 instance for each operating system from the hardened AMI, and tag the instance withSecurityCheck: True. Once EC2 instances have booted up, Step Functions will trigger an AmazonInspector assessment for all instances with the tag SecurityCheck: True. Implement a scheduledAmazon CloudWatch Events rule that triggers Step Functions once each day.</choice>
            <choice>B. Tag each AMI with SecurityCheck: True. Configure AWS Step Functions to first compose an AmazonInspector assessment template for all AMIs that have the tag SecurityCheck: True and second to makea call to the Amazon Inspector API action StartAssessmentRun. Implement a scheduled AmazonCloudWatch Events rule that triggers Step Functions once each day.</choice>
            <choice>C. Tag each AMI with SecurityCheck: True. Implement a scheduled Amazon Inspector assessment to runonce each day for all AMIs with the tag SecurityCheck: True. Amazon Inspector should automaticallylaunch an Amazon EC2 instance for each AMI and perform a security assessment.</choice>
            <choice>D. Tag each instance with SecurityCheck: True. Implement a scheduled Amazon Inspector assessment torun once each day for all instances with the tag SecurityCheck: True. Amazon Inspector shouldautomatically perform an in-place security assessment for each AMI.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A Development team uses AWS CodeCommit for source code control. Developers apply their changes tovarious feature branches and create pull requests to move those changes to the master branch when theyare ready for production. A direct push to the master branch should not be allowed. The team applied theAWS managed policy AWSCodeCommitPowerUser to the Developers’ IAM Rote, but now members areable to push to the master branch directly on every repository in the AWS account.What actions should be taken to restrict this?</question>
        <choices>
            <choice>A. Create an additional policy to include a deny rule for the codecommit:GitPush action, and include arestriction for the specific repositories in the resource statement with a condition for the masterreference.</choice>
            <choice>B. Remove the IAM policy and add an AWSCodeCommitReadOnly policy. Add an allow rule for thecodecommit:GitPush action for the specific repositories in the resource statement with a conditionfor the master reference.</choice>
            <choice>C. Modify the IAM policy and include a deny rule for the codecommit:GitPush action for the specificrepositories in the resource statement with a condition for the master reference.</choice>
            <choice>D. Create an additional policy to include an allow rule for the codecommit:GitPush action and include arestriction for the specific repositories in the resource statement with a condition for the featurebranches reference.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A Developer is designing a continuous deployment workflow for a new Development team to facilitate theprocess for source code promotion in AWS. Developers would like to store and promote code fordeployment from development to production while maintaining the ability to roll back that deployment if itfails.Which design will incur the LEAST amount of downtime?</question>
        <choices>
            <choice>A. Create one repository in AWS CodeCommit. Create a development branch to hold merged changes.Use AWS CodeBuild to build and test the code stored in the development branch triggered on a newcommit. Merge to the master and deploy to production by using AWS CodeDeploy for a blue/greendeployment.</choice>
            <choice>B. Create one repository for each Developer in AWS CodeCommit and another repository to hold theproduction code. Use AWS CodeBuild to merge development and production repositories, and deploy toproduction by using AWS CodeDeploy for a blue/green deployment.</choice>
            <choice>C. Create one repository for development code in AWS CodeCommit and another repository to hold theproduction code. Use AWS CodeBuild to merge development and production repositories, and deploy toproduction by using AWS CodeDeploy for a blue/green deployment.</choice>
            <choice>D. Create a shared Amazon S3 bucket for the Development team to store their code. Set up an AmazonCloudWatch Events rule to trigger an AWS Lambda function that deploys the code to production byusing AWS CodeDeploy for a blue/green deployment.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer discovered a sudden spike in a website's page load times and found that a recentdeployment occurred. A brief diff of the related commit shows that the URL for an external API call wasaltered and the connecting port changed from 80 to 443. The external API has been verified and worksoutside the application. The application logs show that the connection is now timing out, resulting in multipleretries and eventual failure of the call.Which debug steps should the Engineer take to determine the root cause of the issue?</question>
        <choices>
            <choice>A. Check the VPC Flow Logs looking for denies originating from Amazon EC2 instances that are part ofthe web Auto Scaling group. Check the ingress security group rules and routing rules for the VPC.</choice>
            <choice>B. Check the existing egress security group rules and network ACLs for the VPC. Also check theapplication logs being written to Amazon CloudWatch Logs for debug information.</choice>
            <choice>C. Check the egress security group rules and network ACLs for the VPC. Also check the VPC flow logslooking for accepts originating from the web Auto Scaling group.</choice>
            <choice>D. Check the application logs being written to Amazon CloudWatch Logs for debug information. Check theingress security group rules and routing rules for the VPC.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>An Engineering team manages a Node.js e-commerce application. The current environment consists of thefollowing components:• Amazon S3 buckets for storing content• Amazon EC2 for the front-end web servers• AWS Lambda for executing image processing• Amazon DynamoDB for storing session-related dataThe team expects a significant increase in traffic to the site. The application should handle the additionalload without interruption. The team ran initial tests by adding new servers to the EC2 front-end to handle thelarger load, but the instances took up to 20 minutes to become fully configured. The team wants to reducethis configuration time.What changes will the Engineering team need to implement to make the solution the MOST resilient andhighly available while meeting the expected increase in demand?</question>
        <choices>
            <choice>A. Use AWS OpsWorks to automatically configure each new EC2 instance as it is launched. Configure theEC2 instances by using an Auto Scaling group behind an Application Load Balancer across multipleAvailability Zones. Implement Amazon DynamoDB Auto Scaling. Use Amazon Route 53 to point theapplication DNS record to the Application Load Balancer.</choice>
            <choice>B. Deploy a fleet of EC2 instances, doubling the current capacity, and place them behind an ApplicationLoad Balancer. Increase the Amazon DynamoDB read and write capacity units. Add an alias record thatcontains the Application Load Balancer endpoint to the existing Amazon Route 53 DNS record thatpoints to the application.</choice>
            <choice>C. Configure Amazon CloudFront and have its origin point to Amazon S3 to host the web application.Implement Amazon DynamoDB Auto Scaling. Use Amazon Route 53 to point the application DNSrecord to the CloudFront DNS name.</choice>
            <choice>D. Use AWS Elastic Beanstalk with a custom AMI including all web components. Deploy the platform byusing an Auto Scaling group behind an Application Load Balancer across multiple Availability Zones.Implement Amazon DynamoDB Auto Scaling. Use Amazon Route 53 to point the application DNSrecord to the Elastic Beanstalk load balancer.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer is working on a project that is hosted on Amazon Linux and has failed a securityreview. The DevOps Manager has been asked to review the company buildspec.yaml file for an AWSCodeBuild project and provide recommendations. The buildspec.yaml file is configured as follows:  What changes should be recommended to comply with AWS security best practices? (Choose three.)</question>
        <choices>
            <choice>A. Add a post-build command to remove the temporary files from the container before termination toensure they cannot be seen by other CodeBuild users.</choice>
            <choice>B. Update the CodeBuild project role with the necessary permissions and then remove the AWScredentials from the environment variable.</choice>
            <choice>C. Store the DB_PASSWORD as a SecureString value in AWS Systems Manager Parameter Store andthen remove the DB_PASSWORD from the environment variables.</choice>
            <choice>D. Move the environment variables to the ‘db-deploy-bucket’ Amazon S3 bucket, add a prebuild stage todownload, then export the variables.</choice>
            <choice>E. Use AWS Systems Manager run command versus scp and ssh commands directly to the instance.F. Scramble the environment variables using XOR followed by Base64, add a section to install, and thenrun XOR and Base64 to the build phase. </choice>
        </choices>
        <correctAnswers>B,C,E</correctAnswers>
    </exam>
    <exam>
        <question>A Development team is building more than 40 applications. Each app is a three-tiered web applicationbased on an ELB Application Load Balancer, Amazon EC2, and Amazon RDS. Because the applicationswill be used internally, the Security team wants to allow access to the 40 applications only from thecorporate network and block access from external IP addresses. The corporate network reaches theinternet through proxy servers. The proxy servers have 12 proxy IP addresses that are being changed oneor two times per month. The Network Infrastructure team manages the proxy servers; they upload the filethat contains the latest proxy IP addresses into an Amazon S3 bucket. The DevOps Engineer must build asolution to ensure that the applications are accessible from the corporate network.Which solution achieves these requirements with MINIMAL impact to application development, MINIMALoperational effort, and the LOWEST infrastructure cost?</question>
        <choices>
            <choice>A. Implement an AWS Lambda function to read the list of proxy IP addresses from the S3 object and toupdate the ELB security groups to allow HTTPS only from the given IP addresses. Configure the S3bucket to invoke the Lambda function when the object is updated. Save the IP address list to the S3bucket when they are changed.</choice>
            <choice>B. Ensure that all the applications are hosted in the same Virtual Private Cloud (VPC). Otherwise,consolidate the applications into a single VPC. Establish an AWS Direct Connect connection with anactive/standby configuration. Change the ELB security groups to allow only inbound HTTPS connectionsfrom the corporate network IP addresses.</choice>
            <choice>C. Implement a Python script with the AWS SDK for Python (Boto), which downloads the S3 object thatcontains the proxy IP addresses, scans the ELB security groups, and updates them to allow onlyHTTPS inbound from the given IP addresses. Launch an EC2 instance and store the script in theinstance. Use a cron job to execute the script daily.</choice>
            <choice>D. Enable ELB security groups to allow HTTPS inbound access from the Internet. Use Amazon Cognito tointegrate the company's Active Directory as the identity provider. Change the 40 applications to integratewith Amazon Cognito so that only company employees can log into the application. Save the useraccess logs to Amazon CloudWatch Logs to record user access activities</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A company is implementing AWS CodePipeline to automate its testing process. The company wants to benotified when the execution state fails and used the following custom event pattern in Amazon CloudWatch: Which type of events will match this event pattern?  </question>
        <choices>
            <choice>A. Failed deploy and build actions across all the pipelines.</choice>
            <choice>B. All rejected or failed approval actions across all the pipelines.</choice>
            <choice>C. All the events across all pipelines.</choice>
            <choice>D. Approval actions across all the pipelines.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company is using several AWS CloudFormation templates for deploying infrastructure as code. In mostof the deployments, the company uses Amazon EC2 Auto Scaling groups. A DevOps Engineer needs toupdate the AMIs for the Auto Scaling group in the template if newer AMIs are available.How can these requirements be met?</question>
        <choices>
            <choice>A. Manage the AMI mappings in the CloudFormation template. Use Amazon CloudWatch Events fordetecting new AMIs and updating the mapping in the template. Reference the map in the launchconfiguration resource block.</choice>
            <choice>B. Use conditions in the AWS CloudFormation template to check if new AMIs are available and return theAMI ID. Reference the returned AMI ID in the launch configuration resource block.</choice>
            <choice>C. Use an AWS Lambda-backed custom resource in the template to fetch the AMI IDs. Reference thereturned AMI ID in the launch configuration resource block.</choice>
            <choice>D. Launch an Amazon EC2 m4.small instance and run a script on it to check for new AMIs. If new AMIsare available, the script should update the launch configuration resource block with the new AMI ID.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer administers an application that manages video files for a video production company.The application runs on Amazon EC2 instances behind an ELB Application Load Balancer. The instancesrun in an Auto Scaling group across multiple Availability Zones. Data is stored in an Amazon RDSPostgreSQL Multi-AZ DB instance, and the video files are stored in an Amazon S3 bucket. On a typical day,50 GB of new video are added to the S3 bucket. The Engineer must implement a multi-region disasterrecovery plan with the least data loss and the lowest recovery times. The current application infrastructureis already described using AWS CloudFormation.Which deployment option should the Engineer choose to meet the uptime and recovery objectives for thesystem?</question>
        <choices>
            <choice>A. Launch the application from the CloudFormation template in the second region, which sets the capacityof the Auto Scaling group to 1. Create an Amazon RDS read replica in the second region. In the secondregion, enable cross-region replication between the original S3 bucket and a new S3 bucket. To failover, promote the read replica as master. Update the CloudFormation stack and increase the capacityof the Auto Scaling group.</choice>
            <choice>B. Launch the application from the CloudFormation template in the second region, which sets the capacityof the Auto Scaling group to 1. Create a scheduled task to take daily Amazon RDS cross-regionsnapshots to the second region. In the second region, enable cross-region replication between theoriginal S3 bucket and Amazon Glacier. In a disaster, launch a new application stack in the secondregion and restore the database from the most recent snapshot.</choice>
            <choice>C. Launch the application from the CloudFormation template in the second region which sets the capacityof the Auto Scaling group to 1. Use Amazon CloudWatch Events to schedule a nightly task to take asnapshot of the database, copy the snapshot to the second region, and replace the DB instance in thesecond region from the snapshot. In the second region, enable cross-region replication between theoriginal S3 bucket and a new S3 bucket. To fail over, increase the capacity of the Auto Scaling group.</choice>
            <choice>D. Use Amazon CloudWatch Events to schedule a nightly task to take a snapshot of the database andcopy the snapshot to the second region. Create an AWS Lambda function that copies each object to anew S3 bucket in the second region in response to S3 event notifications. In the second region, launchthe application from the CloudFormation template and restore the database from the most recentsnapshot.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A social networking service runs a web API that allows its partners to search public posts. Post data isstored in Amazon DynamoDB and indexed by AWS Lambda functions, with an Amazon ES domain storingthe indexes and providing search functionality to the application.The service needs to maintain full capacity during deployments and ensure that failed deployments do notcause downtime or reduced capacity, or prevent subsequent deployments.How can these requirements be met? (Choose two.)</question>
        <choices>
            <choice>A. Run the web application in AWS Elastic Beanstalk with the deployment policy set to All at Once. Deploythe Lambda functions, DynamoDB tables, and Amazon ES domain with an AWS CloudFormationtemplate.</choice>
            <choice>B. Deploy the web application, Lambda functions, DynamoDB tables, and Amazon ES domain in an AWSCloudFormation template. Deploy changes with an AWS CodeDeploy in-place deployment.</choice>
            <choice>C. Run the web application in AWS Elastic Beanstalk with the deployment policy set to Immutable. Deploythe Lambda functions, DynamoDB tables, and Amazon ES domain with an AWS CloudFormationtemplate.</choice>
            <choice>D. Deploy the web application, Lambda functions, DynamoDB tables, and Amazon ES domain in an AWSCloudFormation template. Deploy changes with an AWS CodeDeploy blue/green deployment.</choice>
            <choice>E. Run the web application in AWS Elastic Beanstalk with the deployment policy set to Rolling. Deploy theLambda functions, DynamoDB tables, and Amazon ES domain with an AWS CloudFormation template.</choice>
        </choices>
        <correctAnswers>A,C,D</correctAnswers>
    </exam>
    <exam>
        <question>A media customer has several thousand amazon EC2 instances in an AWS account. The customer isusing a Slack channel for team communications and important updates. A DevOps Engineer was told tosend all AWS-scheduled EC2 maintenance notifications to the company Slack channel.Which method should the Engineer use to implement this process in the LEAST amount of steps?</question>
        <choices>
            <choice>A. Integrate AWS Trusted Advisor with AWS Config. Based on the AWS Config rules created, the AWSConfig event can invoke an AWS Lambda function to send notifications to the Slack channel.</choice>
            <choice>B. Integrate AWS Personal Health Dashboard with Amazon CloudWatch Events. Based on theCloudWatch Events created, the event can invoke an AWS Lambda function to send notifications to theSlack channel.</choice>
            <choice>C. Integrate EC2 events with Amazon CloudWatch monitoring. Based on the CloudWatch Alarm created,the alarm can invoke an AWS Lambda function to send EC2 maintenance notifications to the Slackchannel.</choice>
            <choice>D. Integrate AWS Support with AWS CloudTrail. Based on the CloudTrail lookup event created, the eventcan invoke an AWS Lambda function to pass EC2 maintenance notifications to the Slack channel.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>After conducting a disaster recovery exercise, an Enterprise Architect discovers that a large team ofDatabase and Storage Administrators need more than seven hours of manual effort to make a flagshipapplication's database functional in a different AWS Region. The Architect also discovers that therecovered database is often missing as much as two hours of data transactions.Which solution provides improved RTO and RPO in a cross-region failover scenario?</question>
        <choices>
            <choice>A. Deploy an Amazon RDS Multi-AZ instance backed by a multi-region Amazon EFS. Configure the RDSoption group to enable multi-region availability for native automation of cross-region recovery andcontinuous data replication. Create an Amazon SNS topic subscribed to RDS-impacted events to sendemails to the Database Administration team when significant query Latency is detected in a singleAvailability Zone.</choice>
            <choice>B. Use Amazon SNS topics to receive published messages from Amazon RDS availability and backupevents. Use AWS Lambda for three separate functions with calls to Amazon RDS to snapshot adatabase instance, create a cross-region snapshot copy, and restore an instance from a snapshot. Usea scheduled Amazon CloudWatch Events rule at a frequency matching the RPO to trigger the Lambdafunction to snapshot a database instance. Trigger the Lambda function to create a cross-regionsnapshot copy when the SNS topic for backup events receives a new message. Configure the Lambdafunction to restore an instance from a snapshot to trigger sending new messages published to theavailability SNS topic.</choice>
            <choice>C. Create a scheduled Amazon CloudWatch Events rule to make a call to Amazon RDS to create asnapshot from a database instance and specify a frequency to match the RPO. Create an AWS StepFunctions task to call Amazon RDS to perform a cross-region snapshot copy into the failover region,and configure the state machine to execute the task when the RDS snapshot create state is complete.Create an SNS topic subscribed to RDS availability events, and push these messages to an AmazonSQS queue located in the failover region. Configure an Auto Scaling group of worker nodes to poll thequeue for new messages and make a call to Amazon RDS to restore a database from a snapshot aftera checksum on the cross-region copied snapshot returns valid.</choice>
            <choice>D. Use Amazon RDS scheduled instance lifecycle events to create a snapshot and specify a frequency tomatch the RPO. Use Amazon RDS scheduled instance lifecycle event configuration to perform a cross-region snapshot copy into the failover region upon SnapshotCreateComplete events. Configure AmazonCloudWatch to alert when the CloudWatch RDS namespace CPUUtilization metric for the databaseinstance falls to 0% and make a call to Amazon RDS to restore the database snapshot in the failoverregion.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company has deployed several applications globally. Recently, Security Auditors found that few AmazonEC2 instances were launched without Amazon EBS disk encryption. The Auditors have requested a reportdetailing all EBS volumes that were not encrypted in multiple AWS accounts and regions. They also want tobe notified whenever this occurs in future.How can this be automated with the LEAST amount of operational overhead?</question>
        <choices>
            <choice>A. Create an AWS Lambda function to set up an AWS Config rule on all the target accounts. Use AWSConfig aggregators to collect data from multiple accounts and regions. Export the aggregated report toan Amazon S3 bucket and use Amazon SNS to deliver the notifications.</choice>
            <choice>B. Set up AWS CloudTrail to deliver all events to an Amazon S3 bucket in a centralized account. Use theS3 event notification feature to invoke an AWS Lambda function to parse AWS CloudTrail logswhenever logs are delivered to the S3 bucket. Publish the output to an Amazon SNS topic using thesame Lambda function.</choice>
            <choice>C. Create an AWS CloudFormation template that adds an AWS Config managed rule for EBS encryption.Use a CloudFormation stack set to deploy the template across all accounts and regions. Storeconsolidated evaluation results from config rules in Amazon S3. Send a notification using Amazon SNSwhen non-compliant resources are detected.</choice>
            <choice>D. Using AWS CLI, run a script periodically that invokes the aws ec2 describe-volumes query with aJMESPATH query filter. Then, write the output to an Amazon S3 bucket. Set up an S3 event notificationto send events using Amazon SNS when new data is written to the S3 bucket.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer has a single Amazon DynamoDB table that received shipping orders and tracksinventory. The Engineer has three AWS Lambda functions reading from a DymamoDB stream on thattable. The Lambda functions perform various functions such as doing an item count, moving items toAmazon Kinesis Data Firehose, monitoring inventory levels, and creating vendor orders when parts are low.While reviewing logs, the Engineer notices the Lambda functions occasionally fail under increased load,receiving a stream throttling error.Which is the MOST cost-effective solution that requires the LEAST amount of operational management?</question>
        <choices>
            <choice>A. Use AWS Glue integration to ingest the DynamoDB stream, then migrate the Lambda code to an AWSFargate task.</choice>
            <choice>B. Use Amazon Kinesis streams instead of DynamoDB streams, then use Kinesis analytics to trigger theLambda functions.</choice>
            <choice>C. Create a fourth Lambda function and configure it to be the only Lambda reading from the stream. Thenuse this Lambda function to pass the payload to the other three Lambda functions.</choice>
            <choice>D. Have the Lambda functions query the table directly and disable DynamoDB streams. Then have theLambda functions query from a global secondary index.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A government agency is storing highly confidential files in an encrypted Amazon S3 bucket. The agencyhas configured federated access and has allowed only a particular on-premises Active Directory user groupto access this bucket.The agency wants to maintain audit records and automatically detect and revert any accidental changesadministrators make to the IAM policies used for providing this restricted federated access.Which of the following options provide the FASTEST way to meet these requirements?</question>
        <choices>
            <choice>A. Configure an Amazon CloudWatch Events Event Bus on an AWS CloudTrail API for triggering the AWSLambda function that detects and reverts the change.</choice>
            <choice>B. Configure an AWS Config rule to detect the configuration change and execute an AWS Lambdafunction to revert the change.</choice>
            <choice>C. Schedule an AWS Lambda function that will scan the IAM policy attached to the federated access rolefor detecting and reverting any changes.</choice>
            <choice>D. Restrict administrators in the on-premises Active Directory from changing the IAM policies. </choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A healthcare provider has a hybrid architecture that includes 120 on-premises VMware servers runningRedHat and 50 Amazon EC2 instances running Amazon Linux. The company is in the middle of an all-inmigration to AWS and wants to implement a solution for collecting information from the on-premises virtualmachines and the EC2 instances for data analysis. The information includes:- Operating system type and version- Data for installed applications- Network configuration information, such as MAC and IP addresses- Amazon EC2 instance AMI ID and IAM profileHow can these requirements be met with the LEAST amount of administration?</question>
        <choices>
            <choice>A. Write a shell script to run as a cron job on EC2 instances to collect and push the data to Amazon S3.For on-premises resources, use VMware vSphere to collect the data and write it into a file gateway forstoring the data in S3. Finally, use Amazon Athena on the S3 bucket for analytics.</choice>
            <choice>B. Use a script on the on-premises virtual machines as well as the EC2 instances to gather and push thedata into Amazon S3, and then use Amazon Athena for analytics.</choice>
            <choice>C. Install AWS Systems Manager agents on both the on-premises virtual machines and the EC2 instances.Enable inventory collection and configure resource data sync to an Amazon S3 bucket to analyze thedata with Amazon Athena.</choice>
            <choice>D. Use AWS Application Discovery Service for deploying Agentless Discovery Connector in the VMwareenvironment and Discovery Agents on the EC2 instances for collecting the data. Then use the AWSMigration Hub Dashboard for analytics.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A company must ensure consistent behavior of an application running on Amazon Linux in its corporateecosystem before moving into AWS. The company has an existing automated server build system usingVMware. The goal is to demonstrate the functionality of the application and its prerequisites on the newtarget operating system.The DevOps Engineer needs to use the existing corporate server pipeline and virtualization software tocreate a server image. The server image will be tested on-premises to resemble the build on Amazon EC2as closely as possible.How can this be accomplished?</question>
        <choices>
            <choice>A. Download and integrate the latest ISO of CentOS 7 and execute the application deployment on theresulting server.</choice>
            <choice>B. Launch an Amazon Linux AMI using an AWS OpsWorks deployment agent onto the on-premisesinfrastructure, then execute the application deployment.</choice>
            <choice>C. Build an EC2 instance with the latest Amazon Linux operating system, and use the AWS Import/Exportservice to export the EC2 image to a VMware ISO in Amazon S3. Then import the resulting ISO ontothe on-premises system.</choice>
            <choice>D. Download and integrate the latest ISO of Amazon Linux 2 and execute the application deployment onthe resulting server. Confirm that operating system testing results are consistent with EC2 operatingsystem behavior.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A Development team is adding a new country to an e-commerce application. This addition requires thatnew application features be added to the shipping component of the application. The team has not decidedif all new features should be added, as some will take approximately six weeks to build. While the finaldecision on the shipping component features is being made, other team members are continuing to workon other features of the application.Based on this situation, how should the application feature deployments be managed?</question>
        <choices>
            <choice>A. Add the code updates as commits to the release branch. The team can delay the deployment until allfeatures are ready.</choice>
            <choice>B. Add the code updates as commits to a feature branch. Merge the commits to a release branch asfeatures are ready.</choice>
            <choice>C. Add the code updates as a single commit when a feature is ready. Tag this commit with “new-country.”</choice>
            <choice>D. Create a new repository named “new-country”. Commit all the code changes to the new repository.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer is asked to implement a strategy for deploying updates to a web application with zerodowntime. The application infrastructure is defined in AWS CloudFormation and is made up of an AmazonRoute 53 record, an Application Load Balancer, Amazon EC2 instances in an EC2 Auto Scaling group, andAmazon DynamoDB tables. To avoid downtime, there must be an active instance serving the application atall times.Which strategies will ensure the deployment happens with zero downtime? (Choose two.)</question>
        <choices>
            <choice>A. In the CloudFormation template, modify the AWS::AutoScaling::AutoscalingGroup resource and add anUpdatePolicy attribute to define the required elements for a deployment with zero downtime.</choice>
            <choice>B. In the CloudFormation template, modify the AWS:: AutoScaling::DeploymentUpdates resource and addan UpdatePolicy attribute to define the required elements for a deployment with zero downtime.</choice>
            <choice>C. Add a new Application Load Balancer and Auto Scaling group to the CloudFormation template. Deploynew changes to the inactive Auto Scaling group. Use Route 53 to change the active Application LoadBalancer.</choice>
            <choice>D. Add a new Application Load Balancer and Auto Scaling group to the CloudFormation template. Modifythe AWS::AutoScaling::AutoScalingGroup resource and add an UpdatePolicy attribute to perform rollingupdates.</choice>
            <choice>E. In the CloudFormation template, modify the UpdatePolicy attribute for the CloudFormation stack andspecify the Auto Scaling group that will be updated. Configure MinSuccessfulInstancesPercent andPauseTime to ensure the deployment happens with zero downtime.</choice>
        </choices>
        <correctAnswers>A,C</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer must create a Linux AMI in an automated fashion. The newly created AMI identificationmust be stored in a location where other build pipelines can access the new identification programmaticallyWhat is the MOST cost-effective way to do this?</question>
        <choices>
            <choice>A. Build a pipeline in AWS CodePipeline to download and save the latest operating system OpenVirtualization Format (OVF) image to an Amazon S3 bucket, then customize the image using theguestfish utility. Use the virtual machine (VM) import command to convert the OVF to an AMI, and storethe AMI identification output as an AWS Systems Manager parameter.</choice>
            <choice>B. Create an AWS Systems Manager automation document with values instructing how the image shouldbe created. Then build a pipeline in AWS CodePipeline to execute the automation document to build theAMI when triggered. Store the AMI identification output as a Systems Manager parameter.</choice>
            <choice>C. Build a pipeline in AWS CodePipeline to take a snapshot of an Amazon EC2 instance running the latestversion of the application. Then start a new EC2 instance from the snapshot and update the runninginstance using an AWS Lambda function. Take a snapshot of the updated instance, then convert it to anAMI. Store the AMI identification output in an Amazon DynamoDB table.</choice>
            <choice>D. Launch an Amazon EC2 instance and install Packer. Then configure a Packer build with values defininghow the image should be created. Build a Jenkins pipeline to invoke the Packer build when triggered tobuild an AMI. Store the AMI identification output in an Amazon DynamoDB table.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>An application is being deployed with two Amazon EC2 Auto Scaling groups, each configured with anApplication Load Balancer. The application is deployed to one of the Auto Scaling groups and an AmazonRoute 53 alias record is pointed to the Application Load Balancer of the last deployed Auto Scaling group.Deployments alternate between the two Auto Scaling groups.Home security devices are making requests into the application. The Development team notes that newrequests are coming into the old stack days after the deployment. The issue is caused by devices that arenot observing the Time to Live (TTL) setting on the Amazon Route 53 alias record.What steps should the DevOps Engineer take to address the issue with requests coming to the old stacks,while creating minimal additional resources?</question>
        <choices>
            <choice>A. Create a fleet of Amazon EC2 instances running HAProxy behind an Application Load Balancer. TheHAProxy instances will proxy the requests to one of the existing Auto Scaling groups. After adeployment the HAProxy instances are updated to send requests to the newly deployed Auto Scalinggroup.</choice>
            <choice>B. Reduce the application to one Application Load Balancer. Create two target groups named Blue andGreen. Create a rule on the Application Load Balancer pointed to a single target group. Add logic to thedeployment to update the Application Load Balancer rule to the target group of the newly deployed AutoScaling group.</choice>
            <choice>C. Move the application to an AWS Elastic Beanstalk application with two environments. Perform newdeployments on the non-live environment. After a deployment, perform an Elastic Beanstalk CNAMEswap to make the newly deployed environment the live environment.</choice>
            <choice>D. Create an Amazon CloudFront distribution. Set the two existing Application Load Balancers as originson the distribution. After a deployment, update the CloudFront distribution behavior to send requests tothe newly deployed Auto Scaling group.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company has microservices running in AWS Lambda that read data from Amazon DynamoDB. TheLambda code is manually deployed by Developers after successful testing. The company now needs thetests and deployments be automated and run in the cloud. Additionally, traffic to the new versions of eachmicroservice should be incrementally shifted over time after deployment.What solution meets all the requirements, ensuring the MOST developer velocity?</question>
        <choices>
            <choice>A. Create an AWS CodePipeline configuration and set up a post-commit hook to trigger the pipeline aftertests have passed. Use AWS CodeDeploy and create a Canary deployment configuration that specifiesthe percentage of traffic and interval.</choice>
            <choice>B. Create an AWS CodeBuild configuration that triggers when the test code is pushed. Use AWSCloudFormation to trigger an AWS CodePipeline configuration that deploys the new Lambda versionsand specifies the traffic shift percentage and interval.</choice>
            <choice>C. Create an AWS CodePipeline configuration and set up the source code step to trigger when code ispushed. Set up the build step to use AWS CodeBuild to run the tests. Set up an AWS CodeDeployconfiguration to deploy, then select the CodeDeployDefault.LambdaLinear10PercentEvery3Minutesoption.</choice>
            <choice>D. Use the AWS CLI to set up a post-commit hook that uploads the code to an Amazon S3 bucket aftertests have passed. Set up an S3 event trigger that runs a Lambda function that deploys the newversion. Use an interval in the Lambda function to deploy the code over time at the required percentage.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A company is using an AWS CloudFormation template to deploy web applications. The template requiresthat manual changes be made for each of the three major environments: production, staging, anddevelopment. The current sprint includes the new implementation and configuration of AWS CodePipelinefor automated deployments.What changes should the DevOps Engineer make to ensure that the CloudFormation template is reusableacross multiple pipelines?</question>
        <choices>
            <choice>A. Use a CloudFormation custom resource to query the status of the CodePipeline to determine whichenvironment is launched. Dynamically alter the launch configuration of the Amazon EC2 instances.</choice>
            <choice>B. Set up a CodePipeline pipeline for each environment to use input parameters. Use CloudFormationmappings to switch associated UserData for the Amazon EC2 instances to match the environmentbeing launched.</choice>
            <choice>C. Set up a CodePipeline pipeline that has multiple stages, one for each development environment. UseAWS Lambda functions to trigger CloudFormation deployments to dynamically alter the UserData of theAmazon EC2 instances launched in each environment.</choice>
            <choice>D. Use CloudFormation input parameters to dynamically alter the LaunchConfiguration and UserDatasections of each Amazon EC2 instance every time the CloudFormation stack is updated.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>An application runs on Amazon EC2 instances behind an Application Load Balancer. Amazon RDS MySOLis used on the backend. The instances run in an Auto Scaling group across multiple Availability Zones. TheApplication Load Balancer health check ensures the web servers are operating and able to make read/writeSQL connections. Amazon Route 53 provides DNS functionality with a record pointing to the ApplicationLoad Balancer. A new policy requires a geographically isolated disaster recovery site with an RTO of 4hours and an RPO of 15 minutes.Which disaster recovery strategy will require the LEAST amount of changes to the application stack?</question>
        <choices>
            <choice>A. Launch a replica stack of everything except RDS in a different Availability Zone. Create an RDS read-only replica in a new Availability Zone and configure the new stack to point to the local RDS instance.Add the new stack to the Route 53 record set with a failover routing policy.</choice>
            <choice>B. Launch a replica stack of everything except RDS in a different region. Create an RDS read-only replicain a new region and configure the new stack to point to the local RDS instance. Add the new stack to theRoute 53 record set with a latency routing policy.</choice>
            <choice>C. Launch a replica stack of everything except RDS in a different region. Upon failure, copy the snapshotover from the primary region to the disaster recovery region. Adjust the Amazon Route 53 record set topoint to the disaster recovery region's Application Load Balancer.</choice>
            <choice>D. Launch a replica stack of everything except RDS in a different region. Create an RDS read-only replicain a new region and configure the new stack to point to the local RDS instance. Add the new stack to theAmazon Route 53 record set with a failover routing policy.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A company wants to use Amazon DynamoDB for maintaining metadata on its forums. See the sample dataset in the image below. A DevOps Engineer is required to define the table schema with the partition key, the sort key, the localsecondary index, projected attributes, and fetch operations. The schema should support the followingexample searches using the least provisioned read capacity units to minimize cost.-Search within ForumName for items where the subject starts with ‘a’.-Search forums within the given LastPostDateTime time frame.-Return the thread value where LastPostDateTime is within the last three months.Which schema meets the requirements?</question>
        <choices>
            <choice>A. Use Subject as the primary key and ForumName as the sort key. Have LSI with LastPostDateTime asthe sort key and fetch operations for thread.</choice>
            <choice>B. Use ForumName as the primary key and Subject as the sort key. Have LSI with LastPostDateTime asthe sort key and the projected attribute thread.</choice>
            <choice>C. Use ForumName as the primary key and Subject as the sort key. Have LSI with Thread as the sort keyand the projected attribute LastPostDateTime.</choice>
            <choice>D. Use Subject as the primary key and ForumName as the sort key. Have LSI with Thread as the sort keyand fetch operations for LastPostDateTime. </choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company used AWS CloudFormation to deploy a three-tier web application that stores data in an AmazonRDS MySOL Multi-AZ DB instance. A DevOps Engineer must upgrade the RDS instance to the latest majorversion of MySQL while incurring minimal downtime.How should the Engineer upgrade the instance while minimizing downtime?</question>
        <choices>
            <choice>A. Update the EngineVersion property of the AWS::RDS::DBInstance resource type in theCloudFormation template to the latest desired version. Launch a second stack and make the new RDSinstance a read replica.</choice>
            <choice>B. Update the DBEngineVersion property of the AWS:: RDS::DBInstance resource type in theCloudFormation template to the latest desired version. Perform an Update Stack operation. Create anew RDS Read Replicas resource with the same properties as the instance to be upgraded. Perform asecond Update Stack operation.</choice>
            <choice>C. Update the DBEngineVersion property of the AWS::RDS::DBInstance resource type in theCloudFormation template to the latest desired version. Create a new RDS Read Replicas resource withthe same properties as the instance to be upgraded. Perform an Update Stack operation.</choice>
            <choice>D. Update the EngineVersion property of the AWS::RDS::DBInstance resource type in theCloudFormation template to the latest version, and perform an Update Stack operation.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A retail company has adopted AWS OpsWorks for managing its deployments. In the last three months, thecompany has discovered that some production instances have been restarting without reason. Uponinspection of the AWS CloudTrail logs, a DevOps Engineer determined that those instances were restartedby OpsWorks. The Engineer now wants automated email notifications whenever OpsWorks restarts aninstance when the instance is deemed unhealthy or unable to communicate with the service endpoint.How can the Engineer meet this requirement?</question>
        <choices>
            <choice>A. Create a Chef recipe to place a cron to run a custom script within the Amazon EC2 instances that sendsan email to the team by using Amazon SES if the OpsWorks agent detects an instance failure.</choice>
            <choice>B. Create an Amazon SNS topic and create a subscription for this topic that contains the destination emailaddress. Create an Amazon CloudWatch rule: specify aws.opsworks as a source and specify auto-healing in the initiated_by details. Use the SNS topic as a target.</choice>
            <choice>C. Create an Amazon SNS topic and create a subscription for this topic that contains the destination emailaddress. Create an Amazon CloudWatch rule specify aws.opsworks as a source and specifyinstance-replacement in the initiated_by details. Use the SNS topic as a target.</choice>
            <choice>D. Create a subscription for this topic that contains the email address. Enable instance restart notificationswithin the OpsWorks layer and indicate the destination email address for the notification.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A healthcare services company is concerned about the growing costs of software licensing for anapplication for monitoring patient wellness. The company wants to create an audit process to ensure thatthe application is running exclusively on Amazon EC2 Dedicated Hosts. A DevOps Engineer must create aworkflow to audit the application to ensure compliance.What steps should the Engineer take to meet this requirement with the LEAST administrative overhead?</question>
        <choices>
            <choice>A. Use AWS Systems Manager Configuration Compliance. Use calls to the put-compliance- items APIaction to scan and build a database of noncompliant EC2 instances based on their host placementconfiguration. Use an Amazon DynamoDB table to store these instance IDs for fast access. Generate areport through Systems Manager by calling the list-compliance- summaries API action.</choice>
            <choice>B. Use custom Java code running on an EC2 instance. Set up EC2 Auto Scaling for the instancedepending on the number of instances to be checked. Send the list of noncompliant EC2 instance IDsto an Amazon SQS queue. Set up another worker instance to process instance IDs from the SQSqueue and write them to Amazon DynamoDB. Use an AWS Lambda function to terminate noncompliantinstance IDs obtained from the queue, and send them to an Amazon SNS email topic for distribution.</choice>
            <choice>C. Use AWS Config. Identify all EC2 instances to be audited by enabling Config Recording on all AmazonEC2 resources for the region. Create a custom AWS Config rule that triggers an AWS Lambda functionby using the “config-rule-change-triggered” blueprint. Modify the Lambda evaluateCompliance ()function to verify host placement to return a NON_COMPLIANT result if the instance is not running onan EC2 Dedicated Host. Use the AWS Config report to address noncompliant instances.</choice>
            <choice>D. Use AWS CloudTrail. Identify all EC2 instances to be audited by analyzing all calls to the EC2RunCommand API action. Invoke an AWS Lambda function that analyzes the host placement of theinstance. Store the EC2 instance ID of noncompliant resources in an Amazon RDS MySOL DBinstance. Generate a report by querying the RDS instance and exporting the query results to a CSV textfile.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>According to Information Security Policy, changes to the contents of objects inside production Amazon S3bucket that contain encrypted secrets should only be made by a trusted group of administrators.How should a DevOps Engineer create real-time, automated checks to meet this requirement?</question>
        <choices>
            <choice>A. Create an AWS Lambda function that is triggered by Amazon S3 data events for object changes andthat also checks the IAM user’s membership in an administrator’s IAM role.</choice>
            <choice>B. Create a periodic AWS Config rule to query Amazon S3 Logs for changes and to check the IAM user’smembership in an administrator’s IAM role.</choice>
            <choice>C. Create a metrics filter for Amazon CloudWatch logs to check for Amazon S3 bucket-level permissionchanges and to check the IAM user’s membership in an administrator’s IAM role.</choice>
            <choice>D. Create a periodic AWS Config rule to query AWS CloudTrail logs for changes to the Amazon S3 bucket-level permissions and to check the IAM user’s membership in an administrator’s IAM role.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A business has an application that consists of five independent AWS Lambda functions.The DevOps Engineer has built a CI/CD pipeline using AWS CodePipeline and AWS CodeBuild that builds,tests, packages, and deploys each Lambda function in sequence. The pipeline uses an AmazonCloudWatch Events rule to ensure the pipeline execution starts as quickly as possible after a change ismade to the application source code.After working with the pipeline for a few months, the DevOps Engineer has noticed the pipeline takes toolong to complete.What should the DevOps Engineer implement to BEST improve the speed of the pipeline?</question>
        <choices>
            <choice>A. Modify the CodeBuild projects within the pipeline to use a compute type with more available networkthroughput.</choice>
            <choice>B. Create a custom CodeBuild execution environment that includes a symmetric multiprocessingconfiguration to run the builds in parallel.</choice>
            <choice>C. Modify the CodePipeline configuration to execute actions for each Lambda function in parallel byspecifying the same runOrder.</choice>
            <choice>D. Modify each CodeBuild project to run within a VPC and use dedicated instances to increase throughput.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A company uses a complex system that consists of networking, IAM policies, and multiple three-tierapplications. Requirements are still being defined for a new system, so the number of AWS componentspresent in the final design is not known. The DevOps Engineer needs to begin defining AWS resourcesusing AWS CloudFormation to automate and version-control the new infrastructure.What is the best practice for using CloudFormation to create new environments?</question>
        <choices>
            <choice>A. Manually construct the networking layer using Amazon VPC and then define all other resources usingCloudFormation.</choice>
            <choice>B. Create a single template to encompass all resources that are required for the system so there is onlyone template to version-control.</choice>
            <choice>C. Create multiple separate templates for each logical part of the system, use cross-stack references inCloudFormation, and maintain several templates in version control.</choice>
            <choice>D. Create many separate templates for each logical part of the system, and provide the outputs from oneto the next using an Amazon EC2 instance running SDK for granular control.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer is deploying a new web application. The company chooses AWS Elastic Beanstalk fordeploying and managing the web application, and Amazon RDS MySQL to handle persistent data. Thecompany requires that new deployments have minimal impact if they fail. The application resources mustbe at full capacity during deployment, and rolling back a deployment must also be possible.Which deployment sequence will meet these requirements?</question>
        <choices>
            <choice>A. Deploy the application using Elastic Beanstalk and connect to an external RDS MySQL instance usingElastic Beanstalk environment properties. Use Elastic Beanstalk features for a blue/green deploymentto deploy the new release to a separate environment, and then swap the CNAME in the twoenvironments to redirect traffic to the new version.</choice>
            <choice>B. Deploy the application using Elastic Beanstalk, and include RDS MySQL as part of the environment.Use default Elastic Beanstalk behavior to deploy changes to the application, and let rolling updatesdeploy changes to the application.</choice>
            <choice>C. Deploy the application using Elastic Beanstalk, and include RDS MySQL as part of the environment.Use Elastic Beanstalk immutable updates for application deployments.</choice>
            <choice>D. Deploy the application using Elastic Beanstalk, and connect to an external RDS MySQL instance usingElastic Beanstalk environment properties. Use Elastic Beanstalk immutable updates for applicationdeployments.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>Am Amazon EC2 instance with no internet access is running in a Virtual Private Cloud (VPC) and needs todownload an object from a restricted Amazon S3 bucket. When the DevOps Engineer tries to gain accessto the object, an AccessDenied error is received.What are the possible causes for this error? (Choose three.)</question>
        <choices>
            <choice>A. The S3 bucket default encryption is enabled. </choice>
            <choice>B.</choice>
            <choice>C.</choice>
            <choice>D.</choice>
            <choice>E.F. There is an error in the S3 bucket policy.There is an error in the VPC endpoint policy.The object has been moved to Amazon Glacier.There is an error in the IAM role configuration.S3 versioning is enabled. </choice>
        </choices>
        <correctAnswers>B,C,E</correctAnswers>
    </exam>
    <exam>
        <question>An application has microservices spread across different AWS accounts and is integrated with an on-premises legacy system for some of its functionality. Because of the segmented architecture and missinglogs, every time the application experiences issues, it is taking too long to gather the logs to identify theissues. A DevOps Engineer must fix the log aggregation process and provide a way to centrally analyze thelogs.Which is the MOST efficient and cost-effective solution?</question>
        <choices>
            <choice>A. Collect system logs and application logs by using the Amazon CloudWatch Logs agent. Use theAmazon S3 API to export on-premises logs, and store the logs in an S3 bucket in a central account.Build an Amazon EMR cluster to reduce the logs and derive the root cause.</choice>
            <choice>B. Collect system logs and application logs by using the Amazon CloudWatch Logs agent. Use theAmazon S3 API to import on-premises logs. Store all logs in S3 buckets in individual accounts. UseAmazon Macie to write a query to search for the required specific event-related data point.</choice>
            <choice>C. Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install theCloudWatch Logs agent on the on-premises servers. Transfer all logs from AWS to the on-premisesdata center. Use an Amazon Elasticsearch Logstash Kibana stack to analyze logs on premises.</choice>
            <choice>D. Collect system logs and application logs by using the Amazon CloudWatch Logs agent. Install aCloudWatch Logs agent for on-premises resources. Store all logs in an S3 bucket in a central account.Set up an Amazon S3 trigger and an AWS Lambda function to analyze incoming logs and automaticallyidentify anomalies. Use Amazon Athena to run ad hoc queries on the logs in the central account.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer is building a continuous deployment pipeline for a serverless application using AWSCodePipeline and AWS CodeBuild. The source, build, and test stages have been created with the deploystage remaining. The company wants to reduce the risk of an unsuccessful deployment by deploying to asmall percentage of customers and monitoring prior to a full release to all customers.How should the deploy stage be configured to meet these requirements?</question>
        <choices>
            <choice>A. Use AWS CloudFormation to publish a new version on every stack update. Then set up a CodePipelineapproval action for a Developer to test and approve the new version. Finally, use a CodePipeline invokeaction to update an AWS Lambda function to use the production alias</choice>
            <choice>B. Use CodeBuild to use the AWS CLI to update the AWS Lambda function code, then publish a newversion of the function and update the production alias to point to the new version of the function.</choice>
            <choice>C. Use AWS CloudFormation to define the serverless application and AWS CodeDeploy to deploy theAWS Lambda functions using DeploymentPreference: Canary10Percent15Minutes.</choice>
            <choice>D. Use AWS CloudFormation to publish a new version on every stack update. Use the RoutingConfigproperty of the AWS::Lambda::Alias resource to update the traffic routing during the stack update.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer must track the health of a stateless RESTful service sitting behind a Classic LoadBalancer. The deployment of new application revisions is through a Cl/CD pipeline. If the service's latencyincreases beyond a defined threshold, deployment should be stopped until the service has recovered.Which of the following methods allow for the QUICKEST detection time?</question>
        <choices>
            <choice>A. Use Amazon CloudWatch metrics provided by Elastic Load Balancing to calculate average latency.Alarm and stop deployment when latency increases beyond the defined threshold.</choice>
            <choice>B. Use AWS Lambda and Elastic Load Balancing access logs to detect average latency. Alarm and stopdeployment when latency increases beyond the defined threshold.</choice>
            <choice>C. Use AWS CodeDeploy's MinimumHealthyHosts setting to define thresholds for rolling backdeployments. If these thresholds are breached, roll back the deployment.</choice>
            <choice>D. Use Metric Filters to parse application logs in Amazon CloudWatch Logs. Create a filter for latency.Alarm and stop deployment when latency increases beyond the defined threshold.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer is leading the implementation for automating patching of Windows-based workstationsin a hybrid cloud environment by using AWS Systems Manager (SSM).What steps should the Engineer follow to set up Systems Manager to automate patching in thisenvironment? (Choose two.)</question>
        <choices>
            <choice>A. Create multiple IAM service roles for Systems Manager so that the ssm.amazonaws.com service canexecute the AssumeRole operation on every instance. Register the role on a per-resource level toenable the creation of a service token. Perform managed-instance activation with the newly createdservice role attached to each managed instance.</choice>
            <choice>B. Create an IAM service role for Systems Manager so that the ssm.amazonaws.com service can executethe AssumeRole operation. Register the role to enable the creation of a service token. Performmanaged-instance activation with the newly created service role.</choice>
            <choice>C. Using previously obtained activation codes and activation IDs, download and install the SSM Agent onthe hybrid servers, and register the servers or virtual machines on the Systems Manager service. Hybridinstances will show with an "mi-" prefix in the SSM console.</choice>
            <choice>D. Using previously obtained activation codes and activation IDs, download and install the SSM Agent onthe hybrid servers, and register the servers or virtual machines on the Systems Manager service. Hybridinstances will show with an "i-" prefix in the SSM console as if they were provisioned as a regularAmazon EC2 instance.</choice>
            <choice>E. Run AWS Config to create a list of instances that are unpatched and not compliant. Create an instancescheduler job, and through an AWS Lambda function, perform the instance patching to bring them up tocompliance.</choice>
        </choices>
        <correctAnswers>B,C</correctAnswers>
    </exam>
    <exam>
        <question>A company needs to introduce automatic DNS failover for a distributed web application to a disasterrecovery or standby installation. The DevOps Engineer plans to configure Amazon Route 53 to provideDNS routing to alternate endpoint in the event of an application failure.What steps should the Engineer take to accomplish this? (Choose two.)</question>
        <choices>
            <choice>A. Create Amazon Route 53 health checks for each endpoint that cannot be entered as alias records.Ensure firewall and routing rules allow Amazon Route 53 to send requests to the endpoints that arespecified in the health checks.</choice>
            <choice>B. Create alias records that route traffic to AWS resources and set the value of the Evaluate Target Healthoption to Yes, then create all the non-alias records.</choice>
            <choice>C. Create a governing Amazon Route 53 record set, set it to failover, and associate it with the primary andsecondary Amazon Route 53 record sets to distribute traffic to healthy DNS entries.</choice>
            <choice>D. Create an Amazon CloudWatch alarm to monitor the primary Amazon Route 53 DNS entry. Then createan associated AWS Lambda function to execute the failover API call to Route 53 to the secondary DNSentry.</choice>
            <choice>E. Map the primary and secondary Amazon Route 53 record sets to an Amazon CloudFront distributionusing primary and secondary origins.</choice>
        </choices>
        <correctAnswers>A,C</correctAnswers>
    </exam>
    <exam>
        <question>A company is implementing an Amazon ECS cluster to run its workload. The company architecture will runmultiple ECS services on the cluster, with an Application Load Balancer on the front end, using multipletarget groups to route traffic. The Application Development team has been struggling to collect logs thatmust be collected and sent to an Amazon S3 bucket for near-real time analysisWhat must the DevOps Engineer configure in the deployment to meet these requirements? (Choose three.)</question>
        <choices>
            <choice>A. Install the Amazon CloudWatch Logs logging agent on the ECS instances. Change the logging driver inthe ECS task definition to 'awslogs’.</choice>
            <choice>B. Download the Amazon CloudWatch Logs container instance from AWS and configure it as a task.Update the application service definitions to include the logging task.</choice>
            <choice>C. Use Amazon CloudWatch Events to schedule an AWS Lambda function that will run every 60 secondsrunning the create-export -task CloudWatch Logs command, then point the output to the logging S3bucket.</choice>
            <choice>D. Enable access logging on the Application Load Balancer, then point it directly to the S3 logging bucket.</choice>
            <choice>E. Enable access logging on the target groups that are used by the ECS services, then point it directly tothe S3 logging bucket.F. Create an Amazon Kinesis Data Firehose with a destination of the S3 logging bucket, then create anAmazon CloudWatch Logs subscription filter for Kinesis.</choice>
        </choices>
        <correctAnswers>A,D,F</correctAnswers>
    </exam>
    <exam>
        <question>A Development team is currently using AWS CodeDeploy to deploy an application revision to an AutoScaling group. If the deployment process fails, it must be rolled back automatically and a notification mustbe sent.What is the MOST effective configuration that can satisfy all of the requirements?</question>
        <choices>
            <choice>A. Create Amazon CloudWatch Events rules for CodeDeploy operations. Configure a CloudWatch Eventsrule to send out an Amazon SNS message when the deployment fails. Configure CodeDeploy toautomatically roll back when the deployment fails.</choice>
            <choice>B. Use available Amazon CloudWatch metrics for CodeDeploy to create CloudWatch alarms. ConfigureCloudWatch alarms to send out an Amazon SNS message when the deployment fails. Use AWS CLI toredeploy a previously deployed revision.</choice>
            <choice>C. Configure a CodeDeploy agent to create a trigger that will send notification to Amazon SNS topics whenthe deployment fails. Configure CodeDeploy to automatically roll back when the deployment fails.</choice>
            <choice>D. Use AWS CloudTrail to monitor API calls made by or on behalf of CodeDeploy in the AWS account.Send an Amazon SNS message when deployment fails. Use AWS CLI to redeploy a previouslydeployed revision.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A large enterprise is deploying a web application on AWS. The application runs on Amazon EC2 instancesbehind an Application Load Balancer. The instances run in an Auto Scaling group across multipleAvailability Zones. The application stores data in an Amazon RDS Oracle DB instance and AmazonDynamoDB. There are separate environments for development, testing, and production.What is the MOST secure and flexible way to obtain password credentials during deployment?</question>
        <choices>
            <choice>A. Retrieve an access key from an AWS Systems Manager SecureString parameter to access AWSservices. Retrieve the database credentials from a Systems Manager SecureString parameter.</choice>
            <choice>B. Launch the EC2 instances with an EC2 IAM role to access AWS services. Retrieve the databasecredentials from AWS Secrets Manager.</choice>
            <choice>C. Retrieve an access key from an AWS Systems Manager plaintext parameter to access AWS services.Retrieve the database credentials from a Systems Manager SecureString parameter.</choice>
            <choice>D. Launch the EC2 instances with an EC2 IAM role to access AWS services. Store the databasepasswords in an encrypted config file with the application artifacts.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer is designing a deployment strategy for a web application. The application will use anAuto Scaling group to launch Amazon EC2 instances using an AMI. The same infrastructure will bedeployed in multiple environments (development, test, and quality assurance). The deployment strategyshould meet the following requirements:• Minimize the startup time for the instance• Allow the same AMI to work in multiple environments• Store secrets for multiple environments securelyHow should this be accomplished?</question>
        <choices>
            <choice>A. Preconfigure the AMI using an AWS Lambda function that launches an Amazon EC2 instance, and thenruns a script to install the software and create the AMI. Configure an Auto Scaling lifecycle hook todetermine which environment the instance is launched in, and, based on that finding, run a configurationscript. Save the secrets on an .ini file and store them in Amazon S3. Retrieve the secrets using aconfiguration script in EC2 user data.</choice>
            <choice>B. Preconfigure the AMI by installing all the software using AWS Systems Manager automation andconfigure Auto Scaling to tag the instances at launch with their specific environment. Then use abootstrap script in user data to read the tags and configure settings for the environment. Use the AWSSystems Manager Parameter Store to store the secrets using AWS KMS.</choice>
            <choice>C. Use a standard AMI from the AWS Marketplace. Configure Auto Scaling to detect the currentenvironment. Install the software using a script in Amazon EC2 user data. Use AWS Secrets Managerto store the credentials for all environments.</choice>
            <choice>D. Preconfigure the AMI by installing all the software and configuration for all environments. Configure AutoScaling to tag the instances at launch with their environment. Use the Amazon EC2 user data to triggeran AWS Lambda function that reads the instance ID and then reconfigures the setting for the properenvironment. Use the AWS Systems Manager Parameter Store to store the secrets using AWS KMS.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A Developer is maintaining a fleet of 50 Amazon EC2 Linux servers. The servers are part of an AmazonEC2 Auto Scaling group, and also use Elastic Load Balancing for load balancing.Occasionally, some application servers are being terminated after failing ELB HTTP health checks. TheDeveloper would like to perform a root cause analysis on the issue, but before being able to accessapplication logs, the server is terminated.How can log collection be automated?</question>
        <choices>
            <choice>A. Use Auto Scaling lifecycle hooks to put instances in a Pending:Wait state. Create an AmazonCloudWatch Alarm for EC2 Instance Terminate Successful and trigger an AWS Lambdafunction that executes an SSM Run Command script to collect logs, push them to Amazon S3, andcomplete the lifecycle action once logs are collected.</choice>
            <choice>B. Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create a Config rulefor EC2 Instance-terminate Lifecycle Action and trigger a step function that executes ascript to collect logs, push them to Amazon S3, and complete the lifecycle action once logs arecollected.</choice>
            <choice>C. Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an AmazonCloudWatch subscription filter for EC2 Instance Terminate Successful and trigger aCloudWatch agent that executes a script to called logs, push them to Amazon S3, and complete thelifecycle action once logs are collected.</choice>
            <choice>D. Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an AmazonCloudWatch Events rule for EC2 Instance-terminate Lifecycle Action and trigger an AWSLambda function that executes a SSM Run Command script to collect logs, push them to Amazon S3,and complete the lifecycle action once logs are collected.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A publishing company used AWS Elastic Beanstalk, Amazon S3, and Amazon DynamoDB to develop aweb application. The web application has increased dramatically in popularity, resulting in unpredictablespikes in traffic. A DevOps Engineer has noted that 90% of the requests are duplicate read requests.How can the Engineer improve the performance of the website?</question>
        <choices>
            <choice>A. Use Amazon ElastiCache for Redis to cache repeated read requests to DynamoDB and AWSElemental MediaStore to cache images stored in S3.</choice>
            <choice>B. Use Amazon ElastiCache for Memcached to cache repeated read requests to DynamoDB and Varnishto cache images stored in S3.</choice>
            <choice>C. Use DynamoDB Accelerator to cache repeated read requests to DynamoDB and Amazon CloudFront tocache images stored in S3.</choice>
            <choice>D. Use DynamoDB Streams to cache repeated read requests to DynamoDB and API Gateway to cacheimages stored in S3.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A company is creating a software solution that executes a specific parallel-processing mechanism. Thesoftware can scale to tens of servers in some special scenarios. This solution uses a proprietary library thatis license-based, requiring that each individual server have a single, dedicated license installed. Thecompany has 200 licenses and is planning to run 200 server nodes concurrently at most.The company has requested the following features:• A mechanism to automate the use of the licenses at scale.• Creation of a dashboard to use in the future to verify which licenses are available at any moment.What is the MOST effective way to accomplish these requirements?</question>
        <choices>
            <choice>A. Upload the licenses to a private Amazon S3 bucket. Create an AWS CloudFormation template with aMappings section for the licenses. In the template, create an Auto Scaling group to launch the servers.In the user data script, acquire an available license from the Mappings section. Create an Auto Scalinglifecycle hook, then use it to update the mapping after the instance is terminated.</choice>
            <choice>B. Upload the licenses to an Amazon DynamoDB table. Create an AWS CloudFormation template thatuses an Auto Scaling group to launch the servers. In the user data script, acquire an available licensefrom the DynamoDB table. Create an Auto Scaling lifecycle hook, then use it to update the mappingafter the instance is terminated.</choice>
            <choice>C. Upload the licenses to a private Amazon S3 bucket. Populate an Amazon SQS queue with the list oflicenses stored in S3. Create an AWS CloudFormation template that uses an Auto Scaling group tolaunch the servers. In the user data script acquire an available license from SQS. Create an AutoScaling lifecycle hook, then use it to put the license back in SQS after the instance is terminated.</choice>
            <choice>D. Upload the licenses to an Amazon DynamoDB table. Create an AWS CLI script to launch the servers byusing the parameter --count, with min:max instances to launch. In the user data script, acquire anavailable license from the DynamoDB table. Monitor each instance and, in case of failure, replace theinstance, then manually update the DynamoDB table.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company has developed a static website hosted on an Amazon S3 bucket. The website is deployed usingAWS CloudFormation. The CloudFormation template defines an S3 bucket and a custom resource thatcopies content into the bucket from a source location.The company has decided that it needs to move the website to a new location, so the existingCloudFormation stack must be deleted and re-created. However, CloudFormation reports that the stackcould not be deleted cleanly.What is the MOST likely cause and how can the DevOps Engineer mitigate this problem for this and futureversions of the website?</question>
        <choices>
            <choice>A. Deletion has failed because the S3 bucket has an active website configuration. Modify theCloudFormation template to remove the WebsiteConfiguration property from the S3 bucketresource.</choice>
            <choice>B. Deletion has failed because the S3 bucket is not empty. Modify the custom resource's AWS Lambdafunction code to recursively empty the bucket when RequestType is Delete.</choice>
            <choice>C. Deletion has failed because the custom resource does not define a deletion policy. Add aDeletionPolicy property to the custom resource definition with a value of RemoveOnDeletion.</choice>
            <choice>D. Deletion has failed because the S3 bucket is not empty. Modify the S3 bucket resource in theCloudFormation template to add a DeletionPolicy property with a value of Empty.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company is deploying a new mobile game on AWS for its customers around the world. The Developmentteam uses AWS Code services and must meet the following requirements:- Clients need to send/receive real-time playing data from the backend frequently and with minimal latency- Game data must meet the data residency requirementWhich strategy can a DevOps Engineer implement to meet their needs?</question>
        <choices>
            <choice>A. Deploy the backend application to multiple regions. Any update to the code repository triggers a two-stage build and deployment pipeline. A successful deployment in one region invokes an AWS Lambdafunction to copy the build artifacts to an Amazon S3 bucket in another region. After the artifact is copied,it triggers a deployment pipeline in the new region.</choice>
            <choice>B. Deploy the backend application to multiple Availability Zones in a single region. Create an AmazonCloudFront distribution to serve the application backend to global customers. Any update to the coderepository triggers a two-stage build-and-deployment pipeline. The pipeline deploys the backendapplication to all Availability Zones.</choice>
            <choice>C. Deploy the backend application to multiple regions. Use AWS Direct Connect to serve the applicationbackend to global customers. Any update to the code repository triggers a two-stage build-and-deployment pipeline in the region. After a successful deployment in the region, the pipeline continues todeploy the artifact to another region.</choice>
            <choice>D. Deploy the backend application to multiple regions. Any update to the code repository triggers a two-stage build-and-deployment pipeline in the region. After a successful deployment in the region, thepipeline invokes the pipeline in another region and passes the build artifact location. The pipeline usesthe artifact location and deploys applications in the new region.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A Development team is working on a serverless application in AWS. To quickly identify and remediatepotential production issues, the team decides to roll out changes to a small number of users as a testbefore the full release. The DevOps Engineer must develop a solution to minimize downtime and impact.Which of the following solutions should be used to meet the requirements? (Choose two.)</question>
        <choices>
            <choice>A. Create an Application Load Balancer with two target groups. Set up the Application Load Balancer forAmazon API Gateway private integration. Associate one target group to the current version and theother target group to the new version. Configure API Gateway to route 10% of incoming traffic to thenew version. As the new version becomes stable, configure API Gateway to send all traffic to the newversion and detach the old version from the load balancer.</choice>
            <choice>B. Create an alias for an AWS Lambda function pointing to both the current and new versions. Configurethe alias to route 10% of incoming traffic to the new version. As the new version is considered stable,update the alias to route all traffic to the new version.</choice>
            <choice>C. Create a failover record set in AWS Route 53 pointing to the AWS Lambda endpoints for the old andnew versions. Configure Route 53 to route 10% of incoming traffic to the new version. As the newversion becomes stable, update the DNS record to route all traffic to the new version.</choice>
            <choice>D. Create an ELB Network Load Balancer with two target groups. Set up the Network Load Balancer forAmazon API Gateway private integration Associate one target group with the current version and theother target group with the new version. Configure the load balancer to route 10% of incoming traffic tothe new version. As the new version becomes stable, detach the old version from the load balancer.</choice>
            <choice>E. In Amazon API Gateway, create a canary release deployment by adding canary settings to the stage ofa regular deployment. Configure API Gateway to route 10% of the incoming traffic to the canary release.As the canary release is considered stable, promote it to a production release</choice>
        </choices>
        <correctAnswers>B,E</correctAnswers>
    </exam>
    <exam>
        <question>A company wants to implement a CI/CD pipeline for an application that is deployed on AWS. The companyalso has a source-code analysis tool hosted on premises that checks for security flaws. The tool has not yetbeen migrated to AWS and can be accessed only on premises. The company wants to run checks againstthe source code as part of the pipeline before the code is compiled. The checks take anywhere fromminutes to an hour to complete.How can a DevOps Engineer meet these requirements?</question>
        <choices>
            <choice>A. Use AWS CodePipeline to create a pipeline. Add an action to the pipeline to invoke an AWS Lambdafunction after the source stage. Have the Lambda function invoke the source-code analysis tool onpremises against the source input from CodePipeline. The function then waits for the execution tocomplete and places the output in a specified Amazon S3 location.</choice>
            <choice>B. Use AWS CodePipeline to create a pipeline, then create a custom action type. Create a job worker forthe custom action that runs on hardware hosted on premises. The job worker handles running securitychecks with the on-premises code analysis tool and then returns the job results to CodePipeline. Havethe pipeline invoke the custom action after the source stage.</choice>
            <choice>C. Use AWS CodePipeline to create a pipeline. Add a step after the source stage to make an HTTPSrequest to the on-premises hosted web service that invokes a test with the source code analysis tool.When the analysis is complete, the web service sends the results back by putting the results in anAmazon S3 output location provided by CodePipeline.</choice>
            <choice>D. Use AWS CodePipeline to create a pipeline. Create a shell script that copies the input source code to alocation on premises. Invoke the source code analysis tool and return the results to CodePipeline.Invoke the shell script by adding a custom script action after the source stage.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company is adopting AWS CodeDeploy to automate its application deployments for a Java-ApacheTomcat application with an Apache webserver. The Development team started with a proof of concept,created a deployment group for a developer environment, and performed functional tests within theapplication. After completion, the team will create additional deployment groups for staging and productionThe current log level is configured within the Apache settings, but the team wants to change thisconfiguration dynamically when the deployment occurs, so that they can set different log levelconfigurations depending on the deployment group without having a different application revision for eachgroup.How can these requirements be met with the LEAST management overhead and without requiring differentscript versions for each deployment group?</question>
        <choices>
            <choice>A. Tag the Amazon EC2 instances depending on the deployment group. Then place a script into theapplication revision that calls the metadata service and the EC2 API to identify which deployment groupthe instance is part of. Use this information to configure the log level settings. Reference the script aspart of the Afterinstall lifecycle hook in the appspec.yml file.</choice>
            <choice>B. Create a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_NAME toidentify which deployment group the instances is part of. Use this information to configure the log levelsettings. Reference this script as part of the BeforeInstall lifecycle hook in the appspec.yml file</choice>
            <choice>C. Create a CodeDeploy custom environment variable for each environment. Then place a script into theapplication revision that checks this environment variable to identify which deployment group theinstance is part of. Use this information to configure the log level settings. Reference this script as partof the ValidateService lifecycle hook in the appspec.yml file.</choice>
            <choice>D. Create a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_ID to identifywhich deployment group the instance is part of to configure the log level settings. Reference this scriptas part of the Install lifecycle hook in the appspec.yml file.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company has an application that has predictable peak traffic times. The company wants the applicationinstances to scale up only during the peak times. The application stores state in Amazon DynamoDB. Theapplication environment uses a standard Node.js application stack and custom Chef recipes stored in aprivate Git repository.Which solution is MOST cost-effective and requires the LEAST amount of management overhead whenperforming rolling updates of the application environment?</question>
        <choices>
            <choice>A. Create a custom AMI with the Node.js environment and application stack using Chef recipes. Use theAMI in an Auto Scaling group and set up scheduled scaling for the required times, then set up anAmazon EC2 IAM role that provides permission to access DynamoDB.</choice>
            <choice>B. Create a Docker file that uses the Chef recipes for the application environment based on an officialNode.js Docker image. Create an Amazon ECS cluster and a service for the application environment,then create a task based on this Docker image. Use scheduled scaling to scale the containers at theappropriate times and attach a task-level IAM role that provides permission to access DynamoDB.</choice>
            <choice>C. Configure AWS OpsWorks stacks and use custom Chef cookbooks. Add the Git repository informationwhere the custom recipes are stored, and add a layer in OpsWorks for the Node.js application server.Then configure the custom recipe to deploy the application in the deploy step. Configure time-basedinstances and attach an Amazon EC2 IAM role that provides permission to access DynamoDB.</choice>
            <choice>D. Configure AWS OpsWorks stacks and push the custom recipes to an Amazon S3 bucket and configurecustom recipes to point to the S3 bucket. Then add an application layer type for a standard Node.jsapplication server and configure the custom recipe to deploy the application in the deploy step from theS3 bucket. Configure time-based instances and attach an Amazon EC2 IAM role that providespermission to access DynamoDB.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>The Development team at an online retailer has moved to Business support and want to take advantage ofthe AWS Health Dashboard and the AWS Health API to automate remediation actions for issues with thehealth of AWS resources. The first use case is to respond to AWS detecting an IAM access key that islisted on a public code repository site. The automated response will be to delete the IAM access key andsend a notification to the Security team.How should this be achieved?</question>
        <choices>
            <choice>A. Create an AWS Lambda function to delete the IAM access key. Send AWS CloudTrail logs to AWSCloudWatch logs. Create a CloudWatch Logs metric filter for theAWS_RISK_CREDENTIALS_EXPOSED event with two actions: first, run the Lambda function; second,use Amazon SNS to send a notification to the Security team.</choice>
            <choice>B. Create an AWS Lambda function to delete the IAM access key. Create an AWS Config rule for changesto aws.health and the AWS_RISK_CREDENTIALS_EXPOSED event with two actions: first, run theLambda function; second, use Amazon SNS to send a notification to the Security team.</choice>
            <choice>C. Use AWS Step Functions to create a function to delete the IAM access key, and then use Amazon SNSto send a notification to the Security team. Create an AWS Personal Health Dashboard rule for theAWS_RISK_CREDENTIALS_EXPOSED event; set the target of the Personal Health Dashboard rule toStep Functions.</choice>
            <choice>D. Use AWS Step Functions to create a function to delete the IAM access key, and then use Amazon SNSto send a notification to the Security team. Create an Amazon CloudWatch Events rule with anaws.health event source and the AWS_RISK_CREDENTIALS_EXPOSED event, set the target of theCloudWatch Events rule to Step Functions.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>The Security team depends on AWS CloudTrail to detect sensitive security issues in the company’s AWSaccount. The DevOps Engineer needs a solution to auto-remediate CloudTrail being turned off in an AWSaccount.What solution ensures the LEAST amount of downtime for the CloudTrail log deliveries?</question>
        <choices>
            <choice>A. Create an Amazon CloudWatch Events rule for the CloudTrail StopLogging event. Create an AWSLambda function that uses the AWS SDK to call StartLogging on the ARN of the resource in whichStopLogging was called. Add the Lambda function ARN as a target to the CloudWatch Events rule.</choice>
            <choice>B. Deploy the AWS-managed CloudTrail-enabled AWS Config rule, set with a periodic interval of 1 hour.Create an Amazon CloudWatch Events rule for AWS Config rules compliance change. Create an AWSLambda function that uses the AWS SDK to call StartLogging on the ARN of the resource in whichStopLogging was called. Add the Lambda function ARN as a target to the CloudWatch Events rule.</choice>
            <choice>C. Create an Amazon CloudWatch Events rule for a scheduled event every 5 minutes. Create an AWSLambda function that uses the AWS SDK to call StartLogging on an CloudTrail trail in the AWS account.Add the Lambda function ARN as a target to the CloudWatch Events rule.</choice>
            <choice>D. Launch a t2.nano instance with a script running every 5 minutes that uses the AWS SDK to queryCloudTrail in the current account. If the CloudTrail trail is disabled, have the script re-enable the trail.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer has been asked by the Security team to ensure that AWS CloudTrail files are nottampered with after being created. Currently, there is a process with multiple trails, using AWS IAM torestrict access to specific trails. The Security team wants to ensure they can trace the integrity of each fileand make sure there has been no tampering.Which option will require the LEAST effort to implement and ensure the legitimacy of the file while allowingthe Security team to prove the authenticity of the logs?</question>
        <choices>
            <choice>A. Create an Amazon CloudWatch Events rule that triggers an AWS Lambda function when a new file isdelivered. Configure the Lambda function to perform an MD5 hash check on the file, store the nameand location of the file, and post the returned hash to an Amazon DynamoDB table. The Security teamcan use the values stored in DynamoDB to verify the file authenticity.</choice>
            <choice>B. Enable the CloudTrail file integrity feature on an Amazon S3 bucket. Create an IAM policy that grantsthe Security team access to the file integrity logs stored in the S3 bucket.</choice>
            <choice>C. Enable the CloudTrail file integrity feature on the trail. Use the digest file created by CloudTrail to verifythe integrity of the delivered CloudTrail files.</choice>
            <choice>D. Create an AWS Lambda function that is triggered each time a new file is delivered to the CloudTrailbucket. Configure the Lambda function to execute an MD5 hash check on the file, and store the resulton a tag in an Amazon S3 object. The Security team can use the information on the tag to verify theintegrity of the file.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A company is building a web and mobile application that uses a serverless architecture powered by AWSLambda and Amazon API Gateway. The company wants to fully automate the backend Lambdadeployment based on code that is pushed to the appropriate environment branch in an AWS CodeCommitrepository.The deployment must have the following:- Separate environment pipelines for testing and production.- Automatic deployment that occurs for test environments only.Which steps should be taken to meet these requirements?</question>
        <choices>
            <choice>A. Configure a new AWS CodePipeline service. Create a CodeCommit repository for each environment.Set up CodePipeline to retrieve the source code from the appropriate repository. Set up a deploymentstep to deploy the Lambda functions with AWS CloudFormation.</choice>
            <choice>B. Create two AWS CodePipeline configurations for test and production environments. Configure theproduction pipeline to have a manual approval step. Create a CodeCommit repository for eachenvironment. Set up each CodePipeline to retrieve the source code from the appropriate repository. Setup the deployment step to deploy the Lambda functions with AWS CloudFormation.</choice>
            <choice>C. Create two AWS CodePipeline configurations for test and production environments. Configure theproduction pipeline to have a manual approval step. Create one CodeCommit repository with a branchfor each environment. Set up each CodePipeline to retrieve the source code from the appropriatebranch in the repository. Set up the deployment step to deploy the Lambda functions with AWSCloudFormation.</choice>
            <choice>D. Create an AWS CodeBuild configuration for test and production environments. Configure the productionpipeline to have a manual approval step. Create one CodeCommit repository with a branch for eachenvironment. Push the Lambda function code to an Amazon S3 bucket. Set up the deployment step todeploy the Lambda functions from the S3 bucket.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A company is using AWS for an application. The Development team must automate its deployments. Theteam has set up an AWS CodePipeline to deploy the application to Amazon EC2 instances by using AWSCodeDeploy after it has been built using the AWS CodeBuild service.The team would like to add automated testing to the pipeline to confirm that the application is healthy beforedeploying it to the next stage of the pipeline using the same code. The team requires a manual approvalaction before the application is deployed, even if the test is successful. The testing and approval must beaccomplished at the lowest costs, using the simplest management solution.Which solution will meet these requirements?</question>
        <choices>
            <choice>A. Add a manual approval action after the last deploy action of the pipeline. Use Amazon SNS to informthe team of the stage being triggered. Next, add a test action using CodeBuild to do the required tests.At the end of the pipeline, add a deploy action to deploy the application to the next stage.</choice>
            <choice>B. Add a test action after the last deploy action of the pipeline. Configure the action to use CodeBuild to perform the required tests. If these tests are successful, mark the action as successful. Add a manualapproval action that uses Amazon SNS to notify the team, and add a deploy action to deploy theapplication to the next stage.</choice>
            <choice>C. Create a new pipeline that uses a source action that gets the code from the same repository as the firstpipeline. Add a deploy action to deploy the code to a test environment. Use a test action using AWSLambda to test the deployment. Add a manual approval action by using Amazon SNS to notify the team,and add a deploy action to deploy the application to the next stage.</choice>
            <choice>D. Add a test action after the last deployment action. Use a Jenkins server on Amazon EC2 to do therequired tests and mark the action as successful if the tests pass. Create a manual approval action thatuses Amazon SQS to notify the team and add a deploy action to deploy the application to the nextstage.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company is building a solution for storing files containing Personally Identifiable Information (PII) on AWS.Requirements state:- All data must be encrypted at rest and in transit.- All data must be replicated in at least two locations that are at least 500 miles apart.Which solution meets these requirements?</question>
        <choices>
            <choice>A. Create primary and secondary Amazon S3 buckets in two separate Availability Zones that are at least500 miles apart. Use a bucket policy to enforce access to the buckets only through HTTPS. Use abucket policy to enforce Amazon S3 SSE-C on all objects uploaded to the bucket. Configure cross-region replication between the two buckets.</choice>
            <choice>B. Create primary and secondary Amazon S3 buckets in two separate AWS Regions that are at least 500miles apart. Use a bucket policy to enforce access to the buckets only through HTTPS. Use a bucketpolicy to enforce S3-Managed Keys (SSE-S3) on all objects uploaded to the bucket. Configure cross-region replication between the two buckets.</choice>
            <choice>C. Create primary and secondary Amazon S3 buckets in two separate AWS Regions that are at least 500miles apart. Use an IAM role to enforce access to the buckets only through HTTPS. Use a bucket policyto enforce Amazon S3-Managed Keys (SSE-S3) on all objects uploaded to the bucket. Configure cross-region replication between the two buckets.</choice>
            <choice>D. Create primary and secondary Amazon S3 buckets in two separate Availability Zones that are at least500 miles apart. Use a bucket policy to enforce access to the buckets only through HTTPS. Use abucket policy to enforce AWS KMS encryption on all objects uploaded to the bucket. Configure cross-region replication between the two buckets. Create a KMS Customer Master Key (CMK) in the primaryregion for encrypting objects.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company is using AWS CodeDeploy to automate software deployment. The deployment must meet theserequirements:- A number of instances must be available to serve traffic during the deployment. Traffic must be balanced across those instances, and the instances must automatically heal in the event of failure.- A new fleet of instances must be launched for deploying a new revision automatically, with no manual provisioning.- Traffic must be rerouted to the new environment to half of the new instances at a time. The deployment should succeed if traffic is rerouted to at least half of the instances; otherwise, it should fail.- Before routing traffic to the new fleet of instances, the temporary files generated during the deployment process must be deleted.- At the end of a successful deployment, the original instances in the deployment group must be deleted immediately to reduce costs.How can a DevOps Engineer meet these requirements?</question>
        <choices>
            <choice>A. Use an Application Load Balancer and an in-place deployment. Associate the Auto Scaling group withthe deployment group. Use the Automatically copy Auto Scaling group option, and useCodeDeployDefault.OneAtAtime as the deployment configuration. Instruct AWS CodeDeploy toterminate the original instances in the deployment group, and use the AllowTraffic hook withinappspec.yml to delete the temporary files.</choice>
            <choice>B. Use an Application Load Balancer and a blue/green deployment. Associate the Auto Scaling group andthe Application Load Balancer target group with the deployment group. Use the Automatically copyAuto Scaling group option, create a custom deployment configuration with minimum healthy hostsdefined as 50%, and assign the configuration to the deployment group. Instruct AWS CodeDeploy toterminate the original instances in the deployment group, and use the BeforeBlockTraffic hook withinappsec.yml to delete the temporary files.</choice>
            <choice>C. Use an Application Load Balancer and a blue/green deployment. Associate the Auto Scaling group andthe Application Load Balancer target group with the deployment group. Use the Automatically copyAuto Scaling group option, and use CodeDeployDefault HalfAtAtime as the deploymentconfiguration. Instruct AWS CodeDeploy to terminate the original instances in the deployment group,and use the BeforeAllowTraffic hook within appspec.yml to delete the temporary files.</choice>
            <choice>D. Use an Application Load Balancer and an in-place deployment. Associate the Auto Scaling group andApplication Load Balancer target group with the deployment group. Use the Automatically copyAuto Scaling group option, and use CodeDeployDefault.AllatOnce as a deployment configuration.Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use theBlockTraffic hook within appsec.yml to delete the temporary files.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer is working with an application deployed to 12 Amazon EC2 instances across 3Availability Zones. New instances can be started from an AMI image. On a typical day, each EC2 instancehas 30% utilization during business hours and 10% utilization after business hours. The CPU utilization hasan immediate spike in the first few minutes of business hours. Other increases in CPU utilization risegradually.The Engineer has been asked to reduce costs while retaining the same or higher reliability.Which solution meets these requirements?</question>
        <choices>
            <choice>A. Create two Amazon CloudWatch Events rules with schedules before and after business hours beginand end. Create two AWS Lambda functions, one invoked by each rule. The first function should stopnine instances after business hours end, the second function should restart the nine instances beforethe business day begins.</choice>
            <choice>B. Create an Amazon EC2 Auto Scaling group using the AMI image, with a scaling action based on theAuto Scaling group’s CPU Utilization average with a target of 75%. Create a scheduled action for thegroup to adjust the minimum number of instances to three after business hours end and reset to sixbefore business hours begin.</choice>
            <choice>C. Create two Amazon CloudWatch Events rules with schedules before and after business hours beginand end. Create an AWS CloudFormation stack, which creates an EC2 Auto Scaling group, with aparameter for the number of instances. Invoke the stack from each rule, passing a parameter value ofthree in the morning, and six in the evening.</choice>
            <choice>D. Create an EC2 Auto Scaling group using the AMI image, with a scaling action based on the AutoScaling group’s CPU Utilization average with a target of 75%. Create a scheduled action to terminate nine instances each evening after the close of business.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer must improve the monitoring of a Finance team payments microservice that handlestransactions for an e-commerce platform. The microservice runs on multiple Amazon EC2 instances. TheFinance team would like to know the number of payments per minute, and the team would like to be notifiedwhen this metric falls below a specified threshold.How can this be cost-effectively automated?</question>
        <choices>
            <choice>A. Have the Development team log successful transactions to an application log. Set up Logstash on eachinstance, which sends logs to an Amazon ES cluster. Create a Kibana dashboard for the Finance teamthat graphs the metric.</choice>
            <choice>B. Have the Development team post the number of successful transactions to Amazon CloudWatch as acustom metric. Create a CloudWatch alarm when the threshold is breached, and use Amazon SNS tonotify the Finance team.</choice>
            <choice>C. Have the Development team log successful transactions to an application log. On each instance, set upthe Amazon CloudWatch Logs agent to send application logs to CloudWatch Logs. Use an EC2instance to monitor a metric filter, and send notifications to the Finance team.</choice>
            <choice>D. Have the Development team log successful transactions to an application log. Set up the AmazonCloudWatch agent on each instance. Create a CloudWatch alarm when the threshold is breached, anduse Amazon SNS to notify the Finance team.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company is migrating an application to AWS that runs on a single Amazon EC2 instance. Because oflicensing limitations, the application does not support horizontal scaling. The application will be usingAmazon Aurora for its database.How can the DevOps Engineer architect automated healing to automatically recover from EC2 and Aurorafailures, in addition to recovering across Availability Zones (AZs), in the MOST cost-effective manner?</question>
        <choices>
            <choice>A. Create an EC2 Auto Scaling group with a minimum and maximum instance count of 1, and have it spanacross AZs. Use a single-node Aurora instance.</choice>
            <choice>B. Create an EC2 instance and enable instance recovery. Create an Aurora database with a read replica ina second AZ, and promote it to a primary database instance if the primary database instance fails.</choice>
            <choice>C. Create an Amazon CloudWatch Events rule to trigger an AWS Lambda function to start a new EC2instance in an available AZ when the instance status reaches a failure state. Create an Aurora databasewith a read replica in a second AZ, and promote it to a primary database instance when the primarydatabase instance fails.</choice>
            <choice>D. Assign an Elastic IP address on the instance. Create a second EC2 instance in a second AZ. Create anAmazon CloudWatch Events rule to trigger an AWS Lambda function to move the Elastic IP address tothe second instance when the first instance fails. Use a single-node Aurora instance.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>An Application team has three environments for their application: development, pre-production, andproduction. The team recently adopted AWS CodePipeline. However, the team has had severaldeployments of misconfigured or nonfunctional development code into the production environment,resulting in user disruption and downtime. The DevOps Engineer must review the pipeline and add steps toidentify problems with the application before it is deployed.What should the Engineer do to identify functional issues during the deployment process? (Choose two.)</question>
        <choices>
            <choice>A. Use Amazon Inspector to add a test action to the pipeline. Use the Amazon Inspector Runtime BehaviorAnalysis Inspector rules package to check that the deployed code complies with company securitystandards before deploying it to production.</choice>
            <choice>B. Using AWS CodeBuild to add a test action to the pipeline to replicate common user activities andensure that the results are as expected before progressing to production deployment.</choice>
            <choice>C. Create an AWS CodeDeploy action in the pipeline with a deployment configuration that automaticallydeploys the application code to a limited number of instances. The action then pauses the deploymentso that the QA team can review the application functionality. When the review is complete, CodeDeployresumes and deploys the application to the remaining production Amazon EC2 instances.</choice>
            <choice>D. After the deployment process is complete, run a testing activity on an Amazon EC2 instance in adifferent region that accesses the application to simulate user behavior. If unexpected results occur, thetesting activity sends a warning to an Amazon SNS topic. Subscribe to the topic to get updates.</choice>
            <choice>E. Add an AWS CodeDeploy action in the pipeline to deploy the latest version of the development code topre-production. Add a manual approval action in the pipeline so that the QA team can test and confirmthe expected functionality. After the manual approval action, add a second CodeDeploy action thatdeploys the approved code to the production environment.</choice>
        </choices>
        <correctAnswers>B,E</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer is responsible for the deployment of a PHP application. The Engineer is working in ahybrid deployment, with the application running on both on-premises servers and Amazon EC2 instances.The application needs access to a database containing highly confidential information. Applicationinstances need access to database credentials, which must be encrypted at rest and in transit beforereaching the instances.How should the Engineer automate the deployment process while also meeting the security requirements?</question>
        <choices>
            <choice>A. Use AWS Elastic Beanstalk with a PHP platform configuration to deploy application packages to theinstances. Store database credentials on AWS Systems Manager Parameter Store using the SecureString data type. Define an IAM role for Amazon EC2 allowing access, and decrypt only the databasecredentials. Associate this role to all the instances.</choice>
            <choice>B. Use AWS CodeDeploy to deploy application packages to the instances. Store database credentials onAWS Systems Manager Parameter Store using the Secure String data type. Define an IAM policy forallowing access, and decrypt only the database credentials. Attach the IAM policy to the role associatedto the instance profile for CodeDeploy-managed instances, and to the role used for on-premisesinstances registration on CodeDeploy.</choice>
            <choice>C. Use AWS CodeDeploy to deploy application packages to the instances. Store database credentials onAWS Systems Manager Parameter Store using the Secure String data type. Define an IAM role with anattached policy that allows decryption of the database credentials. Associate this role to all the instancesand on-premises servers.</choice>
            <choice>D. Use AWS CodeDeploy to deploy application packages to the instances. Store database credentials inthe AppSpec file. Define an IAM policy for allowing access to only the database credentials. Attach theIAM policy to the role associated to the instance profile for CodeDeploy-managed instances and the roleused for on-premises instances registration on CodeDeploy.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company has a single Developer writing code for an automated deployment pipeline. The Developer isstoring source code in an Amazon S3 bucket for each project. The company wants to add more Developersto the team but is concerned about code conflicts and lost work. The company also wants to build a testenvironment to deploy newer versions of code for testing and allow Developers to automatically deploy toboth environments when code is changed in the repository.What is the MOST efficient way to meet these requirements?</question>
        <choices>
            <choice>A. Create an AWS CodeCommit repository for each project, use the master branch for production code,and create a testing branch for code deployed to testing. Use feature branches to develop new featuresand pull requests to merge code to testing and master branches.</choice>
            <choice>B. Create another S3 bucket for each project for testing code, and use an AWS Lambda function topromote code changes between testing and production buckets. Enable versioning on all buckets toprevent code conflicts.</choice>
            <choice>C. Create an AWS CodeCommit repository for each project, and use the master branch for production andtest code with different deployment pipelines for each environment. Use feature branches to developnew features.</choice>
            <choice>D. Enable versioning and branching on each S3 bucket, use the master branch for production code, andcreate a testing branch for code deployed to testing. Have Developers use each branch for developingin each environment.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>After presenting a working proof of concept for a new application that uses AWS API Gateway, a Developermust set up a team development environment for the project. Due to a tight timeline, the Developer wantsto minimize time spent on infrastructure setup, and would like to reuse the code repository created for theproof of concept. Currently, all source code is stored in AWS CodeCommit.Company policy mandates having alpha, beta, and production stages with separate Jenkins servers to buildcode and run tests for every stage. The Development Manager must have the ability to block codepropagation between admins at any time. The Security team wants to make sure that users will not be ableto modify the environment without permission.How can this be accomplished?</question>
        <choices>
            <choice>A. Create API Gateway alpha, beta, and production stages. Create a CodeCommit trigger to deploy codeto the different stages using an AWS Lambda function.</choice>
            <choice>B. Create API Gateway alpha, beta, and production stages. Create an AWS CodePipeline that pulls codefrom the CodeCommit repository. Create CodePipeline actions to deploy code to the API Gatewaystages.</choice>
            <choice>C. Create Jenkins servers for the alpha, beta, and production stages on Amazon EC2 instances. Createmultiple CodeCommit triggers to deploy code to different stages using an AWS Lambda function.</choice>
            <choice>D. Create an AWS CodePipeline pipeline that pulls code from the CodeCommit repository. Create alpha,beta, and production stages with Jenkins servers on CodePipeline.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>An online company uses Amazon EC2 Auto Scaling extensively to provide an excellent customerexperience while minimizing the number of running EC2 instances. The company’s self-hosted Puppetenvironment in the application layer manages the configuration of the instances. The IT manager wants thelowest licensing costs and wants to ensure that whenever the EC2 Auto Scaling group scales down,removed EC2 instances are deregistered from the Puppet master as soon as possible.How can the requirement be met?</question>
        <choices>
            <choice>A. At instance launch time, use EC2 user data to deploy the AWS CodeDeploy agent. Use CodeDeploy toinstall the Puppet agent. When the Auto Scaling group scales out, run a script to register the newlydeployed instances to the Puppet master. When the Auto Scaling group scales in, use the EC2 AutoScaling EC2_INSTANCE_TERMINATING lifecycle hook to trigger de-registration from the Puppetmaster.</choice>
            <choice>B. Bake the AWS CodeDeploy agent into the base AMI. When the Auto Scaling group scales out, useCodeDeploy to install the Puppet agent, and execute a script to register the newly deployed instances tothe Puppet master. When the Auto Scaling group scales in, use the CodeDeploy ApplicationStoplifecycle hook to run a script to de-register the instance from the Puppet master.</choice>
            <choice>C. At instance launch time, use EC2 user data to deploy the AWS CodeDeploy agent. When the AutoScaling group scales out, use CodeDeploy to install the Puppet agent, and run a script to register thenewly deployed instances to the Puppet master. When the Auto Scaling group scales in, use the EC2user data instance stop script to run a script to de-register the instance from the Puppet master.</choice>
            <choice>D. Bake the AWS Systems Manager agent into the base AMI. When the Auto Scaling group scales out,use the AWS Systems Manager to install the Puppet agent, and run a script to register the newlydeployed instances to the Puppet master. When the Auto Scaling group scales in, use the SystemsManager instance stop lifecycle hook to run a script to de-register the instance from the Puppet master.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A company discovers that some IAM users have been storing their AWS access keys in configuration filesthat have been pushed to a Git repository hosting service.Which solution will require the LEAST amount of management overhead while preventing the exposedAWS access keys from being used?</question>
        <choices>
            <choice>A. Build an application that will create a list of all AWS access keys in the account and search each key onGit repository hosting services. If a match is found, configure the application to disable the associatedaccess key. Then deploy the application to an AWS Elastic Beanstalk worker environment and define aperiodic task to invoke the application every hour.</choice>
            <choice>B. Use Amazon Inspector to detect when a key has been exposed online. Have Amazon Inspector send anotification to an Amazon SNS topic when a key has been exposed. Create an AWS Lambda functionsubscribed to the SNS topic to disable the IAM user to whom the key belongs, and then delete the keyso that it cannot be used.</choice>
            <choice>C. Configure AWS Trusted Advisor and create an Amazon CloudWatch Events rule that uses TrustedAdvisor as the event source. Configure the CloudWatch Events rule to invoke an AWS Lambda functionas the target. If the Lambda function finds the exposed access keys, then have it disable the access keyso that it cannot be used.</choice>
            <choice>D. Create an AWS Config rule to detect when a key is exposed online. Haw AWS Config send changenotifications to an SNS topic. Configure an AWS Lambda function that is subscribed to the SNS topic tocheck the notification sent by AWS Config, and then disable the access key so it cannot be used.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>Company policies require that information about IP traffic going between instances in the productionAmazon VPC is captured. The capturing mechanism must always be enabled and the Security team mustbe notified when any changes in configuration occur.What should be done to ensure that these requirements are met?</question>
        <choices>
            <choice>A. Using the UserData section of an AWS CloudFormation template, install tcpdump on everyprovisioned Amazon EC2 instance. The output of the tool is sent to Amazon EFS for aggregation andquerying. In addition, scheduling an Amazon CloudWatch Events rule calls an AWS Lambda function tocheck whether tcpdump is up and running and sends an email to the security organization when there isan exception.</choice>
            <choice>B. Create a flow log for the production VPC and assign an Amazon S3 bucket as a destination for delivery.Using Amazon S3 Event Notification, set up an AWS Lambda function that is triggered when a new logfile gets delivered. This Lambda function updates an entry in Amazon DynamoDB, which is periodicallychecked by scheduling an Amazon CloudWatch Events rule to notify security when logs have notarrived.</choice>
            <choice>C. Create a flow log for the production VPC. Create a new rule using AWS Config that is triggered byconfiguration changes of resources of type ‘EC2:VPC’. As part of configuring the rule, create an AWSLambda function that looks up flow logs for a given VPC. If the VPC flow logs are not configured, returna ‘NON_COMPLIANT’ status and notify the security organization.</choice>
            <choice>D. Configure a new trail using AWS CloudTrail service. Using the UserData section of an AWSCloudFormation template, install tcpdump on every provisioned Amazon EC2 instance. ConnectAmazon Athena to the CloudTrail and write an AWS Lambda function that monitors for a flow logdisable event. Once the CloudTrail entry has been spotted, alert the security organization.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer needs to deploy a scalable three-tier Node.js application in AWS. The application musthave zero downtime during deployments and be able to roll back to previous versions. Other applicationswill also connect to the same MySQL backend database.The CIO has provided the following guidance for logging:- Centrally view all current web access server logs.- Search and filter web and application logs in near-real time.- Retain log data for three months.How should these requirements be met?</question>
        <choices>
            <choice>A. Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic LoadBalancing and Auto Scaling. Create an Amazon RDS MySQL instance inside the Elastic Beanstalkstack. Configure the Elastic Beanstalk log options to stream logs to Amazon CloudWatch Logs. Setretention to 90 days.</choice>
            <choice>B. Deploy the application on Amazon EC2. Configure Elastic Load Balancing and Auto Scaling. Use anAmazon RDS MySQL instance for the database tier. Configure the application to store log files inAmazon S3. Use Amazon EMR to search and filter the data. Set an Amazon S3 lifecycle rule to expireobjects after 90 days.</choice>
            <choice>C. Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic LoadBalancing and Auto Scaling. Create the Amazon RDS MySQL instance outside the Elastic Beanstalkstack. Configure the Elastic Beanstalk log options to stream logs to Amazon CloudWatch Logs. Setretention to 90 days.</choice>
            <choice>D. Deploy the application on Amazon EC2. Configure Elastic Load Balancing and Auto Scaling. Use anAmazon RDS MySQL instance for the database tier. Configure the application to load streaming logdata using Amazon Kinesis Data Firehouse into Amazon ES. Delete and create a new Amazon ESdomain every 90 days.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>An IT team has built an AWS CloudFormation template so others in the company can quickly and reliablydeploy and terminate an application. The template creates an Amazon EC2 instance with a user data scriptto install the application and an Amazon S3 bucket that the application uses to serve static webpages whileit is running.All resources should be removed when the CloudFormation stack is deleted. However, the team observesthat CloudFormation reports an error during stack deletion, and the S3 bucket created by the stack is notdeleted.How can the team resolve the error in the MOST efficient manner to ensure that all resources are deletedwithout errors?</question>
        <choices>
            <choice>A. Add DeletionPolicy attribute to the S3 bucket resource, with the value Delete forcing the bucket tobe removed when the stack is deleted.</choice>
            <choice>B. Add a custom resource when an AWS Lambda function with the DependsOn attribute specifying the S3bucket, and an IAM role. Writhe the Lambda function to delete all objects from the bucket when theRequestType is Delete.</choice>
            <choice>C. Identify the resource that was not deleted. From the S3 console, empty the S3 bucket and then delete it.</choice>
            <choice>D. Replace the EC2 and S3 bucket resources with a single AWS OpsWorks Stacks resource. Define acustom recipe for the stack to create and delete the EC2 instance and the S3 bucket.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer just joined a new company that is already running workloads on Amazon EC2instances. AWS has been adopted incrementally with no central governance. The Engineer must nowassess how well the existing deployments comply with the following requirements:EC2 instances are running only approved AMIs.Amazon EBS volumes are encrypted.EC2 instances have an Owner tag.Root login over SSH is disabled on EC2 instances.Which services should the Engineer use to perform this assessment with the LEAST amount of effort?(Choose two.)  </question>
        <choices>
            <choice>A.</choice>
            <choice>B.</choice>
            <choice>C.</choice>
            <choice>D.</choice>
            <choice>E. AWS ConfigAmazon GuardDutyAWS System ManagerAWS Directory ServiceAmazon Inspector </choice>
        </choices>
        <correctAnswers>A,E</correctAnswers>
    </exam>
    <exam>
        <question>A healthcare company has a critical application running in AWS. Recently, the company experienced somedown time. If it happens again, the company needs to be able to recover its application in another AWSRegion. The application uses Elastic Load Balancing and Amazon EC2 instances. The company alsomaintains a custom AMI that contains its application. This AMI is changed frequently.The workload is required to run in the primary region, unless there is a regional service disruption, in whichcase traffic should fail over to the new region. Additionally, the cost for the second region needs to be low.The RTO is 2 hours.Which solution allows the company to fail over to another region in the event of a failure, and also meet theabove requirements?</question>
        <choices>
            <choice>A. Maintain a copy of the AMI from the main region in the backup region. Create an Auto Scaling groupwith one instance using a launch configuration that contains the copied AMI. Use an Amazon Route 53record to direct traffic to the load balancer in the backup region in the event of failure, as required. Allowthe Auto Scaling group to scale out as needed during a failure.</choice>
            <choice>B. Automate the copying of the AMI in the main region to the backup region. Generate an AWS Lambdafunction that will create an EC2 instance from the AMI and place it behind a load balancer. Using thesame Lambda function, point the Amazon Route 53 record to the load balancer in the backup region.Trigger the Lambda function in the event of a failure.</choice>
            <choice>C. Place the AMI in a replicated Amazon S3 bucket. Generate an AWS Lambda function that can create alaunch configuration and assign it to an already created Auto Scaling group. Have one instance in thisAuto Scaling group ready to accept traffic. Trigger the Lambda function in the event of a failure. Use anAmazon Route 53 record and modify it with the same Lambda function to point to the load balancer inthe backup region.</choice>
            <choice>D. Automate the copying of the AMI to the backup region. Create an AWS Lambda function that can createa launch configuration and assign it to an already created Auto Scaling group. Set the Auto Scalinggroup maximum size to 0 and only increase it with the Lambda function during a failure. Trigger theLambda function in the event of a failure. Use an Amazon Route 53 record and modify it with the sameLambda function to point to the load balancer in the backup region.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A legacy web application stores access logs in a proprietary text format. One of the security requirements isto search application access events and correlate them with access data from many different systems.These searches should be near-real time.Which solution offloads the processing load on the application server and provides a mechanism to searchthe data in near-real time?</question>
        <choices>
            <choice>A. Install the Amazon CloudWatch Logs agent on the application server and use CloudWatch Events rulesto search logs for access events. Use Amazon CloudSearch as an interface to search for events.</choice>
            <choice>B. Use the third-party file-input plugin Logstash to monitor the application log file, then use a customdissect filter on the agent to parse the log entries into the JSON format. Output the events to AmazonES to be searched. Use the Elasticsearch API for querying the data.</choice>
            <choice>C. Upload the log files to Amazon S3 by using the S3 sync command. Use Amazon Athena to define thestructure of the data as a table, with Athena SQL queries to search for access events.</choice>
            <choice>D. Install the Amazon Kinesis Agent on the application server, configure it to monitor the log files, and sendit to a Kinesis stream. Configure Kinesis to transform the data by using an AWS Lambda function, andforward events to Amazon ES for analysis. Use the Elasticsearch API for querying the data.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A company runs a database on a single Amazon EC2 instance in a development environment. The data isstored on separate Amazon EBS volumes that are attached to the EC2 instance. An Amazon Route 53 Arecord has been created and configured to point to the EC2 instance. The company would like to automatethe recovery of the database instance when an instance or Availability Zone (AZ) fails. The company alsowants to keep its costs low. The RTO is 4 hours and RPO is 12 hours.Which solution should a DevOps Engineer implement to meet these requirements?</question>
        <choices>
            <choice>A. Run the database in an Auto Scaling group with a minimum and maximum instance count of 1 inmultiple AZs. Add a lifecycle hook to the Auto Scaling group and define an Amazon CloudWatch Eventsrule that is triggered when a lifecycle event occurs. Have the CloudWatch Events rule invoke an AWSLambda function to detach or attach the Amazon EBS data volumes from the EC2 instance based onthe event. Configure the EC2 instance UserData to mount the data volumes (retry on failure with a shortdelay), then start the database and update the Route 53 record.</choice>
            <choice>B. Run the database on two separate EC2 instances in different AZs with one active and the other as astandby. Attach the data volumes to the active instance. Configure an Amazon CloudWatch Events ruleto invoke an AWS Lambda function on EC2 instance termination. The Lambda function launches areplacement EC2 instance. If the terminated instance was the active node, then the function attachesthe data volumes to the standby node. Start the database and update the Route 53 record.</choice>
            <choice>C. Run the database in an Auto Scaling group with a minimum and maximum instance count of 1 inmultiple AZs. Create an AWS Lambda function that is triggered by a scheduled Amazon CloudWatchEvents rule every 4 hours to take a snapshot of the data volume and apply a tag. Have the instanceUserData get the latest snapshot, create a new volume from it, and attach and mount the volume. Thenstart the database and update the Route 53 record.</choice>
            <choice>D. Run the database on two separate EC2 instances in different AZs. Configure one of the instances as amaster and the other as a standby. Set up replication between the master and standby instances. Pointthe Route 53 record to the master. Configure an Amazon CloudWatch Events rule to invoke an AWSLambda function upon the EC2 instance termination. The Lambda function launches a replacementEC2 instance. If the terminated instance was the active node, the function promotes the standby tomaster and points the Route 53 record to it.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A consulting company was hired to assess security vulnerabilities within a client company’s application andpropose a plan to remediate all identified issues. The architecture is identified as follows: Amazon S3storage for content, an Auto Scaling group of Amazon EC2 instances behind an Elastic Load Balancer withattached Amazon EBS storage, and an Amazon RDS MySQL database. There are also several AWSLambda functions that communicate directly with the RDS database using connection string statements inthe code.The consultants identified the top security threat as follows: the application is not meeting its requirement tohave encryption at rest.What solution will address this issue with the LEAST operational overhead and will provide monitoring forpotential future violations?</question>
        <choices>
            <choice>A. Enable SSE encryption on the S3 buckets and RDS database. Enable OS-based encryption of data onEBS volumes. Configure Amazon Inspector agents on EC2 instances to report on insecure encryptionciphers. Set up AWS Config rules to periodically check for non-encrypted S3 objects.</choice>
            <choice>B. Configure the application to encrypt each file prior to storing on Amazon S3. Enable OS-basedencryption of data on EBS volumes. Encrypt data on write to RDS. Run cron jobs on each instance tocheck for encrypted data and notify via Amazon SNS. Use S3 Events to call an AWS Lambda functionand verify if the file is encrypted.</choice>
            <choice>C. Enable Secure Sockets Layer (SSL) on the load balancer, ensure that AWS Lambda is using SSL tocommunicate to the RDS database, and enable S3 encryption. Configure the application to force SSLfor incoming connections and configure RDS to only grant access if the session is encrypted. ConfigureAmazon Inspector agents on EC2 instances to report on insecure encryption ciphers.</choice>
            <choice>D. Enable SSE encryption on the S3 buckets, EBS volumes, and the RDS database. Store RDScredentials in EC2 Parameter Store. Enable a policy on the S3 bucket to deny unencrypted puts. Set upAWS Config rules to periodically check for non-encrypted S3 objects and EBS volumes, and to ensurethat RDS storage is encrypted.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A new zero-day vulnerability was found in OpenSSL requiring the immediate patching of a production webfleet running on Amazon Linux. Currently, OS updates are performed manually on a monthly basis anddeployed using updates to the production Auto Scaling Group’s launch configuration.Which method should a DevOps Engineer use to update packages in-place without downtime?</question>
        <choices>
            <choice>A. Use AWS CodePipline and AWS CodeBuild to generate new copies of these packages, and update theAuto Scaling group’s launch configuration.</choice>
            <choice>B. Use AWS Inspector to run “yum upgrade” on all running production instances, and manually update theAMI for the next maintenance window.</choice>
            <choice>C. Use Amazon EC2 Run Command to issue a package update command to all running productioninstances, and update the AMI for future deployments.</choice>
            <choice>D. Define a new AWS OpsWorks layer to match the running production instances, and use a recipe toissue a package update command to all running production instances.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A company runs a production application workload in a single AWS account that uses Amazon Route 53,AWS Elastic Beanstalk, and Amazon RDS. In the event of a security incident, the Security team wants theapplication workload to fail over to a new AWS account. The Security team also wants to block all access tothe original account immediately, with no access to any AWS resources in the original AWS account, duringforensic analysis.What is the most cost-effective way to prepare to fail over to the second account prior to a securityincident?</question>
        <choices>
            <choice>A. Migrate the Amazon Route 53 configuration to a dedicated AWS account. Mirror the Elastic Beanstalkconfiguration in a different account. Enable RDS Database Read Replicas in a different account.</choice>
            <choice>B. Migrate the Amazon Route 53 configuration to a dedicated AWS account. Save/copy the ElasticBeanstalk configuration files in a different AWS account. Copy snapshots of the RDS Database to adifferent account.</choice>
            <choice>C. Save/copy the Amazon Route 53 configurations for use in a different AWS account after an incident.Save/copy Elastic Beanstalk configuration files to a different account. Enable the RDS database readreplica in a different account.</choice>
            <choice>D. Save/copy the Amazon Route 53 configurations for use in a different AWS account after an incident.Mirror the configuration of Elastic Beanstalk in a different account. Copy snapshots of the RDSdatabase to a different account.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>Two teams are working together on different portions of an architecture and are using AWSCloudFormation to manage their resources. One team administers operating system-level updates andpatches, while the other team manages application-level dependencies and updates. The Application team must take the most recent AMI when creating new instances and deploying the application.What is the MOST scalable method for linking these two teams and processes?</question>
        <choices>
            <choice>A. The Operating System team uses CloudFormation to create new versions of their AMIs and lists theAmazon Resource names (ARNs) of the AMIs in an encrypted Amazon S3 object as part of the stackoutput section. The Application team uses a cross-stack reference to load the encrypted S3 object andobtain the most recent AMI ARNs.</choice>
            <choice>B. The Operating System team uses CloudFormation stack to create an AWS CodePipeline pipeline thatbuilds new AMIs, then places the latest AMI ARNs in an encrypted Amazon S3 object as part of thepipeline output. The Application team uses a cross-stack reference within their own CloudFormationtemplate to get that S3 object location and obtain the most recent AMI ARNs to use when deployingtheir application.</choice>
            <choice>C. The Operating System team uses CloudFormation stack to create an AWS CodePipeline pipeline thatbuilds new AMIs. The team then places the AMI ARNs as parameters in AWS Systems ManagerParameter Store as part of the pipeline output. The Application team specifies a parameter of type ssmin their CloudFormation stack to obtain the most recent AMI ARN from the Parameter Store.</choice>
            <choice>D. The Operating System team maintains a nested stack that includes both the operating system andApplication team templates. The Operating System team uses a stack update to deploy updates to theapplication stack whenever the Application team changes the application code.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>The Deployment team has grown substantially in recent months and so has the number of projects that useseparate code repositories. The current process involves configuring AWS CodePipeline manually, andthere have been service limit alerts for the count of Amazon S3 buckets.Which pipeline option will reduce S3 bucket sprawl alerts?</question>
        <choices>
            <choice>A. Combine the multiple separate code repositories into a single one, and deploy using a global AWSCodePipeline that has logic for each project.</choice>
            <choice>B. Create new pipelines by using the AWS API or AWS CLI, and configure them to use a single global S3bucket with separate prefixes for each project.</choice>
            <choice>C. Create a new pipeline in a different region for each project to bypass the service limits for S3 buckets ina single region.</choice>
            <choice>D. Create a new pipeline and for S3 bucket for each project by using the AWS API or AWS CLI to bypassthe service limits for S3 buckets in a single account.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A startup company is developing a web application on AWS. It plans to use Amazon RDS for persistenceand deploy the application to Amazon EC2 with an Auto Scaling group. The company would also like toseparate the environments for development, testing, and production.What is the MOST secure and flexible approach to manage the application configuration?</question>
        <choices>
            <choice>A. Create a property file to include the configuration and the encrypted passwords. Check in the propertyfile to the source repository, package the property file with the application, and deploy the application.Create an environment tag for the EC2 instances and tag the instances respectively. The application willextract the necessary property values based on the environment tag.</choice>
            <choice>B. Create a property file for each environment to include the environment-specific configuration and anencrypted password. Check in the property files to the source repository. During deployment, use onlythe environment-specific property file with the application. The application will read the needed propertyvalues from the deployed property file.</choice>
            <choice>C. Create a property file for each environment to include the environment-specific configuration. Create aprivate Amazon S3 bucket and save the property files in the bucket. Save the passwords in the bucketwith AWS KMS encryption. During deployment, the application will read the needed property valuesfrom the environment-specific property file in the S3 bucket.</choice>
            <choice>D. Create a property file for each environment to include the environment-specific configuration. Create aprivate Amazon S3 bucket and save the property files in the bucket. Save the encrypted passwords inthe AWS Systems Manager Parameter Store. Create an environment tag for the EC2 instances and tagthe instances respectively. The application will read the needed property values from the environment-specific property file in the S3 bucket and the parameter store.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer is using AWS CodeDeploy across a fleet of Amazon EC2 instances in an EC2 AutoScaling group. The associated CodeDeploy deployment group, which is integrated with EC2 Auto Scaling,is configured to perform in-place deployments with CodeDeployDefault.OneAtATime. During anongoing new deployment, the Engineer discovers that, although the overall deployment finishedsuccessfully, two out of five instances have the previous application revision deployed. The other threeinstances have the newest application revision.What is likely causing this issue?</question>
        <choices>
            <choice>A. The two affected instances failed to fetch the new deployment.</choice>
            <choice>B. A failed AfterInstall lifecycle event hook caused the CodeDeploy agent to roll back to the previousversion on the affected instances.</choice>
            <choice>C. The CodeDeploy agent was not installed in two affected instances.</choice>
            <choice>D. EC2 Auto Scaling launched two new instances while the new deployment had not yet finished, causingthe previous version to be deployed on the affected instances.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A company runs a three-tier web application in its production environment, which is built on a single AWSCloudFormation template made up of Amazon EC2 instances behind an ELB Application Load Balancer.The instances run in an EC2 Auto Scaling group across multiple Availability Zones. Data is stored in anAmazon RDS Multi-AZ DB instance with read replicas. Amazon Route 53 manages the application’s publicDNS record.A DevOps Engineer must create a workflow to mitigate a failed software deployment by rolling backchanges in the production environment when a software cutover occurs for new application software.What steps should the Engineer perform to meet these requirements with the LEAST amount of downtime?</question>
        <choices>
            <choice>A. Use CloudFormation to deploy an additional staging environment and configure the Route 53 DNS withweighted records. During cutover, change the Route 53 A record weights to achieve an even trafficdistribution between the two environments. Validate the traffic in the new environment and immediatelyterminate the old environment if tests are successful.</choice>
            <choice>B. Use a single AWS Elastic Beanstalk environment to deploy the staging and production environments.Update the environment by uploading the ZIP file with the new application code. Swap the ElasticBeanstalk environment CNAME. Validate the traffic in the new environment and immediately terminatethe old environment if tests are successful.</choice>
            <choice>C. Use a single AWS Elastic Beanstalk environment and an AWS OpsWorks environment to deploy the staging and production environments. Update the environment by uploading the ZIP file with the newapplication code into the Elastic Beanstalk environment deployed with the OpsWorks stack. Validate thetraffic in the new environment and immediately terminate the old environment if tests are successful.</choice>
            <choice>D. Use AWS CloudFormation to deploy an additional staging environment, and configure the Route 53DNS with weighted records. During cutover, increase the weight distribution to have more traffic directedto the new staging environment as workloads are successfully validated. Keep the old productionenvironment in place until the new staging environment handles all traffic.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A company wants to adopt a methodology for handling security threats from leaked and compromised IAMaccess keys. The DevOps Engineer has been asked to automate the process of acting upon compromisedaccess keys, which includes identifying users, revoking their permissions, and sending a notification to theSecurity team.Which of the following would achieve this goal?</question>
        <choices>
            <choice>A. Use the AWS Trusted Advisor generated security report for access keys. Use Amazon EMR to runanalytics on the report. Identify compromised IAM access keys and delete them. Use AmazonCloudWatch with an EMR Cluster State Change event to notify the Security team.</choice>
            <choice>B. Use AWS Trusted Advisor to identify compromised access keys. Create an Amazon CloudWatchEvents rule with Trusted Advisor as the event source, and AWS Lambda and Amazon SNS as targets.Use AWS Lambda to delete compromised IAM access keys and Amazon SNS to notify the Securityteam.</choice>
            <choice>C. Use the AWS Trusted Advisor generated security report for access keys. Use AWS Lambda to scanthrough the report. Use scan result inside AWS Lambda and delete compromised IAM access keys.Use Amazon SNS to notify the Security team.</choice>
            <choice>D. Use AWS Lambda with a third-party library to scan for compromised access keys. Use scan resultinside AWS Lambda and delete compromised IAM access keys. Create Amazon CloudWatch custommetrics for compromised keys. Create a CloudWatch alarm on the metrics to notify the Security team.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company wants to use Amazon ECS to provide a Docker container runtime environment. For compliancereasons, all Amazon EBS volumes used in the ECS cluster must be encrypted. Rolling updates will bemade to the cluster instances and the company wants the instances drained of all tasks before beingterminated.How can these requirements be met? (Choose two.)</question>
        <choices>
            <choice>A. Modify the default ECS AMI user data to create a script that executes docker rm –f {id} for allrunning container instances. Copy the script to the /etc/init.d/rc.d directory and execute chconfigenabling the script to run during operating system shutdown.</choice>
            <choice>B. Use AWS CodePipeline to build a pipeline that discovers the latest Amazon-provided ECS AMI, thencopies the image to an encrypted AMI outputting the encrypted AMI ID. Use the encrypted AMI ID whendeploying the cluster.</choice>
            <choice>C. Copy the default AWS CloudFormation template that ECS uses to deploy cluster instances. Modify thetemplate resource EBS configuration setting to set ‘Encrypted: True’ and include the AWS KMS alias:‘aws/ebs’ to encrypt the AMI.</choice>
            <choice>D. Create an Auto Scaling lifecycle hook backed by an AWS Lambda function that uses the AWS SDK tomark a terminating instance as DRAINING. Prevent the lifecycle hook from completing until the runningtasks on the instance are zero.</choice>
            <choice>E. Create an IAM role that allows the action ECS::EncryptedImage. Configure the AWS CLI and aprofile to use this role. Start the cluster using the AWS CLI providing the --use-encrypted-imageand --kms-key arguments to the create-cluster ECS command. </choice>
        </choices>
        <correctAnswers>C,D</correctAnswers>
    </exam>
    <exam>
        <question>A government agency has multiple AWS accounts, many of which store sensitive citizen information. ASecurity team wants to detect anomalous account and network activities (such as SSH brute force attacks)in any account and centralize that information in a dedicated security account. Event information should bestored in an Amazon S3 bucket in the security account, which is monitored by the department’s SecurityInformation and Even Manager (SIEM) system.How can this be accomplished?</question>
        <choices>
            <choice>A. Enable Amazon Macie in every account. Configure the security account as the Macie Administrator forevery member account using invitation/acceptance. Create an Amazon CloudWatch Events rule in thesecurity account to send all findings to Amazon Kinesis Data Firehouse, which should push the findingsto the S3 bucket.</choice>
            <choice>B. Enable Amazon Macie in the security account only. Configure the security account as the MacieAdministrator for every member account using invitation/acceptance. Create an Amazon CloudWatchEvents rule in the security account to send all findings to Amazon Kinesis Data Streams. Write andapplication using KCL to read data from the Kinesis Data Streams and write to the S3 bucket.</choice>
            <choice>C. Enable Amazon GuardDuty in every account. Configure the security account as the GuardDutyAdministrator for every member account using invitation/acceptance. Create an Amazon CloudWatchrule in the security account to send all findings to Amazon Kinesis Data Firehouse, which will push thefindings to the S3 bucket.</choice>
            <choice>D. Enable Amazon GuardDuty in the security account only. Configure the security account as theGuardDuty Administrator for every member account using invitation/acceptance. Create an AmazonCloudWatch rule in the security account to send all findings to Amazon Kinesis Data Streams. Write andapplication using KCL to read data from Kinesis Data Streams and write to the S3 bucket.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>An AWS CodePipeline pipeline has implemented a code release process. The pipeline is integrated withAWS CodeDeploy to deploy versions of an application to multiple Amazon EC2 instances for eachCodePipeline stage.During a recent deployment, the pipeline failed due to a CodeDeploy issue. The DevOps team wants toimprove monitoring and notifications during deployment to decrease resolution times.What should the DevOps Engineer do to create notifications when issues are discovered?</question>
        <choices>
            <choice>A. Implement AWS CloudWatch Logs for CodePipeline and CodeDeploy, create an AWS Config rule toevaluate code deployment issues, and create an Amazon SNS topic to notify stakeholders ofdeployment issues.</choice>
            <choice>B. Implement AWS CloudWatch Events for CodePipeline and CodeDeploy, create an AWS Lambdafunction to evaluate code deployment issues, and create an Amazon SNS topic to notify stakeholders ofdeployment issues.</choice>
            <choice>C. Implement AWS CloudTrail to record CodePipeline and CodeDeploy API call information, create anAWS Lambda function to evaluate code deployment issues, and create an Amazon SNS topic to notifystakeholders of deployment issues.</choice>
            <choice>D. Implement AWS CloudWatch Events for CodePipeline and CodeDeploy, create an Amazon Inspectorassessment target to evaluate code deployment issues, and create an Amazon SNS topic to notifystakeholders of deployment issues.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company runs an application on Amazon EC2 instances behind an Application Load Balancer. Theinstances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones in us-east1. Theapplication stores data in an Amazon RDS MySQL Multi-AZ DB instance.A DevOps Engineer wants to modify the current solution and create a hot standby of the environment inanother region to minimize downtime if a problem occurs in us-east-1.Which combination of steps should the DevOps Engineer take to meet these requirements? (Choosethree.)</question>
        <choices>
            <choice>A. Add a health check to the Amazon Route 53 alias record to evaluate the health of the primary region.Use AWS Lambda, configured with an Amazon CloudWatch Events trigger, to elect the Amazon RDSmaster in the disaster recovery region.</choice>
            <choice>B. Create a new Application Load Balancer and Amazon EC2 Auto Scaling group in the disaster recoveryregion.</choice>
            <choice>C. Extend the current Amazon EC2 Auto Scaling group to the subnets in the disaster recovery region.</choice>
            <choice>D. Enable multi-region failover for the RDS configuration for the database instance.</choice>
            <choice>E. Deploy a read replica of the RDS instance in the disaster recovery region.F. Create an AWS Lambda function to evaluate the health of the primary region. If it fails, modify theAmazon Route 53 record to point at the disaster recovery region and promote the RDS read replica. </choice>
        </choices>
        <correctAnswers>A,B,E</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer needs to design and implement a backup mechanism for Amazon EFS. The Engineeris given the following requirements:- The backup should run on schedule.- The backup should be stopped if the backup window expires.- The backup should be stopped if the backup completes before the backup window.- The backup logs should be retained for further analysis.- The design should support highly available and fault-tolerant paradigms.- Administrators should be notified with backup metadata.Which design will meet these requirements?</question>
        <choices>
            <choice>A. Use AWS Lambda with an Amazon CloudWatch Events rule for scheduling the start/stop of backupactivity. Run backup scripts on Amazon EC2 in an Auto Scaling group. Use Auto Scaling lifecycle hooksand the SSM Run Command on EC2 for uploading backup logs to Amazon S3. Use Amazon SNS tonotify administrators with backup activity metadata.</choice>
            <choice>B. Use Amazon SWF with an Amazon CloudWatch Events rule for scheduling the start/stop of backupactivity. Run backup scripts on Amazon EC2 in an Auto Scaling group. Use Auto Scaling lifecycle hooksand the SSM Run Command on EC2 for uploading backup logs to Amazon Redshift. Use CloudWatchAlarms to notify administrators with backup activity metadata.</choice>
            <choice>C. Use AWS Data Pipeline with an Amazon CloudWatch Events rule for scheduling the start/stop ofbackup activity. Run backup scripts on Amazon EC2 in a single Availability Zone. Use Auto Scalinglifecycle hooks and the SSM Run Command on EC2 for uploading the backup logs to Amazon RDS.Use Amazon SNS to notify administrators with backup activity metadata.</choice>
            <choice>D. Use AWS CodePipeline with an Amazon CloudWatch Events rule for scheduling the start/stop ofbackup activity. Run backup scripts on Amazon EC2 in a single Availability Zone. Use Auto Scalinglifecycle hooks and the SSM Run Command on Amazon EC2 for uploading backup logs to Amazon S3.Use Amazon SES to notify admins with backup activity metadata.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A rapidly growing company wants to scale for Developer demand for AWS development environments.Development environments are created manually in the AWS Management Console. The Networking teamuses AWS CloudFormation to manage the networking infrastructure, exporting stack output values for theAmazon VPC and all subnets. The development environments have common standards, such asApplication Load Balancers, Amazon EC2 Auto Scaling groups, security groups, and Amazon DynamoDBtables.To keep up with the demand, the DevOps Engineer wants to automate the creation of developmentenvironments. Because the infrastructure required to support the application is expected to grow, theremust be a way to easily update the deployed infrastructure. CloudFormation will be used to create atemplate for the development environments.Which approach will meet these requirements and quickly provide consistent AWS environments forDevelopers?</question>
        <choices>
            <choice>A. Use Fn:ImportValue intrinsic functions in the Resources section of the template to retrieve VirtualPrivate Cloud (VPC) and subnet values. Use CloudFormation StackSets for the developmentenvironments, using the Count input parameter to indicate the number of environments needed. usethe UpdateStackSet command to update existing development environments.</choice>
            <choice>B. Use nested stacks to define common infrastructure components. To access the exported values, useTemplateURL to reference the Networking team’s template. To retrieve Virtual Private Cloud (VPC)and subnet values, use Fn::ImportValue intrinsic functions in the Parameters section of the mastertemplate. Use the CreateChangeSet and ExecuteChangeSet commands to update existingdevelopment environments.</choice>
            <choice>C. Use nested stacks to define common infrastructure components. Use Fn::ImportValue intrinsicfunctions with the resources of the nested stack to retrieve Virtual Private Cloud (VPC) and subnetvalues. Use the CreateChangeSet and ExecuteChangeSet commands to update existingdevelopment environments.</choice>
            <choice>D. Use Fn:ImportValue intrinsic functions in the Parameters section of the master template to retrieveVirtual Private Cloud (VPC) and subnet values. Define the development resources in the order theyneed to be created in the CloudFormation nested stacks. Use the CreateChangeSet andExecuteChangeSet commands to update existing development environments.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A company has a website in an AWS Elastic Beanstalk load balancing and automatic scaling environment.This environment has an Amazon RDS MySQL instance configured as its database resource. After asudden increase in traffic, the website started dropping traffic. An administrator discovered that theapplication on some instances is not responding as the result of out-of-memory errors. Classic LoadBalancer marked those instances as out of service, and the health status of Elastic Beanstalk enhancedhealth reporting is degraded. However, Elastic Beanstalk did not replace those instances. Because of thediminished capacity behind the Classic Load Balancer, the application response times are slower for thecustomers.Which action will permanently fix this issue?</question>
        <choices>
            <choice>A. Clone the Elastic Beanstalk environment. When the new environment is up, swap CNAME andterminate the earlier environment.</choice>
            <choice>B. Temporarily change the maximum number of instances in the Auto Scaling group to allow the group tosupport more traffic.</choice>
            <choice>C. Change the setting for the Auto Scaling group health check from Amazon EC2 to Elastic LoadBalancing, and increase the capacity of the group.</choice>
            <choice>D. Write a cron script for restraining the web server process when memory is full, and deploy it with AWSSystems Manager.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer is launching a new application that will be deployed using Amazon Route 53, anApplication Load Balancer, Auto Scaling, and Amazon DynamoDB. One of the key requirements of thislaunch is that the application must be able to scale to meet a sudden load increase. During periods of lowusage, the infrastructure components must scale down to optimize cost.What steps can the DevOps Engineer take to meet the requirements? (Choose two.)</question>
        <choices>
            <choice>A. Use AWS Trusted Advisor to submit limit increase requests for the Amazon EC2 instances that will beused by the infrastructure.</choice>
            <choice>B. Determine which Amazon EC2 instance limits need to be raised by leveraging AWS Trusted Advisor,and submit a request to AWS Support to increase those limits.</choice>
            <choice>C. Enable Auto Scaling for the DynamoDB tables that are used by the application.</choice>
            <choice>D. Configure the Application Load Balancer to automatically adjust the target group based on the currentload.</choice>
            <choice>E. Create an Amazon CloudWatch Events scheduled rule that runs every 5 minutes to track the currentuse of the Auto Scaling group. If usage has changed, trigger a scale-up event to adjust the capacity. Dothe same for DynamoDB read and write capacities.</choice>
        </choices>
        <correctAnswers>B,C</correctAnswers>
    </exam>
    <exam>
        <question>A company hosts parts of a Python-based application using AWS Elastic Beanstalk. An Elastic BeanstalkCLI is being used to create and update the environments. The Operations team detected an increase inrequests in one of the Elastic Beanstalk environments that caused downtime overnight. The team notedthat the policy used for AWS Auto Scaling is NetworkOut. Based on load testing metrics, the teamdetermined that the application needs to scale CPU utilization to improve the resilience of theenvironments. The team wants to implement this across all environments automatically.Following AWS recommendations, how should this automation be implemented?</question>
        <choices>
            <choice>A. Using ebextensions, place a command within the container_commands key to perform an API call tomodify the scaling metric to CPUUtilization for the Auto Scaling configuration. Use leader_onlyto execute this command in only the first instance launched within the environment.</choice>
            <choice>B. Using ebextensions, create a custom resource that modifies the AWSEBAutoScalingScaleUpPolicyand AWSEBAutoScalingScaleDownPolicy resources to use CPUUtilization as a metric to scalefor the Auto Scaling group.</choice>
            <choice>C. Using ebextensions, configure the option setting MeasureName to CPUUtilization within theaws:autoscaling:trigger namespace.</choice>
            <choice>D. Using ebextensions, place a script within the files key and place it in /opt/elasticbeanstalk/hooks/appdeploy/pre to perform an API call to modify the scaling metric to CPUUtilization forthe Auto Scaling configuration. Use leader_only to place this script in only the first instance launchedwithin the environment.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps team needs to query information in application logs that are generated by an application runningmultiple Amazon EC2 instances deployed with AWS Elastic Beanstalk.Instance log streaming to Amazon CloudWatch Logs was enabled on Elastic Beanstalk.Which approach would be the MOST cost-efficient?</question>
        <choices>
            <choice>A. Use a CloudWatch Logs subscription to trigger an AWS Lambda function to send the log data to anAmazon Kinesis Data Firehouse stream that has an Amazon S3 bucket destination. Use AmazonAthena to query the log data from the bucket.</choice>
            <choice>B. Use a CloudWatch Logs subscription to trigger an AWS Lambda function to send the log data to anAmazon Kinesis Data Firehouse stream that has an Amazon S3 bucket destination. Use a new AmazonRedshift cluster and Amazon Redshift Spectrum to query the log data from the bucket.</choice>
            <choice>C. Use a CloudWatch Logs subscription to send the log data to an Amazon Kinesis Data Firehouse streamthat has an Amazon S3 bucket destination. Use Amazon Athena to query the log data from the bucket.</choice>
            <choice>D. Use a CloudWatch Logs subscription to send the log data to an Amazon Kinesis Data Firehouse streamthat has an Amazon S3 bucket destination. Use a new Amazon Redshift cluster and Amazon RedshiftSpectrum to query the log data from the bucket.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A company’s web application will be migrated to AWS. The application is designed so that there is noserver-side code required. As part of the migration, the company would like to improve the security of theapplication by adding HTTP response headers, following the Open Web Application Security Project(OWASP) secure headers recommendations.How can this solution be implemented to meet the security requirements using best practices?</question>
        <choices>
            <choice>A. Use an Amazon S3 bucket configured for website hosting, then set up server access logging on the S3bucket to track user activity. Then configure the static website hosting and execute a scheduled AWSLambda function to verify, and if missing, add security headers to the metadata.</choice>
            <choice>B. Use an Amazon S3 bucket configured for website hosting, then set up server access logging on the S3bucket to track user activity. Configure the static website hosting to return the required security headers.</choice>
            <choice>C. Use an Amazon S3 bucket configured for website hosting. Create an Amazon CloudFront distributionthat refers to this S3 bucket, with the origin response event set to trigger a Lambda@Edge Node.jsfunction to add in the security headers.</choice>
            <choice>D. Use an Amazon S3 bucket configured for website hosting. Create an Amazon CloudFront distributionthat refers to this S3 bucket. Set “Cache Based on Selected Request Headers” to “Whitelist,” and addthe security headers into the whitelist.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>An e-commerce company is running a web application in an AWS Elastic Beanstalk environment. In recentmonths, the average load of the Amazon EC2 instances has been increased to handle more traffic.The company would like to improve the scalability and resilience of the environment. The Developmentteam has been asked to decouple long-running tasks from the environment if the tasks can be executedasynchronously. Examples of these tasks include confirmation emails when users are registered to theplatform, and processing images or videos. Also, some of the periodic tasks that are currently runningwithin the web server should be offloaded.What is the most time-efficient and integrated way to achieve this?</question>
        <choices>
            <choice>A. Create an Amazon SQS queue and send the tasks that should be decoupled from the Elastic Beanstalkweb server environment to the SQS queue. Create a fleet of EC2 instances under an Auto Scalinggroup. Use an AMI that contains the application to process the asynchronous tasks, configure theapplication to listen for messages within the SQS queue, and create periodic tasks by placing those intothe cron in the operating system. Create an environment variable within the Elastic Beanstalkenvironment with a value pointing to the SQS queue endpoint.</choice>
            <choice>B. Create a second Elastic Beanstalk worker tier environment and deploy the application to process theasynchronous tasks there. Send the tasks that should be decoupled from the original Elastic Beanstalkweb server environment to the auto-generated Amazon SQS queue by the Elastic Beanstalk workerenvironment. Place a cron.yaml file within the root of the application source bundle for the workerenvironment periodic tasks. Use environment links to link the web server environment with the workerenvironment.</choice>
            <choice>C. Create a second Elastic Beanstalk web server tier environment and deploy the application to processthe asynchronous tasks. Send the tasks that should be decoupled from the original Elastic Beanstalkweb server to the auto-generated Amazon SQS queue by the Elastic Beanstalk web server tierenvironment. Place a cron.yaml file within the root of the application source bundle for the second webserver tier environment with the necessary periodic tasks. Use environment links to link both web serverenvironments.</choice>
            <choice>D. Create an Amazon SQS queue and send the tasks that should be decoupled from the Elastic Beanstalkweb server environment to the SQS queue. Create a fleet of EC2 instances under an Auto Scalinggroup. Install and configure the application to listen for messages within the SQS queue from UserDataand create periodic tasks by placing those into the cron in the operating system. Create an environmentvariable within the Elastic Beanstalk web server environment with a value pointing to the SQS queueendpoint.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A defect was discovered in production and a new sprint item has been created for deploying a hotfix.However, any code change must go through the following steps before going into production:Scan the code for security breaches, such as password and access key leaks.Run the code through extensive, long running unit tests.Which source control strategy should a DevOps Engineer use in combination with AWS CodePipeline tocomplete this process?</question>
        <choices>
            <choice>A. Create a hotfix tag on the last commit of the master branch. Trigger the development pipeline from thehotfix tag. Use AWS CodeDeploy with Amazon ECS to do a content scan and run unit tests. Add amanual approval stage that merges the hotfix tag into the master branch.</choice>
            <choice>B. Create a hotfix branch from the master branch. Triger the development pipeline from the hotfix branch.Use AWS CodeBuild to do a content scan and run unit tests. Add a manual approval stage that mergesthe hotfix branch into the master branch.</choice>
            <choice>C. Create a hotfix branch from the master branch. Triger the development pipeline from the hotfix branch.Use AWS Lambda to do a content scan and run unit tests. Add a manual approval stage that mergesthe hotfix branch into the master branch.</choice>
            <choice>D. Create a hotfix branch from the master branch. Create a separate source stage for the hotfix branch in the production pipeline. Trigger the pipeline from the hotfix branch. Use AWS Lambda to do a content scan and use AWS CodeBuild to run unit tests. Add a manual approval stage that merges the hotfixbranch into the master branch.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>The management team at a company with a large on-premises OpenStack environment wants to movenon-production workloads to AWS. An AWS Direct Connect connection has been provisioned andconfigured to connect the environments. Due to contractual obligations, the production workloads mustremain on-premises, and will be moved to AWS after the next contract negotiation. The company followsCenter for Internet Security (CIS) standards for hardening images; this configuration was developed usingthe company’s configuration management system.Which solution will automatically create an identical image in the AWS environment without significantoverhead?</question>
        <choices>
            <choice>A. Write an AWS CloudFormation template that will create an Amazon EC2 instance. Use cloud-unit toinstall the configuration management agent, use cfn-wait to wait for configuration management tosuccessfully apply, and use an AWS Lambda-backed custom resource to create the AMI.</choice>
            <choice>B. Log in to the console, launch an Amazon EC2 instance, and install the configuration managementagent. When changes are applied through the configuration management system, log in to the consoleand create a new AMI from the instance.</choice>
            <choice>C. Create a new AWS OpsWorks layer and mirror the image hardening standards. Use this layer as thebaseline for all AWS workloads.</choice>
            <choice>D. When a change is made in the configuration management system, a job in Jenkins is triggered to usethe VM Import command to create an Amazon EC2 instance in the Amazon VPC. Use lifecycle hooks tolaunch an AWS Lambda function to create the AMI.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps engineer is writing an AWS CloudFormation template to stand up a web service that will run onAmazon EC2 instances in a private subnet behind an ELB Application Load Balancer. The Engineer mustensure that the service can accept requests from clients that have IPv6 addresses.Which configuration items should the Engineer incorporate into the CloudFormation template to allow IPv6clients to access the web service?</question>
        <choices>
            <choice>A. Associate an IPv6 CIDR block with the Amazon VPC and subnets where the EC2 instances will live.Create route table entries for the IPv6 network, use EC2 instance types that support IPv6, and assignIPv6 addresses to each EC2 instance.</choice>
            <choice>B. Replace the Application Load Balancer with a Network Load Balancer. Associate an IPv6 CIDR blockwith the Virtual Private Cloud (VPC) and subnets where the Network Load Balancer lives, and assignthe Network Load Balancer an IPv6 Elastic IP address.</choice>
            <choice>C. Assign each EC2 instance an IPv6 Elastic IP address. Create a target group and add the EC2 instancesas targets. Create a listener on port 443 of the Application Load Balancer, and associate the newlycreated target group as the default target group.</choice>
            <choice>D. Create a target group and add the EC2 instances as targets. Create a listener on port 443 of theApplication Load Balancer. Associate the newly created target group as the default target group. Selecta dual stack IP address, and create a rule in the security group that allows inbound traffic fromanywhere.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A Security team is concerned that a Developer can unintentionally attach an Elastic IP address to anAmazon EC2 instance in production. No Developer should be allowed to attach an Elastic IP address to aninstance. The Security team must be notified if any production server has an Elastic IP address at any time.How can this task be automated?</question>
        <choices>
            <choice>A. Use Amazon Athena to query AWS CloudTrail logs to check for any associate-address attempts. Createan AWS Lambda function to dissociate the Elastic IP address from the instance, and alert the Securityteam.</choice>
            <choice>B. Attach an IAM policy to the Developer’s IAM group to deny associate-address permissions. Create acustom AWS Config rule to check whether an Elastic IP address is associated with any instance taggedas production, and alert the Security team.</choice>
            <choice>C. Ensure that all IAM groups are associated with Developers do not have associate-address permissions.Create a scheduled AWS Lambda function to check whether an Elastic IP address is associated withany instance tagged as production, and alert the Security team if an instance has an Elastic IP addressassociated with it.</choice>
            <choice>D. Create an AWS Config rule to check that all production instances have the EC2 IAM roles that includedeny associate-address permissions. Verify whether there is an Elastic IP address associated with anyinstance, and alert the Security team if an instance has an Elastic IP address associated with it.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company has developed a Node.js web application which provides REST services to store and retrievetime series data. The web application is built by the Development team on company laptops, tested locally,and manually deployed to a single on-premises server, which accesses a local MySQL database. Thecompany is starting a trial in two weeks, during which the application will undergo frequent updates basedon customer feedback. The following requirements must be met:- The team must be able to reliably build, test, and deploy new updates on a daily basis, without downtimeor degraded performance.- The application must be able to scale to meet an unpredictable number of concurrent users during thetrial.Which action will allow the team to quickly meet these objectives?</question>
        <choices>
            <choice>A. Create two Amazon Lightsail virtual private servers for Node.js; one for test and one for production.Build the Node.js application using existing process and upload it to the new Lightsail test server usingthe AWS CLI. Test the application, and if it passes all tests, upload it to the production server. Duringthe trial, monitor the production server usage, and if needed, increase performance by upgrading theinstance type.</choice>
            <choice>B. Develop an AWS CloudFormation template to create an Application Load Balancer and two AmazonEC2 instances with Amazon EBS (SSD) volumes in an Auto Scaling group with rolling updates enabled.Use AWS CodeBuild to build and test the Node.js application and store it in an Amazon S3 bucket. Useuser-data scripts to install the application and the MySQL database on each EC2 instance. Update thestack to deploy new application versions.</choice>
            <choice>C. Configure AWS Elastic Beanstalk to automatically build the application using AWS CodeBuild and todeploy it to a test environment that is configured to support auto scaling. Create a second ElasticBeanstalk environment for production. Use Amazon RDS to store data. When new versions of theapplications have passed all tests, use Elastic Beanstalk ‘swap cname’ to promote the test environmentto production.</choice>
            <choice>D. Modify the application to use Amazon DynamoDB instead of a local MySQL database. Use AWSOpsWorks to create a stack for the application with a DynamoDB layer, an Application Load Balancer layer, and an Amazon EC2 instance layer. Use a Chef recipe to build the application and a Chef recipeto deploy the application to the EC2 instance layer. Use custom health checks to run unit tests on eachinstance with rollback on failure.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer is developing a deployment strategy that will allow for data-driven decisions before afeature is fully approved for general availability. The current deployment process uses AWSCloudFormation and blue/green-style deployments. The development team has decided that customersshould be randomly assigned to groups, rather than using a set percentage, and redirects should beavoided.What process should be followed to implement the new deployment strategy?</question>
        <choices>
            <choice>A. Configure Amazon Route 53 weighted records for the blue and green stacks, with 50% of trafficconfigured to route to each stack.</choice>
            <choice>B. Configure Amazon CloudFront with an AWS Lambda@Edge function to set a cookie when CloudFrontreceives a request. Assign the user to a version A or B, and configure the web server to redirect toversion A or </choice>
            <choice>B.</choice>
            <choice>C. Configure Amazon CloudFront with an AWS Lambda@Edge function to set a cookie when CloudFrontreceives a request. Assign the user to a version A or B, then return the corresponding version to theviewer.</choice>
            <choice>D. Configure Amazon Route 53 with an AWS Lambda function to set a cookie when Amazon CloudFrontreceives a request. Assign the user to version A or B, then return the corresponding version to theviewer.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A company is testing a web application that runs on Amazon EC2 instances behind an Application LoadBalancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company usesa blue/green deployment process with immutable instances when deploying new software.During testing, users are being automatically logged out of the application at random times. Testers alsoreport that, when a new version of the application is deployed, all users are logged out. The Developmentteam needs a solution to ensure users remain logged in across scaling events and applicationdeployments.What is the MOST efficient way to ensure users remain logged in?</question>
        <choices>
            <choice>A. Enable smart sessions on the load balancer and modify the application to check for an existing session.</choice>
            <choice>B. Enable session sharing on the load balancer and modify the application to read from the session store.</choice>
            <choice>C. Store user session information in an Amazon S3 bucket and modify the application to read sessioninformation from the bucket.</choice>
            <choice>D. Modify the application to store user session information in an Amazon ElastiCache cluser.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A company is reviewing its IAM policies. One policy written by the DevOps Engineer has been flagged astoo permissive. The policy is used by an AWS Lambda function that issues a stop command to AmazonEC2 instances tagged with Environment: NonProduction over the weekend. The current policy is:  What changes should the Engineer make to achieve a policy of least permission? (Choose three.)</question>
        <choices>
            <choice>A.  </choice>
            <choice>B.</choice>
            <choice>C.  </choice>
            <choice>D.  </choice>
            <choice>E.F. </choice>
        </choices>
        <correctAnswers>B,D,E</correctAnswers>
    </exam>
    <exam>
        <question>A web application for healthcare services runs on Amazon EC2 instances behind an ELB Application LoadBalancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. ADevOps Engineer must create a mechanism in which an EC2 instance can be taken out of production so itssystem logs can be analyzed for issues to quickly troubleshot problems on the web tier.How can the Engineer accomplish this task while ensuring availability and minimizing downtime?</question>
        <choices>
            <choice>A. Implement EC2 Auto Scaling groups cooldown periods. Use EC2 instance metadata to determine theinstance state, and an AWS Lambda function to snapshot Amazon EBS volumes to preserve systemlogs.</choice>
            <choice>B. Implement Amazon CloudWatch Events rules. Create an AWS Lambda function that can react to aninstance termination to deploy the CloudWatch Logs agent to upload the system and access logs toAmazon S3 for analysis.</choice>
            <choice>C. Terminate the EC2 instances manually. The Auto Scaling service will upload all log information toCloudWatch Logs for analysis prior to instance termination.</choice>
            <choice>D. Implement EC2 Auto Scaling groups with lifecycle hooks. Create an AWS Lambda function that canmodify an EC2 instance lifecycle hook into a standby state, extract logs from the instance through aremote script execution, and place them in an Amazon S3 bucket for analysis.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A Development team creates a build project in AWS CodeBuild. The build project invokes automated testsof modules that access AWS services.Which of the following will enable the tests to run the MOST securely?</question>
        <choices>
            <choice>A. Generate credentials for an IAM user with a policy attached to allow the actions on AWS services. Storecredentials as encrypted environment variables for the build project. As part of the build script, obtainthe credentials to run the integration tests.</choice>
            <choice>B. Have CodeBuild run only the integration tests as a build job on a Jenkins server. Create a role that hasa policy attached to allow the actions on AWS services. Generate credentials for an IAM user that isallowed to assume the role. Configure the credentials as secrets in Jenkins, and allow the build job touse them to run the integration tests.</choice>
            <choice>C. Create a service role in IAM to be assumed by CodeBuild with a policy attached to allow the actions onAWS services. Configure the build project to use the role created.</choice>
            <choice>D. Use AWS managed credentials. Encrypt the credentials with AWS KMS. As part of the build script,decrypt with AWS KMS and use these credentials to run the integration tests.D7694BE00C90AA </choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A retail company wants to use AWS Elastic Beanstalk to host its online sales website running on Java.Since this will be the production website, the CTO has the following requirements for the deploymentstrategy:Zero downtime. While the deployment is ongoing, the current Amazon EC2 instances in service shouldremain in service. No deployment or any other action should be performed on the EC2 instancesbecause they serve production traffic.A new fleet of instances should be provisioned for deploying the new application version.Once the new application version is deployed successfully in the new fleet of instances, the newinstances should be placed in service and the old ones should be removed.The rollback should be as easy as possible. If the new fleet of instances fail to deploy the newapplication version, they should be terminated and the current instances should continue serving trafficas normal.The resources within the environment (EC2 Auto Scaling group, Elastic Load Balancing, ElasticBeanstalk DNS CNAME) should remain the same and no DNS change should be made.Which deployment strategy will meet the requirements?</question>
        <choices>
            <choice>A. Use rolling deployments with a fixed amount of one instance at a time and set the healthy threshold toOK.</choice>
            <choice>B. Use rolling deployments with additional batch with a fixed amount of one instance at a time and set thehealthy threshold to OK.</choice>
            <choice>C. launch a new environment and deploy the new application version there, then perform a CNAME swapbetween environments.</choice>
            <choice>D. Use immutable environment updates to meet all the necessary requirements.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A company is using AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline to deploy applicationsautomatically to an Amazon EC2 instance. A DevOps Engineer needs to perform a security assessmentscan of the operating system on every application deployment to the environment.How should this be automated?</question>
        <choices>
            <choice>A. Use Amazon CloudWatch Events to monitor for Auto Scaling event notifications of new instances andconfigure CloudWatch Events to trigger an Amazon Inspector scan.</choice>
            <choice>B. Use Amazon CloudWatch Events to monitor for AWS CodeDeploy notifications of a successful codedeployment and configure CloudWatch Events to trigger an Amazon Inspector scan.</choice>
            <choice>C. Use Amazon CloudWatch Events to monitor for CodePipeline notifications of a successful codedeployment and configure CloudWatch Events to trigger an AWS X-Ray scan.</choice>
            <choice>D. Use Amazon Inspector as a CodePipeline task after the successful use of CodeDeploy to deploy thecode to the systems.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company that uses electronic health records is running a fleet of Amazon EC2 instances with an AmazonLinux operating system. As part of patient privacy requirements, the company must ensure continuouscompliance for patches for operating system and applications running on the EC2 instances.How can the deployments of the operating system and application patches be automated using a defaultand custom repository?</question>
        <choices>
            <choice>A. Use AWS Systems Manager to create a new patch baseline including the custom repository. Executethe AWS-RunPatchBaseline document using the run command to verify and install patches.</choice>
            <choice>B. Use AWS Direct Connect to integrate the corporate repository and deploy the patches using AmazonCloudWatch scheduled events, then use the CloudWatch dashboard to create reports.</choice>
            <choice>C. Use yum-config-manager to add the custom repository under /etc/yum.repos.d and run yum-config-manager-enable to activate the repository.</choice>
            <choice>D. Use AWS Systems Manager to create a new patch baseline including the corporate repository. Executethe AWS-AmazonLinuxDefaultPatchBaseline document using the run command to verify and installpatches.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A company using AWS CodeCommit for source control wants to automate its continuous integration andcontinuous deployment pipeline on AWS in its development environment. The company has threerequirements:1. There must be a legal and a security review of any code change to make sure sensitive information isnot leaked through the source code.2. Every change must go through unit testing.3. Every change must go through a suite of functional testing to ensure functionality.In addition, the company has the following requirements for automation:1. Code changes should automatically trigger the CI/CD pipeline.2. Any failure in the pipeline should notify devops-admin@xyz.com.3. There must be an approval to stage the assets to Amazon S3 after tests have been performed.What should a DevOps Engineer do to meet all of these requirements while following CI/CD best practices?</question>
        <choices>
            <choice>A. Commit to the development branch and trigger AWS CodePipeline from the development branch. Makean individual stage in CodePipeline for security review, unit tests, functional tests, and manual approval.Use Amazon CloudWatch metrics to detect changes in pipeline stages and Amazon SES for emailingdevops-admin@xyz.com.</choice>
            <choice>B. Commit to mainline and trigger AWS CodePipeline from mainline. Make an individual stage inCodePipeline for security review, unit tests, functional tests, and manual approval. Use AWS CloudTraillogs to detect changes in pipeline stages and Amazon SNS for emailing devops-admin@xyz.com.</choice>
            <choice>C. Commit to the development branch and trigger AWS CodePipeline from the development branch. Makean individual stage in CodePipeline for security review, unit tests, functional tests, and manual approval.Use Amazon CloudWatch Events to detect changes in pipeline stages and Amazon SNS for emailingdevops-admin@xyz.com.</choice>
            <choice>D. Commit to mainline and trigger AWS CodePipeline from mainline. Make an individual stage inCodePipeline for security review, unit tests, functional tests, and manual approval. Use AmazonCloudWatch Events to detect changes in pipeline stages and Amazon SES for emailing devops-admin@xyz.com.8A3E48E222C4B4B15D7694BE00C90AAA </choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer uses Docker container technology to build an image-analysis application. Theapplication often sees spikes in traffic. The Engineer must automatically scale the application in response tocustomer demand while maintaining cost effectiveness and minimizing any impact on availability.What will allow the FASTEST response to spikes in traffic while fulfilling the other requirements?</question>
        <choices>
            <choice>A. Create an Amazon ECS cluster with the container instances in an Auto Scaling group. Configure theECS service to use Service Auto Scaling. Set up Amazon CloudWatch alarms to scale the ECS serviceand cluster.</choice>
            <choice>B. Deploy containers on an AWS Elastic Beanstalk Multicontainer Docker environment. Configure ElasticBeanstalk to automatically scale the environment based on Amazon CloudWatch metrics.</choice>
            <choice>C. Create an Amazon ECS cluster using Spot instances. Configure the ECS service to use Service AutoScaling. Set up Amazon CloudWatch alarms to scale the ECS service and cluster.</choice>
            <choice>D. Deploy containers on Amazon EC2 instances. Deploy a container scheduler to schedule containers ontoEC2 instances. Configure EC2 Auto Scaling for EC2 instances based on available Amazon CloudWatchmetrics.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer is building a multi-stage pipeline with AWS CodePipeline to build, verify, stage, test,and deploy an application. There is a manual approval stage required between the test and deploy stages.The development team uses a team chat tool with webhook support.How can the Engineer configure status updates for pipeline activity and approval requests to post to thechat tool?</question>
        <choices>
            <choice>A. Create an AWS CloudWatch Logs subscription that filters on “detail-type”: “CodePipeline PipelineExecution State Change.” Forward that to an Amazon SNS topic. Add the chat webhook URL to theSNS topic as a subscriber and complete the subscription validation.</choice>
            <choice>B. Create an AWS Lambda function that is triggered by the updating of AWS CloudTrail events. When a“CodePipeline Pipeline Execution State Change” event is detected in the updated events, send theevent details to the chat webhook URL.</choice>
            <choice>C. Create an AWS CloudWatch Events rule that filters on “CodePipeline Pipeline Execution State Change.”Forward that to an Amazon SNS topic. Subscribe an AWS Lambda function to the Amazon SNS topicand have it forward the event to the chat webhook URL.</choice>
            <choice>D. Modify the pipeline code to send event details to the chat webhook URL at the end of each stage.Parametrize the URL so each pipeline can send to a different URL based on the pipeline environment.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A company is beginning to move to the AWS Cloud. Internal customers are classified into two groupsaccording to their AWS skills: beginners and experts.The DevOps Engineer needs to build a solution to allow beginners to deploy a restricted set of AWSarchitecture blueprints expresses as AWS CloudFormation templates. Deployment should only be possibleon predetermined Virtual Private Clouds (VPCs). However, expert users should be able to deploy blueprintswithout constraints. Experts should also be able to access other AWS services, as needed.How can the Engineer implement a solution to meet these requirements with the LEAST amount of overhead?</question>
        <choices>
            <choice>A. Apply constraints to the parameters in the templates, limiting the VPCs available for deployments. Storethe templates on Amazon S3. Create an IAM group for beginners and give them access to thetemplates and CloudFormation. Create a separate group for experts, giving them access to thetemplates, CloudFormation, and other AWS services.</choice>
            <choice>B. Store the templates on Amazon S3. Use AWS Service Catalog to create a portfolio of products basedon those templates. Apply template constraints to the products with rules limiting VPCs available fordeployments. Create an IAM group for beginners giving them access to the portfolio. Create a separategroup for experts giving them access to the templates, CloudFormation, and other AWS services.</choice>
            <choice>C. Store the templates on Amazon S3. Use AWS Service Catalog to create a portfolio of products basedon those templates. Create an IAM role restricting VPCs available for creation of AWS resources. Applya launch constraint to the products using this role. Create an IAM group for beginners giving themaccess to the portfolio. Create a separate group for experts giving them access to the portfolio andother AWS services.</choice>
            <choice>D. Create two templates for each architecture blueprint where only one of them limits the VPC available fordeployments. Store the templates in Amazon DynamoDB. Create an IAM group for beginners givingthem access to the constrained templates and CloudFormation. Create a separate group for expertsgiving them access to the unconstrained templates, CloudFormation, and other AWS services.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer encountered the following error when attempting to use an AWS CloudFormationtemplate to create an Amazon ECS cluster:An error occurred (InsufficientCapabilitiesException) when calling theCreateStack operation.What caused this error and what steps need to be taken to allow the Engineer to successfully execute theAWS CloudFormation template?</question>
        <choices>
            <choice>A. The AWS user or role attempting to execute the CloudFormation template does not have thepermissions required to create the resources within the template. The Engineer must review the userpolicies and add any permissions needed to create the resources and then rerun the templateexecution.</choice>
            <choice>B. The AWS CloudFormation service cannot be reached and is not capable of creating the cluster. TheEngineer needs to confirm that routing and firewall rules are not preventing the AWS CloudFormationscript from communicating with the AWS service endpoints, and then rerun the template execution.</choice>
            <choice>C. The CloudFormation execution was not granted the capability to create IAM resources. The Engineerneeds to provide CAPABILITY_IAM and CAPABILITY_NAMED_IAM as capabilities in theCloudFormation execution parameters or provide the capabilities in the AWS Management Console.</choice>
            <choice>D. CloudFormation is not capable of fulfilling the request of the specified resources in the current AWSRegion. The Engineer needs to specify a new region and rerun the template.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A retail company is currently hosting a Java-based application in its on-premises data center. Managementwants the DevOps Engineer to move this application to AWS. Requirements state that while keeping highavailability, infrastructure management should be as simple as possible. Also, during deployments of newapplication versions, while cost is an important metric, the Engineer needs to ensure that at least half of thefleet is available to handle user traffic.What option requires the LEAST amount of management overhead to meet these requirements?</question>
        <choices>
            <choice>A. Create an AWS CodeDeploy deployment group and associate it with an Auto Scaling group configuredto launch instances across subnets in different Availability Zones. Configure an in-place deploymentwith a CodeDeploy.HalfAtAtime configuration for application deployments.</choice>
            <choice>B. Create an AWS Elastic Beanstalk Java-based environment using Auto Scaling and load balancing.Configure the network setting for the environment to launch instances across subnets in differentAvailability Zones. Use “Rolling with additional batch” as a deployment strategy with a batch size of 50%.</choice>
            <choice>C. Create an AWS CodeDeploy deployment group and associate it with an Auto Scaling group configuredto launch instances across subnets in different Availability Zones. Configure an in-place deploymentwith a custom deployment configuration with the MinimumHealthyHosts option set to typeFLEET_PERCENT and a value of 50.</choice>
            <choice>D. Create an AWS Elastic Beanstalk Java-based environment using Auto Scaling and load balancing.Configure the network options for the environment to launch instances across subnets in differentAvailability Zones. Use “Rolling” as a deployment strategy with a batch size of 50%.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A global company with distributed Development teams built a web application using a microservicesarchitecture running on Amazon ECS. Each application service is independent and runs as a service in theECS cluster. The container build files and source code reside in a private GitHub source code repository.Separate ECS clusters exist for development, testing, and production environments.Developers are required to push features to branches in the GitHub repository and then merge the changesinto an environment-specific branch (development, test, or production). This merge needs to trigger anautomated pipeline to run a build and a deployment to the appropriate ECS cluster.What should the DevOps Engineer recommend as an automated solution to these requirements?</question>
        <choices>
            <choice>A. Create an AWS CloudFormation stack for the ECS cluster and AWS CodePipeline services. Store thecontainer build files in an Amazon S3 bucket. Use a post-commit hook to trigger a CloudFormationstack update that deploys the ECS cluster. Add a task in the ECS cluster to build and push images toAmazon ECR, based on the container build files in S3.</choice>
            <choice>B. Create a separate pipeline in AWS CodePipeline for each environment. Trigger each pipeline based oncommits to the corresponding environment branch in GitHub. Add a build stage to launch AWSCodeBuild to create the container image from the build file and push it to Amazon ECR. Then addanother stage to update the Amazon ECS task and service definitions in the appropriate cluster for thatenvironment.</choice>
            <choice>C. Create a pipeline in AWS CodePipeline. Configure it to be triggered by commits to the master branch inGitHub. Add a stage to use the Git commit message to determine which environment the commit shouldbe applied to, then call the create-image Amazon ECR command to build the image, passing it to thecontainer build file. Then add a stage to update the ECS task and service definitions in the appropriatecluster for that environment.</choice>
            <choice>D. Create a new repository in AWS CodeCommit. Configure a scheduled project in AWS CodeBuild tosynchronize the GitHub repository to the new CodeCommit repository. Create a separate pipeline foreach environment triggered by changes to the CodeCommit repository. Add a stage using AWSLambda to build the container image and push to Amazon ECR. Then add another stage to update theECS task and service definitions in the appropriate cluster for that environment.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>For auditing, analytics, and troubleshooting purposes, a DevOps Engineer for a data analytics application needs to collect all of the application and Linux system logs from the Amazon EC2 instances beforetermination. The company, on average, runs 10,000 instances in an Auto Scaling group. The companyrequires the ability to quickly find logs based on instance IDs and date ranges.Which is the MOST cost-effective solution?</question>
        <choices>
            <choice>A. Create an EC2 Instance-terminate Lifecycle Action on the group, write a termination scriptfor pushing logs into Amazon S3, and trigger an AWS Lambda function based on S3 PUT to create acatalog of log files in an Amazon DynamoDB table with the primary key being Instance ID and sort keybeing Instance Termination Date.</choice>
            <choice>B. Create an EC2 Instance-terminate Lifecycle Action on the group, write a termination scriptfor pushing logs into Amazon CloudWatch Logs, create a CloudWatch Events rule to trigger an AWSLambda function to create a catalog of log files in an Amazon DynamoDB table with the primary keybeing Instance ID and sort key being Instance Termination Date.</choice>
            <choice>C. Create an EC2 Instance-terminate Lifecycle Action on the group, create an AmazonCloudWatch Events rule based on it to trigger an AWS Lambda function for storing the logs in AmazonS3, and create a catalog of log files in an Amazon DynamoDB table with the primary key being InstanceID and sort key being Instance Termination Date.</choice>
            <choice>D. Create an EC2 Instance-terminate Lifecycle Action on the group, push the logs intoAmazon Kinesis Data Firehouse, and select Amazon ES as the destination for providing storage andsearch capability.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer manages a large commercial website that runs on Amazon EC2. The website usesAmazon Kinesis Data Streams to collect and process web logs. The Engineer manages the Kinesisconsumer application, which also runs on EC2. Spikes of data cause the Kinesis consumer application tofall behind, and the streams drop records before they can be processed.What is the FASTEST method to improve stream handling?</question>
        <choices>
            <choice>A. Modify the Kinesis consumer application to store the logs durably in amazon S3. Use Amazon EMR toprocess the data directly on S3 to derive customer insights and store the results in S3.</choice>
            <choice>B. Horizontally scale the Kinesis consumer application by adding more EC2 instances based on theGetRecord.IteratorAgeMilliseconds Amazon CloudWatch metric. Increase the Kinesis DataStreams retention period.</choice>
            <choice>C. Convert the Kinesis consumer application to run as an AWS Lambda function. Configure the KinesisData Streams as the event source for the Lambda function to process the data streams.</choice>
            <choice>D. Increase the number of shards in the Kinesis Data Streams to increase the overall throughput so thatthe consumer processes data faster.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer must automate a weekly process of identifying unnecessary permissions on a per-userbasis, across all users in an AWS account. This process should evaluate the permissions currently grantedto each user by examining the user’s attached IAM access policies compared to the permissions the userhas actually used in the past 90 days. Any differences in the comparison would indicate that the user hasmore permissions than are required. A report of the deltas should be sent to the Information Security teamfor further review and IAM user access policy revisions, as required.Which solution is fully automated and will produce the MOST detailed deltas report?</question>
        <choices>
            <choice>A. Create an AWS Lambda function that calls the IAM Access Advisor API to pull service permissionsgranted on a user-by-user basis for all users in the AWS account. Ensure that Access Advisor isconfigured with a tracking period of 90 days. Invoke the Lambda function using an Amazon CloudWatchEvents rule on a weekly schedule. For each record, by user, by service, if the Access Advisor LastAccesses field indicates a day count instead of “Not accesses in the tracking period,” this indicates adelta compared to what is in the user’s currently attached access polices. After Lambda has iteratedthrough all users in the AWS account, configure it to generate a report and send the report usingAmazon SES.</choice>
            <choice>B. Configure an AWS CloudTrail trail that spans all AWS Regions and all read/write events, and point thistrail to an Amazon S3 bucket. Create Amazon Athena table and specify the S3 bucket ARN in theCREATE TABLE query. Create an AWS Lambda function that accesses the Athena table using theSDK, which performs a SELECT, ensuring that the WHERE clause includes userIdentity,eventName, and eventTime. Compare the results against the user’s currently attached IAM accesspolicies to determine any deltas. Configure an Amazon CloudWatch Events schedule to automate thisprocess to run once a week. Configure Amazon SES to send a consolidated report to the InformationSecurity team.</choice>
            <choice>C. Configure VPC Flow Logs on all subnets across all VPCs in all regions to capture user traffic across theentire account. Ensure that all logs are being sent to a centralized Amazon S3 bucket, so all flow logscan be consolidated and aggregated. Create an AWS Lambda function that is triggered once a week byan Amazon CloudWatch Events schedule. Ensure that the Lambda function parses the flow log files forthe following information: IAM user ID, subnet ID, VPC ID, Allow/Reject status per API call, and servicename. Then have the function determine the deltas on a user-by-user basis. Configure the Lambdafunction to send the consolidated report using Amazon SES.</choice>
            <choice>D. Create an Amazon ES cluster and note its endpoint URL, which will be provided as an environmentvariable into a Lambda function. Configure an Amazon S3 event on a AWS CloudTrail trail destinationS3 bucket and ensure that the event is configured to send to a Lambda function. Create the Lambdafunction to consume the events, parse the input from JSON, and transform it to an Amazon ESdocument format. POST the documents to the Amazon ES cluster’s endpoint by way of the passed-inenvironment variable. Make sure that the proper indexing exists in Amazon ES and use Apache Lucenequeries to parse the permissions on a user-by-user basis. Export the deltas into a report and haveAmazon ES send the reports to the Information Security team using Amazon SES every week.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A company is hosting a web application in an AWS Region. For disaster recovery purposes, a secondregion is being used as a standby. Disaster recovery requirements state that session data must bereplicated between regions in near-real time and 1% of requests should route to the secondary region tocontinuously verify system functionality. Additionally, if there is a disruption in service in the main region,traffic should be automatically routed to the secondary region, and the secondary region must be able toscale up to handle all traffic.How should a DevOps Engineer meet these requirements?</question>
        <choices>
            <choice>A. In both regions, deploy the application on AWS Elastic Beanstalk and use Amazon DynamoDB globaltables for session data. Use an Amazon Route 53 weighted routing policy with health checks todistribute the traffic across the regions.</choice>
            <choice>B. In both regions, launch the application in Auto Scaling groups and use DynamoDB for session data. Usea Route 53 failover routing policy with health checks to distribute the traffic across the regions.</choice>
            <choice>C. In both regions, deploy the application in AWS Lambda, exposed by Amazon API Gateway, and useAmazon RDS PostgreSQL with cross-region replication for session data. Deploy the web applicationwith client-side logic to call the API Gateway directly.</choice>
            <choice>D. In both regions, launch the application in Auto Scaling groups and use DynamoDB global tables forsession data. Enable an Amazon CloudFront weighted distribution across regions. Point the AmazonRoute 53 DNS record at the CloudFront distribution.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer manages an application that has a cross-region failover requirement. The applicationstores its data in an Amazon Aurora on Amazon RDS database in the primary region with a read replica inthe secondary region. The application uses Amazon Route 53 to direct customer traffic to the active region.Which steps should be taken to MINIMIZE downtime if a primary database fails?</question>
        <choices>
            <choice>A. Use Amazon CloudWatch to monitor the status of the RDS instance. In the event of a failure, use aCloudWatch Events rule to send a short message service (SMS) to the Systems Operator usingAmazon SNS. Have the Systems Operator redirect traffic to an Amazon S3 static website that displaysa downtime message. Promote the RDS read replica to the master. Confirm that the application isworking normally, then redirect traffic from the Amazon S3 website to the secondary region.</choice>
            <choice>B. Use RDS Event Notification to publish status updates to an Amazon SNS topic. Use an AWS Lambdafunction subscribed to the topic to monitor database health. In the event of a failure, the Lambdafunction promotes the read replica, then updates Route 53 to redirect traffic from the primary region tothe secondary region.</choice>
            <choice>C. Set up an Amazon CloudWatch Events rule to periodically invoke an AWS Lambda function that checksthe health of the primary database. If a failure is detected, the Lambda function promotes the readreplica. Then, update Route 53 to redirect traffic from the primary to the secondary region.</choice>
            <choice>D. Set up Route 53 to balance traffic between both regions equally. Enable the Aurora multi-master option,then set up a Route 53 health check to analyze the health of the databases. Configure Route 53 toautomatically direct all traffic to the secondary region when a primary database fails.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company is running an application on Amazon EC2 instances behind an ELB Application Load Balancer.The instances run in an EC2 Auto Scaling group across multiple Availability Zones.After a recent application update, users are getting HTTP 502 Bad Gateway errors from the applicationURL. The DevOps Engineer cannot analyze the problem because Auto Scaling is terminating all EC2instances shortly after launch for being unhealthy.What steps will allow the DevOps Engineer access to one of the unhealthy instances to troubleshoot thedeployed application?</question>
        <choices>
            <choice>A. Create an image from the terminated instance and create a new instance from that image. TheApplication team can then log into the new instance.</choice>
            <choice>B. As soon as a new instance is created by AutoScaling, put the instance into a Standby state as this willprevent the instance from being terminated.</choice>
            <choice>C. Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to theTerminating:Wait state.</choice>
            <choice>D. Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances frombeing terminated.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question> An application is running on Amazon EC2. It has an attached IAM role that is receiving an AccessDeniederror while trying to access a SecureString parameter resource in the AWS Systems Manager ParameterStore. The SecureString parameter is encrypted with a customer-managed Customer Master Key (CMK),What steps should the DevOps Engineer take to grant access to the role while granting least privilege?(Choose three.)  </question>
        <choices>
            <choice>A. Set ssm:GetParamter for the parameter resource in the instance role’s IAM policy.</choice>
            <choice>B. Set kms:Decrypt for the instance role in the customer-managed CMK policy.</choice>
            <choice>C. Set kms:Decrypt for the customer-managed CMK resource in the role’s IAM policy.</choice>
            <choice>D. Set ssm:DecryptParameter for the parameter resource in the instance role IAM policy.</choice>
            <choice>E. Set kms:GenerateDataKey for the user on the AWS managed SSM KMS key.</choice>
            <choice>F. Set kms:Decrypt for the parameter resource in the customer-managed CMK policy.</choice>
        </choices>
        <correctAnswers>A,B,C</correctAnswers>
    </exam>
    <exam>
        <question>An Application team is refactoring one of its internal tools to run in AWS instead of on-premises hardware.All of the code is currently written in Python and is standalone. There is also no external state store orrelational database to be queried.Which deployment pipeline incurs the LEAST amount of changes between development and production?</question>
        <choices>
            <choice>A. Developers should use Docker for local development. When dependencies are changed and a newcontainer is ready, use AWS CodePipeline and AWS CodeBuild to perform functional tests and thenupload the new container to Amazon ECR. Use AWS CloudFormation with the custom container todeploy the new Amazon ECS.</choice>
            <choice>B. Developers should use Docker for local development. Use AWS SMS to import these containers asAMIs for Amazon EC2 whenever dependencies are updated. Use AWS CodePipeline to test new codechanges against the Auto Scaling group.</choice>
            <choice>C. Developers should use their native Python environment. When Dependencies are changed and a newcontainer is ready, use AWS CodePipeline and AWS CodeBuild to perform functional tests and thenupload the new container to the Amazon ECR. Use AWS CloudFormation with the custom container todeploy the new Amazon ECS.</choice>
            <choice>D. Developers should use their native Python environment. When Dependencies are changed and a newcode is ready, use AWS CodePipeline and AWS CodeBuild to perform functional tests and then uploadthe new container to the Amazon ECR. Use CodePipeline and CodeBuild with the custom container totest new code changes inside AWS Elastic Beanstalk.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A company is using an AWS CodeBuild project to build and package an application. The packages arecopied to a shared Amazon S3 bucket before being deployed across multiple AWS accounts.The buildspec.yml file contains the following:The DevOps Engineer has noticed that anybody with an AWS account is able to download the artifacts.What steps should the DevOps Engineer take to stop this?</question>
        <choices>
            <choice>A. Modify the post_build to command to use –-acl public-read and configure a bucket policy thatgrants read access to the relevant AWS accounts only.</choice>
            <choice>B. Configure a default ACL for the S3 bucket that defines the set of authenticated users as the relevantAWS accounts only and grants read-only access.</choice>
            <choice>C. Create an S3 bucket policy that grants read access to the relevant AWS accounts and denies readaccess to the principal “*”</choice>
            <choice>D. Modify the post_build command to remove –-acl authenticated-read and configure a bucketpolicy that allows read access to the relevant AWS accounts only.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A web application has been deployed using an AWS Elastic Beanstalk application. The ApplicationDevelopers are concerned that they are seeing high latency in two different areas of the application:- HTTP client requests to a third-party API- MySQL client library queries to an Amazon RDS databaseA DevOps Engineer must gather trace data to diagnose the issues.Which steps will gather the trace information with the LEAST amount of changes and performance impactsto the application?</question>
        <choices>
            <choice>A. Add additional logging to the application code. Use the Amazon CloudWatch agent to stream theapplication logs into Amazon Elasticsearch Service. Query the log data in Amazon ES.</choice>
            <choice>B. Instrument the application to use the AWS X-Ray SDK. Post trace data to an Amazon ElasticsearchService cluster. Query the trace data for calls to the HTTP client and the MySQL client.</choice>
            <choice>C. On the AWS Elastic Beanstalk management page for the application, enable the AWS X-Ray daemon.View the trace data in the X-Ray console.</choice>
            <choice>D. Instrument the application using the AWS X-Ray SDK. On the AWS Elastic Beanstalk managementpage for the application, enable the X-Ray daemon. View the trace data in the X-Ray console.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>An Information Security policy requires that all publicly accessible systems be patched with critical OSsecurity patches within 24 hours of a patch release. All instances are tagged with the Patch Group key set to 0. Two new AWS Systems Manager patch baselines for Windows and Red Hat Enterprise Linux (RHEL)with zero-day delay for security patches of critical severity were created with an auto-approval rule. PatchGroup 0 has been associated with the new patch baselines.Which two steps will automate patch compliance and reporting? (Choose two.)</question>
        <choices>
            <choice>A. Create an AWS Systems Manager Maintenance Window and add a target with Patch Group 0. Add atask that runs the AWS-InstallWindowsUpdates document with a daily schedule.</choice>
            <choice>B. Create an AWS Systems Manager Maintenance Window with a daily schedule and add a target withPatch Group 0. Add a task that runs the AWS-RunPatchBaseline document with the Install action.</choice>
            <choice>C. Create an AWS Systems Manager State Manager configuration. Associate the AWS-RunPatchBaselinetask with the configuration and add a target with Patch Group 0.</choice>
            <choice>D. Create an AWS Systems Manager Maintenance Window and add a target with Patch Group 0. Add atask that runs the AWS-ApplyPatchBaseline document with a daily schedule.</choice>
            <choice>E. Use the AWS Systems Manager Run Command to associate the AWS-ApplyPatchBaseline documentwith instances tagged with Patch Group 0.</choice>
        </choices>
        <correctAnswers>B,C</correctAnswers>
    </exam>
    <exam>
        <question>A Security team requires all Amazon EBS volumes that are attached to an Amazon EC2 instance to haveAWS Key Management Service (AWS KMS) encryption enabled. If encryption is not enabled, thecompany's policy requires the EBS volume to be detached and deleted. A DevOps Engineer must automatethe detection and deletion of unencrypted EBS volumes.Which method should the Engineer use to accomplish this with the LEAST operational effort?</question>
        <choices>
            <choice>A. Create an Amazon CloudWatch Events rule that invokes an AWS Lambda function when an EBSvolume is created. The Lambda function checks the EBS volume for encryption. If encryption is notenabled and the volume is attached to an instance, the function deletes the volume.</choice>
            <choice>B. Create an AWS Lambda function to describe all EBS volumes in the region and identify volumes thatare attached to an EC2 instance without encryption enabled. The function then deletes all non-compliantvolumes. The AWS Lambda function is invoked every 5 minutes by an Amazon CloudWatch Eventsscheduled rule.</choice>
            <choice>C. Create a rule in AWS Config to check for unencrypted and attached EBS volumes. Subscribe an AWSLambda function to the Amazon SNS topic that AWS Config sends change notifications to. The Lambdafunction checks the change notification and deletes any EBS volumes that are non-compliant.</choice>
            <choice>D. Launch an EC2 instance with an IAM role that has permissions to describe and delete volumes. Run ascript on the EC2 instance every 5 minutes to describe all EBS volumes in all regions and identifyvolumes that are attached without encryption enabled. The script then deletes those volumes.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A company wants to implement a CI/CD pipeline for building and testing its mobile apps. A DevOpsEngineer has been given the following requirements:- Use AWS CodePipeline to orchestrate the workflow.- Test the application on real devices.- Trigger a notification.- Stage the application binary on a production bucket in a different account.- \Make the application binary publicly accessible.Which sequence of actions should the Engineer perform in the pipeline to meet the requirements?</question>
        <choices>
            <choice>A. Use AWS CodeCommit as the code source and AWS CodeDeploy to compile and package theapplication. Use CodeDeploy to deploy the application binary to an AWS Lambda function for testing.Use a third-party library on AWS Lambda to simulate the device platform. Allow a Lambda role toupload to the production Amazon S3 bucket. Make the binary publicly accessible. Trigger notificationsusing Amazon SNS.</choice>
            <choice>B. Use GitHub as the code source and AWS Lambda to compile and package the application. Use anotherLambda function to run unit tests and deliver the application binary to a development bucket. Use thebinary from the development bucket and install the application on a personal device for testing. Deliverthe binary to the production bucket after approval. Trigger notifications using Amazon SNS.</choice>
            <choice>C. Use an Amazon S3 bucket as the code source and AWS CodeBuild to compile and package theapplication. Use AWS CodeDeploy to deploy the application binary to a device farm for testing. Deliverthe binary to the production S3 bucket. Use an S3 bucket policy to allow public read on the productionS3 bucket. Trigger notifications using an Amazon CloudWatch Events rule with Amazon SNS.</choice>
            <choice>D. Use AWS CodeCommit as the code source and AWS CodeBuild to compile and package theapplication. Invoke an AWS Lambda function that uploads the application binary to a device farm fortesting. Deliver the binary to the production Amazon S3 bucket. Use an S3 bucket policy to allow publicread on the production S3 bucket. Trigger notifications by using an Amazon CloudWatch Events rule.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer is reviewing a system that uses Amazon EC2 instances in an Auto Scaling group. Thissystem uses a configuration management tool that runs locally on each EC2 instance. Because of thevolatility of the application load, new instances must be fully functional within 3 minutes of entering arunning state. Current setup tasks include:Installing the configuration management agent – 2 minutesInstalling the application framework – 15 minutesCopying configuration data from Amazon S3 – 2 minutesRunning the configuration management agent to configure instances – 1 minuteDeploying the application code from Amazon S3 – 2 minutesHow should the Engineer set up system so it meets the launch time requirement?</question>
        <choices>
            <choice>A. Trigger an AWS Lambda function from an Amazon CloudWatch Events rule when a new EC2 instancelaunches. Have the function install the configuration management agent and the application framework,pull configuration data from Amazon S3, run the agent to configure the instance, and deploy theapplication from S3.</choice>
            <choice>B. Write a bootstrap script to install the configuration management agent, install and the applicationframework, pull configuration data from Amazon S3, run the agent to configure the instance, and deploythe application from S3.</choice>
            <choice>C. Build a custom AMI that includes the configuration management agent and application framework. Writea bootstrap script to pull configuration data from Amazon S3, run the agent to configure the instance,and deploy the application from S3.</choice>
            <choice>D. Build a custom AMI that includes the configuration management agent, application framework, andconfiguration data. Write a bootstrap script to run the agent to configure the instance and deploy theapplication from Amazon S3.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>The resources for a business-critical, three-tier web application are expressed in a series of AWSCloudFormation templates. The application is using Amazon RDS for data and Amazon ElastiCache forsession state. Users have reported degraded performance in the application. A DevOps Engineer noticesthat the T2 instance type is being used for the application tier and CPU usage is at 100% in AmazonCloudWatch.What process should the Engineer follow to restore operations with the LEAST amount of distribution to theend users?</question>
        <choices>
            <choice>A. Write a new CloudFormation template to include Amazon CloudFront in the environment, launch thestack, and update the Amazon Route 53 A record</choice>
            <choice>B. Launch a new CloudFormation stack for the application tier using the M4 instance type, run acceptancetests against the new stack, and update the Amazon Route 53 A record</choice>
            <choice>C. Update the CloudFormation stack for the application tier using the T2 Unlimited option, run acceptancetests against the new stack, and update the Amazon Route 53 A record</choice>
            <choice>D. Launch a new CloudFormation stack for all tiers of the application in a different region, run acceptancetests against the new stack, and update the Amazon Route 53 A record</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company has developed an AWS Lambda function that handles orders received through an API. Thecompany is using AWS CodeDeploy to deploy the Lambda function as the final stage of a CI/CD pipeline.A DevOps Engineer has notices there are intermittent failures of the ordering API for a few seconds afterdeployment. After some investigation, the DevOps Engineer believes the failures are due to databasechanges not having fully propagated before the lambda function begins executing.How should the DevOps Engineer overcome this?</question>
        <choices>
            <choice>A. Add a BeforeAllowTraffic hook to the AppSpec file that tests and waits for any necessary databasechanges before traffic can flow to the new version of the Lambda function</choice>
            <choice>B. Add an AfterAllowTraffic hook to the AppSpec file that forces traffic to wait for any pending databasechanges before allowing the new version of the Lambda function to respond</choice>
            <choice>C. Add a BeforeInstall hook to the AppSpec file that tests and waits for any necessary database changesbefore deploying the new version of the Lambda function</choice>
            <choice>D. Add a ValidateService hook to the AppSpec file that inspects incoming traffic and rejects the payload ifdependent services such as the database are not yet ready</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A mobile application running on eight Amazon EC2 instances is relying on a third-party API endpoint. Thethird-party service has a high failure rate because of limited capacity, which is expected to be resolved in afew weeks.In the meantime, the mobile application developers have added a retry mechanism and are logging failedAPI requests. A DevOps Engineer must automate the monitoring of application logs and count the specificerror messages; if there are more than 10 errors within a 1-minute window, the system must issue an alert.How can the requirements be met with MINIMAL management overhead?</question>
        <choices>
            <choice>A. Install the Amazon CloudWatch Logs agent on all instances to push the application logs to CloudWatchLogs. Use metric filters to count the error messages every minute, and trigger a CloudWatch alarm ifthe count exceeds 10 errors.</choice>
            <choice>B. Install the Amazon CloudWatch Logs agent on all instances to push the access logs to CloudWatchLogs. Create CloudWatch Events rule to count the error messages every minute, and trigger aCloudWatch alarm if the count exceeds 10 errors.</choice>
            <choice>C. Install the Amazon CloudWatch Logs agent on all instances to push the application logs toCloudWatchLogs. Use a metric filter to generate a custom CloudWatch metric that records the numberof failures and triggers a CloudWatch alarm if the custom metric reaches 10 errors in a 1-minute period.</choice>
            <choice>D. Deploy a custom script on all instances to check application logs regularly in a cron job. Count thenumber of error messages every minute, and push a data point to a custom. CloudWatch metric.Trigger a CloudWatch alarm if the custom metric reaches 10 errors in a 1-minute period.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer has several legacy applications that all generate different log formats. The Engineermust standardize the formats before writing them to Amazon S3 for querying and analysis.How can this requirement be met at the LOWEST cost?</question>
        <choices>
            <choice>A. Have the application send its logs to an Amazon EMR cluster and normalize the logs before sendingthem to Amazon S3</choice>
            <choice>B. Have the application send its logs to Amazon QuickSight, then use the Amazon QuickSight SPICEengine to normalize the logs. Do the analysis directly from Amazon QuickSight</choice>
            <choice>C. Keep the logs in Amazon S3 and use Amazon Redshift Spectrum to normalize the logs in place</choice>
            <choice>D. Use Amazon Kinesis Agent on each server to upload the logs and have Amazon Kinesis Data Firehoseuse an AWS Lambda function to normalize the logs before writing them to Amazon S3 </choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A company uses Amazon S3 to store proprietary information. The Development team creates buckets fornew projects on a daily basis. The Security team wants to ensure that all existing and future buckets haveencryption, logging, and versioning enabled. Additionally, no buckets should ever be publicly read or writeaccessible.What should a DevOps Engineer do to meet these requirements?</question>
        <choices>
            <choice>A. Enable AWS CloudTrail and configure automatic remediation using AWS Lambda.</choice>
            <choice>B. Enable AWS Config rules and configure automatic remediation using AWS Systems Managerdocuments.</choice>
            <choice>C. Enable AWS Trusted Advisor and configure automatic remediation using Amazon CloudWatch Events.</choice>
            <choice>D. Enable AWS Systems Manager and configure automatic remediation using Systems Managerdocuments.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer is researching the least-expensive way to implement an image batch processingcluster in AWS. The application cannot run in Docker containers and must run on Amazon EC2. The batchjob stores checkpoint data on a Network File System (NFS) and can tolerate interruptions. Configuring thecluster software from a bare EC2 Amazon Linux image takes 30 minutes.Which is the MOST cost-effective solution?</question>
        <choices>
            <choice>A. Use Amazon EFS for checkpoint data. To complete the job, use an EC2 Auto Scaling group and an On-Demand pricing model to provision EC2 instances temporarily.</choice>
            <choice>B. Use ClusterFS on EC2 instances for checkpoint data. To run the batch job, configure EC2 instancesmanually. When the job completes, shut down the instances manually.</choice>
            <choice>C. Use Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances, and use user datato configure the EC2 Amazon Linux instance on startup. (3번 사용 use-use-use)</choice>
            <choice>D. Use Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances. Create a standardcluster AMI and use the latest AMI when creating instances.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A company is using AWS CodeDeploy to manage its application deployments. Recently, the Developmentteam decided to use GitHub for version control, and the team is looking for ways to integrate the GitHubrepository with CodeDeploy. The team also needs to develop a way to automate deployment wheneverthere is a new commit on that repository. The team is currently deploying new application revisions bymanually indicating the Amazon S3 location.How can the integration be achieved in the MOST efficient way?</question>
        <choices>
            <choice>A. Create a GitHub webhook to replicate the repository to AWS CodeCommit. Create an AWSCodePipeline pipeline that uses CodeCommit as a source provider and AWS CodeDeploy as adeployment provider. Once configured, commit a change to the GitHub repository to start the firstdeployment.</choice>
            <choice>B. Create an AWS CodePipeline pipeline that uses GitHub as a source provider and AWS CodeDeploy asa deployment provider. Connect this new pipeline with the GitHub account and instruct CodePipeline touse webhooks in GitHub to automatically start the pipeline when a change occurs.</choice>
            <choice>C. Create an AWS Lambda function to check periodically if there has been a new commit within the GitHubrepository. If a new commit is found, trigger a CreateDeployment API call to AWS CodeDeploy to start anew deployment based on the last commit ID within the deployment group.</choice>
            <choice>D. Create an AWS CodeDeploy custom deployment configuration to associate the GitHub repository withthe deployment group. During the association process, authenticate the deployment group with GitHubto obtain the GitHub security authentication token. Configure the deployment group options toautomatically deploy if a new commit is found. Perform a new commit to the GitHub repository to triggerthe first deployment. </choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer must implement monitoring for a workload running on Amazon EC2 and Amazon RDSMySQL. The monitoring must include:- Application logs and operating system metrics for the Amazon EC2 instances- Database logs and operating system metrics for the Amazon RDS databaseWhich steps should the Engineer take?</question>
        <choices>
            <choice>A. Install an Amazon CloudWatch agent on the EC2 and RDS instances. Configure the agent to send theoperating system metrics and application and database logs to CloudWatch.</choice>
            <choice>B. Install an Amazon CloudWatch agent on the EC2 instance, and configure the agent to send theapplication logs and operating system metrics to CloudWatch. Enable RDS Enhanced Monitoring, andmodify the RDS instance to publish database logs to CloudWatch Logs.</choice>
            <choice>C. Install an Amazon CloudWatch Logs agent on the EC2 instance and configure it to send application logsto CloudWatch.</choice>
            <choice>D. Set up scheduled tasks on the EC2 and RDS instances to put operating system metrics and applicationand database logs into an Amazon S3 bucket. Set up an event on the bucket to invoke an AWSLambda function to monitor for errors each time an object is put into the bucket.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company mandates the creation of capture logs for everything running in its AWS account. The accounthas multiple VPCs with Amazon EC2 instances, Application Load Balancers, Amazon RDS MySQLdatabases, and AWS WAF rules configured. The logs must be protected from deletion. A daily visualanalysis of log anomalies from the previous day is required.Which combination of actions should a DevOps Engineer take to accomplish this? (Choose three.)</question>
        <choices>
            <choice>A. Configure an AWS Lambda function to send all CloudWatch logs to an Amazon S3 bucket. Create adashboard report in Amazon QuickSight.</choice>
            <choice>B. Configure AWS CloudTrail to send all logs to Amazon Inspector. Create a dashboard report in AmazonQuickSight.</choice>
            <choice>C. Configure Amazon S3 MFA Delete on the logging Amazon S3 bucket.</choice>
            <choice>D. Configure an Amazon S3 object lock legal hold on the logging Amazon S3 bucket.</choice>
            <choice>E. Configure AWS Artifact to send all logs to the logging Amazon S3 bucket. Create a dashboard report inAmazon QuickSight.F. Deploy an Amazon CloudWatch agent to all Amazon EC2 instances.</choice>
        </choices>
        <correctAnswers>C,E,F</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer wants to prevent Developers from pushing updates directly to the company’s masterbranch in AWS CodeCommit. These updates should be approved before they are merged.Which solution will meet these requirements?</question>
        <choices>
            <choice>A. Configure an IAM role for the Developers with access to CodeCommit and an explicit deny for writeactions when the reference is the master. Allow Developers to use feature branches and create a pullrequest when a feature is complete. Allow an approver to use CodeCommit to view the changes and approve the pull requests.</choice>
            <choice>B. Configure an IAM role for the Developers to use feature branches and create a pull request when afeature is complete. Allow CodeCommit to test all code in the feature branches, and dynamically modifythe IAM role to allow merging the feature branches into the master. Allow an approver to useCodeCommit to view the changes and approve the pull requests.</choice>
            <choice>C. Configure an IAM role for the Developers to use feature branches and create a pull request when afeature is complete. Allow CodeCommit to test all code in the feature branches, and issue a new AWSSecurity Token Service (STS) token allowing a one-time API call to merge the feature branches into themaster. Allow an approver to use CodeCommit to view the changes and approve the pull requests.</choice>
            <choice>D. Configure an IAM role for the Developers with access to CodeCommit and attach an access policy tothe CodeCommit repository that denies the Developers role access when the reference is master. AllowDevelopers to use feature branches and create a pull request when a feature is complete. Allow anapprover to use CodeCommit to view the changes and approve the pull requests. </choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A company is using AWS Organizations to create separate AWS accounts for each of its departments. Itneeds to automate the following tasks:- Updating the Linux AMIs with new patches periodically and generating a golden image- Installing a new version of Chef agents in the golden image, if available- Enforcing the use of the newly generated golden AMIs in the department's accountWhich option requires the LEAST management overhead?</question>
        <choices>
            <choice>A. Write a script to launch an Amazon EC2 instance from the previous golden AMI, apply the patchupdates, install the new version of the Chef agent, generate a new golden AMI, and then modify the AMIpermissions to share only the new image with the departments’ accounts.</choice>
            <choice>B. Use an AWS Systems Manager Run Command to update the Chef agent first, use Amazon EC2Systems Manager Automation to generate an updated AMI, and then assume an IAM role to copy thenew golden AMI into the departments’ accounts.</choice>
            <choice>C. Use AWS Systems Manager Automation to update the Linux AMI using the previous image, provide theURL for the script that will update the Chef agent, and then use AWS Organizations to replace theprevious golden AMI into the departments’ accounts.</choice>
            <choice>D. Use AWS Systems Manager Automation to update the Linux AMI from the previous golden image,provide the URL for the script that will update the Chef agent, and then share only the newly generatedAMI with the departments’ accounts.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company wants to automatically re-create its infrastructure using AWS CloudFormation as part of thecompany's quality assurance (QA) pipeline. For each QA run, a new VPC must be created in a singleaccount, resources must be deployed into the VPC, and tests must be run against this new infrastructure.The company policy states that all VPCs must be peered with a central management VPC to allowcentralized logging. The company has existing CloudFormation templates to deploy its VPC and associatedresources.Which combination of steps will achieve the goal in a way that is automated and repeatable? (Choose two.)</question>
        <choices>
            <choice>A. Create an AWS Lambda function that is invoked by an Amazon CloudWatch Events rule when a CreateVpcPeeringConnection API call is made. The Lambda function should check the source of the peering request, accepts the request, and update the route tables for the management VPC to allow traffic to go over the peering connection.</choice>
            <choice>B. In the CloudFormation template:  Invoke a custom resource to generate unique VPC CIDR ranges for the VPC and subnets.  Create a peering connection to the management VPC.  Update route tables to allow traffic to the management VPC. </choice>
            <choice>C. In the CloudFormation template:  Use the Fn::Cidr function to allocate an unused CIDR range for the VPC and subnets.  Create a peering connection to the management VPC.  Update route tables to allow traffic to the management VPC. </choice>
            <choice>D. Modify the CloudFormation template to include a mappings object that includes a list of /16 CIDR ranges for each account where the stack will be deployed. </choice>
            <choice>E. Use CloudFormation StackSets to deploy the VPC and associated resources to multiple AWS accounts usinga custom resource to allocate unique CIDR ranges. Create peering connections from each VPC to the central management VPC and accept those connections in the management VPC.</choice>
        </choices>
        <correctAnswers>B,D</correctAnswers>
    </exam>
    <exam>
        <question>A company has multiple development groups working in a single shared AWS account. The SeniorManager of the groups wants to be alerted via a third-party API call when the creation of resourcesapproaches the service limits for the account.Which solution will accomplish this with the LEAST amount of development effort?</question>
        <choices>
            <choice>A. Create an Amazon CloudWatch Event rule that runs periodically and targets an AWS Lambda function.Within the Lambda function, evaluate the current state of the AWS environment and compare deployedresource values to resource limits on the account. Notify the Senior Manager if the account isapproaching a service limit.</choice>
            <choice>B. Deploy an AWS Lambda function that refreshes AWS Trusted Advisor checks, and configure anAmazon CloudWatch Events rule to run the Lambda function periodically. Create another CloudWatchEvents rule with an event pattern matching Trusted Advisor events and a target Lambda function. In thetarget Lambda function, notify the Senior Manager.</choice>
            <choice>C. Deploy an AWS Lambda function that refreshes AWS Personal Health Dashboard checks, andconfigure an Amazon CloudWatch Events rule to run the Lambda function periodically. Create anotherCloudWatch Events rule with an event pattern matching Personal Health Dashboard events and a targetLambda function. In the target Lambda function, notify the Senior Manager.</choice>
            <choice>D. Add an AWS Config custom rule that runs periodically, checks the AWS service limit status, andstreams notifications to an Amazon SNS topic. Deploy an AWS Lambda function that notifies the SeniorManager, and subscribe the Lambda function to the SNS topic.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A highly regulated company has a policy that DevOps Engineers should not log in to their Amazon EC2instances except in emergencies. If a DevOps Engineer does log in, the Security team must be notifiedwithin 15 minutes of the occurrence.Which solution will meet these requirements?</question>
        <choices>
            <choice>A. Install the Amazon Inspector agent on each EC2 instance. Subscribe to Amazon CloudWatch Eventsnotifications. Trigger an AWS Lambda function to check if a message is about user logins. If it is, send a notification to the Security team using Amazon SNS.</choice>
            <choice>B. Install the Amazon CloudWatch agent on each EC2 instance. Configure the agent to push all logs toAmazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user logins. If a loginis found, send a notification to the Security team using Amazon SNS.</choice>
            <choice>C. Set up AWS CloudTrail with Amazon CloudWatch Logs. Subscribe CloudWatch Logs to AmazonKinesis. Attach AWS Lambda to Kinesis to parse and determine if a log contains a user login. If it does,send a notification to the Security team using Amazon SNS.</choice>
            <choice>D. Set up a script on each Amazon EC2 instance to push all logs to Amazon S3. Set up an S3 event totrigger an AWS Lambda function, which triggers an Amazon Athena query to run. The Athena querychecks for logins and sends the output to the Security team using Amazon SNS.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer manages a web application that runs on Amazon EC2 instances behind an ApplicationLoad Balancer (ALB). The instances run in an EC2 Auto Scaling group across multiple Availability Zones.The Engineer needs to implement a deployment strategy that:- Launches a second fleet of instances with the same capacity as the original fleet.- Maintains the original fleet unchanged while the second fleet is launched.- Transitions traffic to the second fleet when the second fleet is fully deployed.- Terminates the original fleet automatically 1 hour after transition.Which solution will satisfy these requirements?</question>
        <choices>
            <choice>A. Use an AWS CloudFormation template with a retention policy for the ALB set to 1 hour. Update theAmazon Route 53 record to reflect the new ALB.</choice>
            <choice>B. Use two AWS Elastic Beanstalk environments to perform a blue/green deployment from the originalenvironment to the new one. Create an application version lifecycle policy to terminate the originalenvironment in 1 hour.</choice>
            <choice>C. Use AWS CodeDeploy with a deployment group configured with a blue/green deployment configuration.Select the option Terminate the original instances in the deployment group with a waiting period of 1hour.</choice>
            <choice>D. Use AWS Elastic Beanstalk with the configuration set to Immutable. Create an .ebextension using theResources key that sets the deletion policy of the ALB to 1 hour, and deploy the application.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company is using Docker containers for an application deployment and wants to move its application toAWS. The company currently manages its own clusters on premises to manage the deployment of thesecontainers. It wants to deploy its application to a managed service in AWS and wants the entire flow of thedeployment process to be automated. In addition, the company has the following requirements:- Focus first on the development workload.- The environment must be easy to manage.- Deployment should be repeatable and reusable for new environments.- Store the code in a GitHub repository.Which solution will meet these requirements?</question>
        <choices>
            <choice>A. Set up an Amazon ECS environment. Use AWS CodePipeline to create a pipeline that is triggered on acommit to the GitHub repository. Use AWS CodeBuild to create the container images and AWSCodeDeploy to publish the container image to the ECS environment.</choice>
            <choice>B. Use AWS CodePipeline that triggers on a commit from the GitHub repository, build the containerimages with AWS CodeBuild, and publish the container images to Amazon ECR. In the final stage, useAWS CloudFormation to create an Amazon ECS environment that gets the container images from theECR repository.</choice>
            <choice>C. Create a Kubernetes Cluster on Amazon EC2. Use AWS CodePipeline to create a pipeline that istriggered when the code is committed to the repository. Create the container images with a Jenkinsserver on EC2 and store them in the Docker Hub. Use AWS Lambda from the pipeline to trigger thedeployment to the Kubernetes Cluster.</choice>
            <choice>D. Set up an Amazon ECS environment. Use AWS CodePipeline to create a pipeline that is triggered on acommit to the GitHub repository. Use AWS CodeBuild to create the container and store it in the DockerHub. Use an AWS Lambda function to trigger a deployment and pull the new container image from theDocker Hub.</choice>
        </choices>
        <correctAnswers>A</correctAnswers>
    </exam>
    <exam>
        <question>A company has migrated its container-based applications to Amazon EKS and want to establish automatedemail notifications. The notifications sent to each email address are for specific activities related to EXScomponents. The solution will include Amazon SNS topics and an AWS Lambda function to evaluateincoming log events and publish messages to the correct SNS topic.Which logging solution will support these requirements?</question>
        <choices>
            <choice>A. Enable Amazon CloudWatch Logs to log the EKS components. Create a CloudWatch subscription filterfor each component with Lambda as the subscription feed destination.</choice>
            <choice>B. Enable Amazon CloudWatch Logs to log the EKS components. Create CloudWatch Logs Insightsqueries linked to Amazon CloudWatch Events events that trigger Lambda.</choice>
            <choice>C. Enable Amazon S3 logging for the EKS components. Configure an Amazon CloudWatch subscriptionfilter for each component with Lambda as the subscription feed destination.</choice>
            <choice>D. Enable Amazon S3 logging for the EKS components. Configure S3 PUT Object event notifications withAWS Lambda as the destination.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>An n-tier application requires a table in an Amazon RDS MySQL DB instance to be dropped andrepopulated at each deployment. This process can take several minutes and the web tier cannot comeonline until the process is complete. Currently, the web tier is configured in an Amazon EC2 Auto Scalinggroup, with instances being terminated and replaced at each deployment. The MySQL tableis populated by running a SQL query through an AWS CodeBuild job.What should be done to ensure that the web tier does not come online before the database is completelyconfigured?</question>
        <choices>
            <choice>A. Use Amazon Aurora as a drop-in replacement for RDS MySQL. Use snapshots to populate the tablewith the correct data.</choice>
            <choice>B. Modify the launch configuration of the Auto Scaling group to pause user data execution for 600seconds, allowing the table to be populated.</choice>
            <choice>C. Use AWS Step Functions to monitor and maintain the state of data population. Mark the database inservice before continuing with the deployment.</choice>
            <choice>D. Use an EC2 Auto Scaling lifecycle hook to pause the configuration of the web tier until the table ispopulated. </choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A web application with multiple services runs on Amazon EC2 instances behind an Application LoadBalancer. The application stores data in an Amazon RDS Multi-AZ DB instance. The instance health checkused by the load balancer returns PASS if at least one service is running on the instance.The company uses AWS CodePipeline with AWS CodeBuild and AWS CodeDeploy steps to deploy code totest and production environments. Recently, a new version was unable to connect to the database server inthe test environment. One process was running, so the health checks reported healthy and the applicationwas promoted to production, causing a production outage. The company wants to ensure that test buildsare fully functional before a promotion to production.Which changes should a DevOps Engineer make to the test and deployment process? (Choose two.)</question>
        <choices>
            <choice>A. Add an automated functional test to the pipeline that ensures solid test cases are performed.</choice>
            <choice>B. Add a manual approval action to the CodeDeploy deployment pipeline that requires a Testing Engineerto validate the testing environment.</choice>
            <choice>C. Refactor the health check endpoint the Elastic Load Balancer is checking to better validate actualapplication functionality.</choice>
            <choice>D. Refactor the health check endpoint the Elastic Load Balancer is checking to return a text-based statusresult and configure the load balancer to check for a valid response.</choice>
            <choice>E. Add a dependency checking step to the existing testing framework to ensure compatibility.</choice>
        </choices>
        <correctAnswers>D,E</correctAnswers>
    </exam>
    <exam>
        <question>A company's application is currently deployed to a single AWS Region. Recently, the company opened anew office on a different continent. The users in the new office are experiencing high latency. Thecompany's application runs on Amazon EC2 instances behind an Application Load Balancer (ALB) anduses Amazon DynamoDB as the database layer. The instances run in an EC2 Auto Scaling group acrossmultiple Availability Zones. A DevOps Engineer is tasked with minimizing application response times andimproving availability for users in both Regions.Which combination of actions should be taken to address the latency issues? (Choose three.)</question>
        <choices>
            <choice>A. Create a new DynamoDB table in the new Region with cross-Region replication enabled.</choice>
            <choice>B. Create new ALB and Auto Scaling group global resources and configure the new ALB to direct traffic tothe new Auto Scaling group.</choice>
            <choice>C. Create new ALB and Auto Scaling group resources in the new Region and configure the new ALB todirect traffic to the new Auto Scaling group.</choice>
            <choice>D. Create Amazon Route 53 records, health checks, and latency-based routing policies to route to theALB.</choice>
            <choice>E. Create Amazon Route 53 aliases, health checks, and failover routing policies to route to the ALB.F. Convert the DynamoDB table to a global table.8A3E48E222C4B4B15D7694BE00C90AAA </choice>
        </choices>
        <correctAnswers>C,D,F</correctAnswers>
    </exam>
    <exam>
        <question>A security review has identified that an AWS CodeBuild project is downloading a database population scriptfrom an Amazon S3 bucket using an unauthenticated request. The Security team does not allowunauthenticated requests to S3 buckets for this project.How can this issue be corrected in the MOST secure manner?</question>
        <choices>
            <choice>A. Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the buildspec to use the AWS CLI to download the database population script.</choice>
            <choice>B. Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update thebuild spec to use cURL to pass the token and download the database population script.</choice>
            <choice>C. Remove unauthenticated access from the S3 bucket with a bucket policy. Modify the service role for theCodeBuild project to include Amazon S3 access. Use the AWS CLI to download the databasepopulation script.</choice>
            <choice>D. Remove unauthenticated access from the S3 bucket with a bucket policy. Use the AWS CLI todownload the database population script using an IAM access key and a secret access key.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer is deploying an Amazon API Gateway API with an AWS Lambda function providing thebackend functionality. The Engineer needs to record the source IP address and response status of everyAPI call.Which combination of actions should the DevOps Engineer take to implement this functionality? (Choosethree.)  </question>
        <choices>
            <choice>A. Configure AWS X-Ray to enable access logging for the API Gateway requests.</choice>
            <choice>B. Configure the API Gateway stage to enable access logging and choose a logging format.</choice>
            <choice>C. Create a new Amazon CloudWatch Logs log group or choose an existing log group to store the logs.</choice>
            <choice>D. Grant API Gateway permission to read and write logs to Amazon CloudWatch through an IAM role.</choice>
            <choice>E. Create a new Amazon S3 bucket or choose an existing S3 bucket to store the logs.</choice>
            <choice>F. Configure API Gateway to stream its log data to Amazon Kinesis.</choice>
        </choices>
        <correctAnswers>B,D,E</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer at a startup cloud-based gaming company has the task formalizing deploymentstrategies. The strategies must meet the following requirements:Use standard Git commands, such as git clone and git push for the code repository.Management tools should maximize the use of platform solutions where possible.Deployment packages must be immutable and in the form of Docker images.How can the Engineer meet these requirements?</question>
        <choices>
            <choice>A. Use AWS CodePipeline to trigger a build process when software is pushed to a self-hosted GitHubrepository. CodePipeline will use a Jenkins build server to build new Docker images. CodePipeline willdeploy into a second target group in Amazon ECS behind an Application Load Balancer. Cutover will bemanaged by swapping the listener rules on the Application Load Balancer.</choice>
            <choice>B. Use AWS CodePipeline to trigger a build process when software is pushed to a private GitHubrepository. CodePipeline will use AWS CodeBuild to build new Docker images. CodePipeline will deployinto a second target group in Amazon ECS behind an Application Load Balancer. Cutover will bemanaged by swapping the listener rules on the Application Load Balancer.</choice>
            <choice>C. Use a Jenkins pipeline to trigger a build process when software is pushed to a private GitHub repository.AWS CodePipeline will use AWS CodeBuild new Docker images. CodePipeline will deploy into asecond target group in Amazon ECS behind an Application Load Balancer. Cutover will be managed byswapping the listener rules on the Application Load Balancer.</choice>
            <choice>D. Use AWS CodePipeline to trigger a build process when software is pushed to an AWS CodeCommitrepository CodePipeline will use an AWS CodeBuild build server to build new Docker images.CodePipeline will deploy into a second target group in a Kubernetes Cluster hosted on Amazon EC2behind an Application Load Balancer. Cutover will be managed by swapping the listener rules on theApplication Load Balancer. </choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>An application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). A DevOpsEngineer is using AWS CodeDeploy to release a new version. The deployment fails during the AllowTrafficlifecycle event, but a cause for the failure is not indicated in the deployment logs.What would cause this?</question>
        <choices>
            <choice>A. The appspec.yml file contains an invalid script to execute in the AllowTraffic lifecycle hook.</choice>
            <choice>B. The user who initiated the deployment does not have the necessary permissions to interact with theALB.</choice>
            <choice>C. The health checks specified for the ALB target group are misconfigured.</choice>
            <choice>D. The CodeDeploy agent was not installed in the EC2 instances that are part of the ALB target group.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A company is deploying a container-based application using AWS CodeBuild. The Security team mandatesthat all containers are scanned for vulnerabilities prior to deployment using a password-protected endpoint.All sensitive information must be stored securely.Which solution should be used to meet these requirements?</question>
        <choices>
            <choice>A. Encrypt the password using AWS KMS. Store the encrypted password in the buildspec.yml file as anenvironment variable under the variables mapping. Reference the environment variable to initiatescanning.</choice>
            <choice>B. Import the password into an AWS CloudHSM key. Reference the CloudHSM key in the buildpec.yml fileas an environment variable under the variables mapping. Reference the environment variable to initiatescanning. </choice>
            <choice>C. Store the password in the AWS Systems Manager Parameter Store as a secure string. Add theParameter Store key to the buildspec.yml file as an environment variable under the parameter-storemapping. Reference the environment variable to initiate scanning.</choice>
            <choice>D. Use the AWS Encryption SDK to encrypt the password and embed in the buildspec.yml file as a variableunder the secrets mapping. Attach a policy to CodeBuild to enable access to the required decryptionkey.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer must ensure all IAM entity configurations across multiple AWS accounts in AWSOrganizations are compliant with corporate IAM policies.Which combination of steps will accomplish this? (Choose two.)  </question>
        <choices>
            <choice>A. Enable AWS Trusted Advisor in Organizations for all accounts to report on noncompliant IAM entities.</choice>
            <choice>B. Configure an AWS Config aggregator in the Organizations master account for all accounts.</choice>
            <choice>C. Deploy AWS Config rules to the master account in Organizations that match corporate IAM policies.</choice>
            <choice>D. Apply an SCP in Organizations to ensure compliance of IAM entities.</choice>
            <choice>E. Deloy AWS Config rules to all accounts in Organizations that match the corporate IAM policies</choice>
        </choices>
        <correctAnswers>D,E</correctAnswers>
    </exam>
    <exam>
        <question>A company has thousands of Amazon EC2 instances as well as hundreds of virtual machines on-premises.Developers routinely sign in to the console for on-premises systems to perform troubleshooting. TheDevelopers want to sign in to AWS instances to run performance tools, but are unable to due to the lack ofa central console logging system. A DevOps Engineer wants to ensure that console access is logged on allsystems.Which combination of steps will meet these requirements? (Choose two.)</question>
        <choices>
            <choice>A. Attach a role to all AWS instances that contains the appropriate permissions. Create an AWS SystemsManager managed-instance activation. Install and configure Systems Manager Agent on on-premisesmachines.</choice>
            <choice>B. Enable AWS Systems Manager Session Manager logging to an Amazon S3 bucket. Direct Developersto connect to the systems with Session Manager only.</choice>
            <choice>C. Enable AWS Systems Manager Session Manager logging to AWS CloudTrail. Direct Developers tocontinue normal sign-in procedures for on-premises. Use Session Manager for AWS instances.</choice>
            <choice>D. Install and configure an Amazon CloudWatch Logs agent on all systems. Create an AWS SystemsManager managed-instance activation.</choice>
            <choice>E. Set up a Site-to-Site VPN connection between the on-premises and AWS networks. Set up a bastioninstance to allow Developers to sign in to the AWS instances.</choice>
        </choices>
        <correctAnswers>A,B</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps team wants to be able to work on the same source code repository. The team has the followingrequirements for their development workflow and repository access controls:Only team members can clone the repository and create new branches.A production-ready code state should be isolated from any untested code changes.Code changes should be approved by another team member before merging to the production-readymaster branch.All code change approvals must have an audit record.New team members can quickly modify code.Which combination of actions will these requirements? (Choose three.)</question>
        <choices>
            <choice>A. Check out the master branch and develop new features locally on a feature branch to keep theproduction-ready code isolated. Ask team members to review the changes before committing thechanges locally.</choice>
            <choice>B. Create an AWS CodeCommit repository and an IAM group with permissions to read/write changes tothe repository. Add new team members to this group.</choice>
            <choice>C. Create an AWS CodeCommit repository and an IAM role with permissions to read/write changes to therepository. Attach this IAM role to a single IAM user. Ensure each member of the team uses this IAMuser. Provide new team members the credentials to this IAM user.</choice>
            <choice>D. Create a local feature branch from the master branch for new features. Commit the new code and pushthe changes to the feature branch in the repository.</choice>
            <choice>E. Create a pull request so other team members can review the code changes. Implement anysuggestions, pull any additional changes from the master branch, and push to the feature branch again.Merge the master branch with the feature branch.F. Create a pull request so other team members can review the code changes. Implement anysuggestions, pull any additional changes from the master branch, resolve any conflicts, and push to thefeature branch again. Merge the feature branch with the master branch.</choice>
        </choices>
        <correctAnswers>A,B,E</correctAnswers>
    </exam>
    <exam>
        <question>A company has a web application that uses an Amazon DynamoDB table in a single AWS Region to storeuser information. To support an increasingly global user base, the application must run in a secondaryRegion and allow users to connect to their closest Region and fail over to the secondary Region.Which approach should be used to ensure the deployment meets these requirements?</question>
        <choices>
            <choice>A. Configure DynamoDB streams to copy data between Regions, deploy the web stack in both Regions,and configure Amazon Route 53 to use a geoproximity routing policy with health checks.</choice>
            <choice>B. Convert the DynamoDB table to a global table, deploy the web stack in both Regions, and configureAmazon Route 53 to use a geoproximity routing policy with health checks.</choice>
            <choice>C. Define DynamoDB cross-region backups to copy data to the secondary Region, deploy the web stack inboth Regions, and configure Amazon Route 53 to use a latency-based routing policy with health checks.</choice>
            <choice>D. Use DynamoDB Accelerator to copy data to the secondary Region, deploy the web stack in bothRegions, and configure Amazon Route 53 to use a failover routing policy.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>An ecommerce company uses a large number of Amazon EBS backed Amazon EC2 instances. Todecrease manual work across all the instances, a DevOps Engineer is tasked with automating restartactions when EC2 instance retirement events are scheduled.How can this be accomplished?</question>
        <choices>
            <choice>A. Create a scheduled Amazon CloudWatch Events rule to execute an AWS Systems Managerautomation document that checks if any EC2 instances are scheduled for retirement once a week. If theinstance is scheduled for retirement, the automation document will hibernate the instance.</choice>
            <choice>B. Enable EC2 Auto Recovery on all of the instances. Create an AWS Config rule to limit the recovery tooccur during a maintenance window only.</choice>
            <choice>C. Reboot all EC2 instances during an approved maintenance window that is outside of standard businesshours. Set up Amazon CloudWatch alarms to send a notification in case any instance is failing EC2instance status checks.</choice>
            <choice>D. Set up an AWS Health Amazon CloudWatch Events rule to execute AWS Systems Managerautomation documents that stop and start the EC2 instance when a retirement scheduled event occurs.</choice>
        </choices>
        <correctAnswers>D</correctAnswers>
    </exam>
    <exam>
        <question>A company has containerized all of its in-house quality control applications. The company is runningJenkins on Amazon EC2, which requires patching and upgrading. The Compliance Officer has requested aDevOps Engineer begin encrypting build artifacts since they contain company intellectual property.What should the DevOps Engineer do to accomplish this in the MOST maintainable manner?</question>
        <choices>
            <choice>A. Automate patching and upgrading using AWS Systems Manager on EC2 instances and encryptAmazon EBS volumes by default.</choice>
            <choice>B. Deploy Jenkins to an Amazon ECS cluster and copy build artifacts to an Amazon S3 bucket with defaultencryption enabled.</choice>
            <choice>C. Leverage AWS CodePipeline with a build action and encrypt the artifacts using AWS Secrets Manager.</choice>
            <choice>D. Use AWS CodeBuild with artifact encryption to replace the Jenkins instance running on Amazon EC2.</choice>
        </choices>
        <correctAnswers>C</correctAnswers>
    </exam>
    <exam>
        <question>A DevOps Engineer is setting up a container-based architecture. The Engineer has decided to use AWSCloudFormation to automatically provision an Amazon ECS cluster and an Amazon EC2 Auto Scalinggroup to launch the EC2 container instances. After successfully creating the CloudFormation stack, theEngineer noticed that, even though the ECS cluster and the EC2 instances were created successfully andthe stack finished the creation, the EC2 instances were associating with a different cluster.How should the DevOps Engineer update the CloudFormation template to resolve this issue?</question>
        <choices>
            <choice>A. Reference the EC2 instances in the AWS::ECS::Cluster resource and reference the ECS cluster in theAWS::ECS::Service resource.</choice>
            <choice>B. Reference the ECS cluster in the AWS::AutoScaling::LaunchConfiguration resource of the UserDataproperty.</choice>
            <choice>C. Reference the ECS cluster in the AWS::EC2::Instance resource of the UserData property.</choice>
            <choice>D. Reference the ECS cluster in the AWS::CloudFormation::CustomResource resource to trigger an AWSLambda function that registers the EC2 instances with the appropriate ECS cluster.</choice>
        </choices>
        <correctAnswers>B</correctAnswers>
    </exam>
    <exam>
        <question>A company indexes all of its Amazon CloudWatch Logs on Amazon ES and uses Kibana to view adashboard for actionable insight. The company wants to restrict user access to Kibana by user.Which actions can a DevOps Engineer take to meet this requirement? (Choose two.)</question>
        <choices>
            <choice>A. Create a proxy server with user authentication in an Auto Scaling group, and restrict access of the Amazon ES endpoint to an Auto Scaling group tag.</choice>
            <choice>B. Create a proxy server with user authentication and an Elastic IP address, and restrict access of theAmazon ES endpoint to the IP address.</choice>
            <choice>C. Create a proxy server with AWS IAM user, and restrict access of the Amazon ES endpoint to the IAMuser.</choice>
            <choice>D. Use AWS SSO to offer user name and password protection for Kibana.</choice>
            <choice>E. Use Amazon Cognito to offer user name and password protection for Kibana.</choice>
        </choices>
        <correctAnswers>C,E</correctAnswers>
    </exam>
</examlList>